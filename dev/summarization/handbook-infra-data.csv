No.,end_point,post_date,link,title,content,summary,classification,keyword
1,https://alphasignal.ai/,2025.02.06,https://www.youtube.com/watch?v=7xTGNNLPyMI,Deep Dive into LLMs like ChatGPT,"This is a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full training stack of how the models are developed, along with mental models of how to think about their ""psychology"", and how to get the best use them in practical applications. I have one ""Intro to LLMs"" video already from ~year ago, but that is just a re-recording of a random talk, so I wanted to loop around and do a lot more comprehensive version.

Instructor
Andrej was a founding member at OpenAI (2015) and then Sr. Director of AI at Tesla (2017-2022), and is now a founder at Eureka Labs, which is building an AI-native school. His goal in this video is to raise knowledge and understanding of the state of the art in AI, and empower people to effectively use the latest and greatest in their work.
Find more at https://karpathy.ai/ and https://x.com/karpathy",,,
2,https://alphasignal.ai/,2025.02.05,https://www.anthropic.com/research/constitutional-classifiers,Constitutional Classifiers: Defending against universal jailbreaks,"A new paper from the Anthropic Safeguards Research Team describes a method that defends AI models against universal jailbreaks. A prototype version of the method was robust to thousands of hours of human red teaming for universal jailbreaks, albeit with high overrefusal rates and compute overhead. An updated version achieved similar robustness on synthetic evaluations, and did so with a 0.38% increase in refusal rates and moderate additional compute costs.

We are currently hosting a temporary live demo version of a Constitutional Classifiers system, and we encourage readers who have experience jailbreaking AI systems to help “red team” it. Find out more below and at the demo website.

*Update 5 February 2025: We are now offering $10,000 to the first person to pass all eight levels of our jailbreaking demo, and $20,000 to the first person to do so with a universal jailbreaking strategy. Full details of the reward and the associated conditions can be found at HackerOne.

Large language models have extensive safety training to prevent harmful outputs. For example, we train Claude to refuse to respond to user queries involving the production of biological or chemical weapons.

Nevertheless, models are still vulnerable to jailbreaks: inputs designed to bypass their safety guardrails and force them to produce harmful responses. Some jailbreaks flood the model with very long prompts; others modify the style of the input, such as uSiNg uNuSuAl cApItALiZaTiOn. Historically, jailbreaks have proved difficult to detect and block: these kinds of attacks were described over 10 years ago, yet to our knowledge there are still no fully robust deep-learning models in production.

We’re developing better jailbreak defenses so that we can safely deploy increasingly capable models in the future. Under our Responsible Scaling Policy, we may deploy such models as long as we’re able to mitigate risks to acceptable levels through appropriate safeguards—but jailbreaking lets users bypass these safeguards. In particular, we’re hopeful that a system defended by Constitutional Classifiers could allow us to mitigate jailbreaking risks for models which have passed the CBRN capability threshold outlined in our Responsible Scaling Policy1.

In our new paper, we describe a system based on Constitutional Classifiers that guards models against jailbreaks. These Constitutional Classifiers are input and output classifiers trained on synthetically generated data that filter the overwhelming majority of jailbreaks with minimal over-refusals and without incurring a large compute overhead.

Results from human red teaming
We ran two main categories of tests to assess the effectiveness of Constitutional Classifiers.

First, we developed a prototype version of the system to identify and block specific scientific knowledge related to chemical, biological, radiological, and nuclear harms. We then invited independent jailbreakers to a bug-bounty program in which they were challenged to “red team” the system (i.e., to attempt to break it under experimental conditions to test its robustness).

Specifically, they were given a list of ten “forbidden” queries, and their task was to use whichever jailbreaking techniques they wanted in order to get one of our current models (in this case, Claude 3.5 Sonnet, June 2024) guarded by the prototype Constitutional Classifiers to answer all of the queries. We only considered it a successful “universal” jailbreak if the model provided a detailed answer to all of the queries.

183 active2 participants spent an estimated >3,000 hours over a two-month experimental period attempting to jailbreak the model. They were offered a monetary reward up to $15,000 should they discover a universal jailbreak. Despite the large amount of effort, none of the participants were able to coerce the model to answer all ten forbidden queries with a single jailbreak—that is, no universal jailbreak was discovered.

Despite its robustness to jailbreaks, this prototype system had some problems: it refused too many harmless queries and cost a lot of computational resources to run. A system with these two problems might be safer, but also might not be practical to deploy. Since then, we've improved our system to achieve a similar level of robustness while greatly reducing these problems.

Results from automated evaluations
We ran a set of automated evaluations where we synthetically generated 10,000 jailbreaking prompts including many of the most-effective attacks on current LLMs, as well as attacks designed to circumvent classifier safeguards. We tried these prompts on a version of Claude 3.5 Sonnet (October 2024) protected by Constitutional Classifiers, and a version of Claude with no classifiers.

Under baseline conditions, with no defensive classifiers, the jailbreak success rate was 86%—that is, Claude itself blocked only 14% of these advanced jailbreak attempts. Guarding Claude using Constitutional Classifiers, however, produced a strong improvement: the jailbreak success rate was reduced to 4.4%, meaning that over 95% of jailbreak attempts were refused.

A graph showing the results for vulnerability to jailbreaks, overrefusals, and compute overhead for the Constitutional Classifiers system versus the base model
Results from automated evaluations. For all plots, lower is better. (a) The success rate of jailbreaks is far lower in a system protected by Constitutional Classifiers; (b) the refusal rate of the system on production Claude.ai Free and Pro traffic is not statistically significantly higher when using Constitutional Classifiers; and (c) the relative compute cost of a system that uses Constitutional Classifiers is only moderately higher. Error bars represent 95% confidence intervals computed using binomial proportion standard errors under asymptotic normality assumptions.
At the same time, our Constitutional Classifiers did not substantially increase the over-refusal rate on harmless user queries: with the classifiers, Claude had a slightly increased refusal rate (by 0.38%), but this was not statistically significant in a random sample of 5,000 conversations. Finally, the compute cost was moderately higher (by 23.7%) than that of the unguarded model. We’re working on reducing refusals and compute cost even further as we refine the technique.

Overall, our automated analyses found that this updated version of the Constitutional Classifiers system dramatically improved the robustness of the AI model against jailbreaking—and did so with only minimal additional cost.

How it works
Constitutional Classifiers is based on a similar process to Constitutional AI, another technique we have used to align Claude. Both techniques use a constitution: a list of principles to which the model should adhere. In the case of Constitutional Classifiers, the principles define the classes of content that are allowed and disallowed (for example, recipes for mustard are allowed, but recipes for mustard gas are not).

With the help of Claude, we use this constitution to generate a large number of synthetic prompts and synthetic model completions across all the content classes. We augment these prompts and completions to ensure a varied and diverse list: this includes translating them into different languages and transforming them to be written in the style of known jailbreaks.

A schematic diagram of the how the Constitutional Classifiers system works, from the creation of the constitution through to generating a test set to using the system to guard an LLM
Training and implementing Constitutional Classifiers. (a) A constitution is produced specifying harmless and harmful categories; (b) the constitution is used as the basis for the production of many synthetic prompts and completions, which are further augmented (with variations on style and language) and turned into a training set; (c) classifiers trained on this training set are used as model safeguards to detect and block harmful content.
We then use these synthetic data to train our input and output classifiers to flag (and block) potentially harmful content according to the given constitution. To help minimize over-refusals (i.e., harmless content incorrectly flagged as harmful), we also train the classifiers on a fixed set of benign queries generated by a contractor.

Limitations
Constitutional Classifiers may not prevent every universal jailbreak, though we believe that even the small proportion of jailbreaks that make it past our classifiers require far more effort to discover when the safeguards are in use. It’s also possible that new jailbreaking techniques might be developed in the future that are effective against the system; we therefore recommend using complementary defenses. Nevertheless, the constitution used to train the classifiers can rapidly be adapted to cover novel attacks as they’re discovered.

The full paper contains all the details about the Constitutional Classifiers method, and about the classifiers themselves.

Constitutional Classifiers live demo
Want to try red teaming Claude yourself? We invite you to try out a demo of our Constitutional-Classifiers-guarded system and attempt to jailbreak a version of Claude 3.5 Sonnet that is guarded using our new technique.

Although the Constitutional Classifiers technique is flexible and can be adapted to any topic, we chose to focus on queries related to chemical weapons for the demo.

Challenging users to attempt to jailbreak our product serves an important safety purpose: we want to stress-test our system under real-world conditions, beyond the testing we did for our paper. This allows us to gather additional data and improve the robustness of the method prior to deploying this method on our production systems in the future.

The demo will be live from Feb 3, 2025 to Feb 10, 2025. It includes a feedback form where you can contact us to report any successful jailbreaks as well as information on our Responsible Disclosure Policy, which we ask that participants follow. We’ll announce any successes and the general results of the demo in an update to this post.

*Update 5 February 2025: As noted above, we are now offering a monetary reward for successful jailbreaking of our system. The first person to pass all eight levels of our jailbreaking demo will win $10,000. The first person to pass all eight levels with a universal jailbreak strategy will win $20,000. Full details of the reward and the associated conditions can be found at HackerOne.

Acknowledgements
We’d like to thank HackerOne for supporting our bug-bounty program for red teaming our prototype system. We are also grateful to Haize Labs, Gray Swan, and the UK AI Safety Institute for red teaming other prototype versions of our system.

Join our team
If you’re interested in working on problems such as jailbreak robustness or on other questions related to model safeguards, we’re currently recruiting for Research Engineers / Scientists, and we’d love to see your application.",,,
3,https://alphasignal.ai/,2024.10.31,https://openai.com/index/introducing-chatgpt-search/,Introducing ChatGPT search,"ChatGPT can now search the web in a much better way than before. You can get fast, timely answers with links to relevant web sources, which you would have previously needed to go to a search engine for. This blends the benefits of a natural language interface with the value of up-to-date sports scores, news, stock quotes, and more.
ChatGPT will choose to search the web based on what you ask, or you can manually choose to search by clicking the web search icon.
Search will be available at chatgpt.com, as well as on our desktop and mobile apps. All ChatGPT Plus and Team users, as well as SearchGPT waitlist users, will have access today. Enterprise and Edu users will get access in the next few weeks. We’ll roll out to all Free users over the coming months. 
Designed to get you to a better answer
Getting useful answers on the web can take a lot of effort. It often requires multiple searches and digging through links to find quality sources and the right information for you.
Now, chat can get you to a better answer: Ask a question in a more natural, conversational way, and ChatGPT can choose to respond with information from the web. Go deeper with follow-up questions, and ChatGPT will consider the full context of your chat to get a better answer for you.
We also partnered with news and data providers to add up-to-date information and new visual designs for categories like weather, stocks, sports, news, and maps.
“ChatGPT search promises to better highlight and attribute information from trustworthy news sources, benefiting audiences while expanding the reach of publishers like ourselves who produce premium journalism.” Pam Wasserstein, President, Vox Media
Go straight to the source
Chats now include links to sources, such as news articles and blog posts, giving you a way to learn more. Click the Sources button below the response to open a sidebar with the references.
“We are convinced that AI search will be, in a near future and for the next generations, a primary way to access information, and partnering with OpenAI positions Le Monde at the forefront of this shift. It allows us to test innovations at an early stage while safeguarding journalism’s core values and integrity.” Louis Dreyfus, CEO & Publisher of Le Monde
ChatGPT search connects people with original, high-quality content from the web and makes it part of their conversation. By integrating search with a chat interface, users can engage with information in a new way, while content owners gain new opportunities to reach a broader audience. We hope to help users discover publishers and websites, while bringing more choice to search.
“As AI reshapes the media landscape, Axel Springer’s partnership with OpenAI opens up tremendous opportunities for innovative advancements. Together, we're driving new business models that ensure journalism remains both trustworthy and profitable.” Mathias Sanchez, SVP Global Strategic Partnerships Axel Springer SE
We collaborated extensively with the news industry and carefully listened to feedback from our global publisher partners, including Associated Press, Axel Springer, Condé Nast, Dotdash Meredith, Financial Times, GEDI, Hearst, Le Monde, News Corp, Prisa (El País), Reuters, The Atlantic, Time, and Vox Media. Any website or publisher can choose to appear in ChatGPT search. If you’d like to share feedback, please email us at publishers-feedback@openai.com
How it works and what comes next
The search model is a fine-tuned version of GPT-4o, post-trained using novel synthetic data generation techniques, including distilling outputs from OpenAI o1-preview. ChatGPT search leverages third-party search providers, as well as content provided directly by our partners, to provide the information users are looking for. Learn more here.
Thanks to feedback from the SearchGPT prototype, we brought the best of the SearchGPT experience into ChatGPT. We plan to keep improving search, particularly in areas like shopping and travel, and leverage the reasoning capabilities of the OpenAI o1 series to do deeper research. We also plan to bring our new search experience to Advanced Voice and canvas, as well as to Free and logged out users in the future.
ChatGPT Plus and Team users can try it out today at chatgpt.com. They can also download the Chrome extension to search directly via the browser URL bar.",,,
4,https://alphasignal.ai/,2025.02.04,https://huggingface.co/blog/open-deep-research,Open-source DeepResearch – Freeing our search agents,"TLDR
Yesterday, OpenAI released Deep Research, a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time.

One of the main results in the blog post is a strong improvement of performances on the General AI Assistants benchmark (GAIA), a benchmark we’ve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on especially challenging “level 3” questions that involve multiple steps of reasoning and tool usage (see below for a presentation of GAIA).

DeepResearch is composed of an LLM (which can be selected from the current list of LLMs provided by OpenAI, 4o, o1, o3, etc) and an internal “agentic framework” which guide the LLM to use tools like web search and organize its actions in steps.

While powerful LLMs are now freely available in open-source (see e.g. the recent DeepSeek R1 model), OpenAI didn’t disclose much about the agentic framework underlying Deep Research…

So we decided to embark on a 24-hour mission to reproduce their results and open-source the needed framework along the way!

The clock is ticking, let’s go! ⏱️

Table of Contents
What are Agent frameworks and why they matter?
The GAIA benchmark
Building an open Deep Research
Using a CodeAgent
Making the right tools 🛠️
Results 🏅
Community reproductions
Most important next steps
What are Agent frameworks and why they matter?
An Agent framework is a layer on top of an LLM to make said LLM execute actions (like browse the web or read PDF documents), and organize its operations in a series of steps. For a quick intro to agents, check this great interview by Andrew Ng and our introduction blog post to the smolagents library. For a more detailed dive in agents you can subscribe to our agents course that starts in just a few days: link here.

Almost everyone has already experienced how powerful LLMs can be simply by playing with chatbots.. However, what not everyone is aware of yet is that integrating these LLMs into agentic systems can give them real superpowers!

Here is a recent example comparing the performance of a few frontier LLMs with and without an agentic framework (in this case the simple smolagents library) - using an agentic framework bumps performance by up to 60 points!

Benchmarks

In fact, OpenAI also highlighted in its release blogpost how Deep Research performed dramatically better than standalone LLMs on the knowledge-intensive ""Humanity’s Last Exam"" benchmark.

So, what happens when we integrate our current top LLM in an agentic framework, to work toward an open-DeepResearch ?

A quick note: We’ll benchmark our results on the same GAIA challenge but keep in mind that this is a work in progress. DeepResearch is a massive achievement and its open reproduction will take time. In particular, full parity will require improved browser use and interaction like OpenAI Operator is providing, i.e. beyond the current text-only web interaction we explore in this first step.

Let’s first understand the scope of the challenge: GAIA.

The GAIA benchmark
GAIA is arguably the most comprehensive benchmark for agents. Its questions are very difficult and hit on many challenges of LLM-based systems. Here is an example of a hard question:

Which of the fruits shown in the 2008 painting ""Embroidery from Uzbekistan"" were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film ""The Last Voyage""? Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o'clock position. Use the plural form of each fruit.

You can see this question involves several challenges:

Answering in a constrained format,
Using multimodal capabilities (to extract the fruits from the image),
Gathering several pieces of information, some depending on others:
Identifying the fruits on the picture
Finding which ocean liner was used as a floating prop for “The Last Voyage”
Finding the October 1949 breakfast menu for the above ocean liner
Chaining together a problem-solving trajectory in the correct order.
Solving this requires both high-level planning abilities and rigorous execution, which are two areas where LLMs struggle when used alone.

So it’s an excellent test set for agent systems!

On GAIA’s public leaderboard, GPT-4 does not even reach 7% on the validation set when used without any agentic setup. On the other side of the spectrum, with Deep Research, OpenAI reached 67.36% score on the validation set, so an order of magnitude better! (Though we don’t know how they would actually fare on the private test set.)

Let’s see if we can do better with open source tools!

Building an open Deep Research
Using a CodeAgent
The first improvement over traditional AI agent systems we’ll tackle is to use a so-called “code agent”. As shown by Wang et al. (2024), letting the agent express its actions in code has several advantages, but most notably that code is specifically designed to express complex sequences of actions.

Consider this example given by Wang et al.:

Code Agent

This highlights several advantages of using code:

Code actions are much more concise than JSON.
Need to run 4 parallel streams of 5 consecutive actions ? In JSON, you would need to generate 20 JSON blobs, each in their separate step; in Code it’s only 1 step.
On average, the paper shows that Code actions require 30% fewer steps than JSON, which amounts to an equivalent reduction in the tokens generated. Since LLM calls are often the dimensioning cost of agent systems, it means your agent system runs are ~30% cheaper.
Code enables to re-use tools from common libraries
Better performance in benchmarks, due to two reasons:
More intuitive way to express actions
Extensive exposure of LLMs to code in training
The advantages above were confirmed by our experiments on the agent_reasoning_benchmark.

From building smolagents we can also cite a notable additional advantage, which is a better handling of state: this is very useful for multimodal tasks in particular. Need to store this image/audio/other for later use? No problem, just assign it as a variable in your state and you can re-use it 4 steps later if needed. In JSON you would have to let the LLM name it in a dictionary key and trust the LLM will later understand that it can still use it.

Making the right tools 🛠️
Now we need to provide the agent with the right set of tools.

1. A web browser. While a fully fledged web browser interaction like Operator will be needed to reach full performance, we started with an extremely simple text-based web browser for now for our first proof-of-concept. You can find the code here

2. A simple text inspector, to be able to read a bunch of text file format, find it here.

These tools were taken from the excellent Magentic-One agent by Microsoft Research, kudos to them! We didn’t change them much, as our goal was to get as high a performance as we can with the lowest complexity possible.

Here is a short roadmap of improvements which we feel would really improve these tools’ performance (feel free to open a PR and contribute!):

extending the number of file formats which can be read.
proposing a more fine-grained handling of files.
replacing the web browser with a vision-based one, which we’ve started doing here.
Results 🏅
In our 24h+ reproduction sprint, we’ve already seen steady improvements in the performance of our agent on GAIA!

We’ve quickly gone up from the previous SoTA with an open framework, around 46% for Magentic-One, to our current performance of 55.15% on the validation set.

This bump in performance is due mostly to letting our agents write their actions in code! Indeed, when switching to a standard agent that writes actions in JSON instead of code, performance of the same setup is instantly degraded to 33% average on the validation set.

Here is the final agentic system.

We’ve set up a live demo here for you to try it out!


However, this is only the beginning, and there are a lot of things to improve! Our open tools can be made better, the smolagents framework can also be tuned, and we’d love to explore the performance of better open models to support the agent.

We welcome the community to come join us in this endeavour, so we can leverage the power of open research together to build a great open-source agentic framework! It would allow anyone to run a DeepResearch-like agent at home, with their favorite models, using a completely local and customized approach!

Community Reproductions
While we were working on this and focusing on GAIA, other great open implementations of Deep Research emerged from the community, specifically from

dzhng,
assafelovic,
nickscamara,
jina-ai and
mshumer.
Each of these implementations use different libraries for indexing data, browsing the web and querying LLMs. In this project, we would like to reproduce the benchmarks presented by OpenAI (pass@1 average score), benchmark and document our findings with switching to open LLMs (like DeepSeek R1), using vision LMs, benchmark traditional tool calling against code-native agents.

Most important next steps
OpenAI’s Deep Research is probably boosted by the excellent web browser that they introduced with Operator.

So we’re tackling that next! In a more general problem: we’re going to build GUI agents, i.e. “agents that view your screen and can act directly with mouse & keyboard”. If you’re excited about this project, and want to help everyone get access to such cool capabilities through open source, we’d love to get your contribution!

We’re also hiring a full time engineer to help us work on this and more, apply if you’re interested 🙂

To get started with Open Deep Research, try the examples here.
Check the smolagents repo.
Read more about smolagents docs, introduction blog post.",,,
5,https://decodingml.substack.com/,2025.02.06,https://decodingml.substack.com/p/build-your-second-brain-ai-assistant?utm_source=substack&utm_medium=email,Build your Second Brain AI assistant,"The first lesson of the open-source course “Building Your Second Brain AI Assistant Using LLMs and RAG” — a free course that will teach you how to design and build a Notion-like AI assistant that talks to digital notes and resources.

Lessons:
Lesson 1: Build your Second Brain AI assistant

Lesson 2: Data pipelines for building AI assistants (WIP)

Lesson 3: Generate high-quality fine-tuning datasets (WIP)

Lesson 4: Fine-tune and deploy open-source LLMs (WIP)

Lesson 5: RAG feature pipelines for building AI assistants (WIP)

Lesson 6: Agents and LLMOps (WIP)

🔗 Learn more about the course and its outline.

Build your Second Brain AI assistant
Welcome to Decoding ML’s “Building Your Second Brain AI Assistant Using LLMs and RAG” open-source course, where you will learn to architect and build a production-ready Notion-like AI assistant:

Intuitively building a Second Brain AI assistant is like having access to the collective wisdom of your own mind. Sounds fantastic, right? More on this later.

To build the Second Brain AI assistant, we must implement an LLM application using agents, advanced Retrieval-augmented generation (RAG), fine-tuning, LLMOps and AI systems techniques.

The best way to learn something new is by doing. Thus, within this course, you will learn these concepts and how to apply them while building your Second Brain AI assistant, which you can later customize.

This lesson will present what you will build and learn throughout the course.

Next, we will explore the system architecture of the Second Brain AI assistant illustrated in Figure 1. We will explain each component's role, what it is, what algorithms and tools we used, and, most importantly, why we used them.

By the end of this lesson, you will have a strong intuition of what it takes to architect an AI assistant for your Second Brain, such as a Notion-like AI assistant that allows you to chat with your notes and resources.


Figure 1: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.
We will zoom in on each component in future lessons and present the theory and implementation behind each ML pipeline. Thus, by the end of this course, you will learn how to build your own AI assistant.

LLM systems have the same fundamental blocks. Hence, after going through our lessons, you will have the necessary knowledge to build your production LLM apps on your favorite use cases.

Meanwhile, here is a quick and fun podcast-like audio that will walk you through what you will build and learn through the course (at 1.5x speed, it’s super helpful):

Let’s get started. Enjoy!

Table of Contents:
What are we going to build?

What are we going to learn?

Introducing the custom Notion data source

Exploring the flow of the data

Presenting the feature/training/inference (FTI) architecture

Architecting our Second Brain AI assistant

Offline vs. online ML pipelines

Running the code

1. What are we going to build?
As the first lesson of the open-source six-lesson course “Building Your Second Brain AI Assistant Using LLMs and RAG,” we must clearly define our ultimate goal, including what we will build and learn throughout the process.

Defining our end goal or target is critical whether you're doing a course or implementing a real-world application.

In this course, we will show you how to build a custom AI assistant on top of your notes, lists or other resources that you usually store in apps such as Notion, Apple Notes, Google Keep, Evernote, Obsidian or similar applications.

The productivity geeks (such as myself) like to call the system that captures all your thoughts, tasks, meetings, events and notes “your Second Brain.” Usually, a Second Brain is more than just a note-taking app. It includes tools such as a calendar for meetings and cloud storage for, well…, storage.

For the sake of simplicity, we will narrow down our problem to building an AI assistant on top of our Notion custom data sources, imitating Notion’s AI features, as seen in Figure 2. Another similar example is NotebookLM, where you provide a set of sources, and the AI generates answers only based on them.


Figure 2: Screenshot of asking the Notion AI assistant, “How can I optimize LLMs during inference?”
So… What will we build?

An AI assistant that generates answers solely based on our digital knowledge stored in our Second Brain, which, in our case, will be the data we store in Notion.

As we said in the beginning, it’s like having access to the collective wisdom of your own mind.

This is a relevant use case for avoiding hallucinations, as you limit the domain to your resources. You can easily control the generation and evaluation steps by conditioning the LLM to your resources.

As a fun (and relevant) example, we will use our list of filtered resources (which we keep in Notion) on AI and ML, such as GenAI, LLMs, RAG, MLOps, LLMOps and information retrieval.

Everyone has one of those, right?

The thing is that it gets hard to access exactly what you need when you need it.

Thus, we will show you how to hook a GenAI system on top of your research and resources to ask questions, retrieve relevant resources and synthesize information solely based on your research, which you already know is valuable and useful.

2. What are we going to learn?
Following Decoding ML’s mission, we will show you how to build an end-to-end AI system using the Second Brain AI Assistant as an example.

Thus, we will walk you through how to design such a system with production in mind.

Then, we will show you how to implement it, starting with collecting data from Notion, preprocessing and storing it, until using it to fine-tune LLMs and build an agentic RAG application.

As this is an educational project, we tried to avoid using frameworks such as LangChain and build everything from scratch. Doing so will help you develop your intuition, making using LLM frameworks a breeze.

Still, extensibility is a real pain when using LLM frameworks such as LangChain. Thus, real-world skills include extending these frameworks using object-oriented programming (OOP).

That’s why we used LangChain to load and retrieve data from a MongoDB vector database while showing you how to extend its components and add your app’s custom implementation.

Thus, in this course, we will cover the following concepts, algorithms and tools:

Architecting an AI system using the FTI architecture.

Using MLOps best practices such as data registries, model registries, and experiment trackers.

Crawling over 700 links and normalizing everything into Markdown using Crawl4AI.

Computing quality scores using LLMs.

Generating summarization datasets using distillation.

Fine-tuning a Llama model using Unsloth and Comet.

Deploying the Llama model as an inference endpoint to Hugging Face serverless Dedicated Endpoints.

Implement advanced RAG algorithms using contextual retrieval, hybrid search and MongoDB vector search.

Build an agent that uses multiple tools using Hugging Face’s smolagents framework.

Using LLMOps best practices such as prompt monitoring and RAG evaluation using Opik.

Integrate pipeline orchestration, artifact and metadata tracking using ZenML.

Manage the Python project using uv and ruff.

Apply software engineering best practices.

Excited?

Let’s start by exploring our custom Notion data source in more depth.

3. Introducing the custom Notion data source
You know what everyone says: “Every successful AI/ML project starts with understanding your data.”

Our use case is not different. It all starts with understanding our custom Notion data source.

If you are unfamiliar with Notion, you must know that it’s a fancier note-taking app that allows you to create notes, tasks, wikis and databases.

Figure 3 shows our eight Notion databases, which contain various resources on topics such as GenAI, information retrieval, MLOps and system design.

We use these notes, resources, and research to build AI and software products: “Yes, it’s the actual database we reference while building.”


Figure 3: Our Notion databases
Let’s dig into a specific database.

Figure 4 shows the “Generative AI” Notion database, which contains ~25 data entries on different topics in the space. The “Node” type contains high-level links, such as blogs, benchmarks, or awesome lists, while a “Leaf” contains super-specific resources or tools.


Figure 4: The “Generative AI” Notion database.
Let’s open a data entry.

As seen in Figure 5, we can see that a data entry contains:

Multiple “Notes” pages containing my thoughts on various topics.

Multiple “Toggle” elements contain links to various blogs, articles or tools.


Figure 5: The “LLM Inference Optimization & Other Techniques” Notion database entry
We integrated with Notion’s API and automatically downloaded and parsed all these documents in Markdown format.

The fun and interesting thing about this problem is that the data contains relevant links that must be crawled and further processed into Markdown. But, the catch is that the links are mixed with insightful notes we want to keep and feed into our system. Thus, we must find a way to differentiate between documents that contain only links for crawling and valuable documents by themselves.

A database entry will not always look like the one from Figure 5. The data is noisy and can have any form. The only rule is that it contains links, text, images, and attached documents (similar to a real-world use case).

We will stick to links and text for this course, but it can be extended to processing images and documents (a fun exercise for you).

For ease of use, we stored a snapshot of the Notion data from above in a public S3 bucket, which you can download without AWS credentials.

Thus, you don’t need to use Notion or hook your Notion to complete this course. But if you want to, you can, as we expose in the GitHub repository, a flexible pipeline that can load any Notion database.

The next step is to explore the data flow required to build your Second Brain AI assistant, from Notion to fine-tuning and RAG.

4. Exploring the flow of the data
The first step to understanding how our AI system looks is to understand its data flow, abstracting away other details such as tooling, infrastructure or algorithms.

Our goal is to collect data for Retrieval-Augmented Generation (RAG). Thus, we can feed our custom data as context to an LLM. We also need to collect data to fine-tune an open-source LLM (such as Llama 3.1 8B) to specialize in summarization (you will soon understand “why summarization”).

If you are not familiar with how a naive Retrieval Augmented Generation (RAG) system works, read more about it in Decoding ML:

Retrieval-Augmented Generation (RAG) Fundamentals First
Retrieval-Augmented Generation (RAG) Fundamentals First
Paul Iusztin
·
2024년 8월 31일
Read full story
As illustrated in Figure 6, let’s walk you through the flow and lifecycle of your data:

We collect raw Notion documents in Markdown format.

We crawl each link in the Notion documents and normalize them in Markdown.

We store a snapshot of the data in a NoSQL database.

For fine-tuning, we filter the documents more strictly to narrow the data to only high-quality samples.

We use the high-quality samples to distillate a summarization instruction dataset, which we store in a data registry.

Using the generated dataset, we fine-tune an open-source LLM, which we save in a model registry.

In parallel, we use a different filter threshold for RAG to narrow down the documents to medium to high-quality samples (for RAG, we can work with more noise).

We chunk, embed, plus other advanced RAG preprocessing steps to optimize the retrieval of the documents.

We load the embedded chunks and their metadata in a vector database.

Leveraging the vector database, we use semantic search to retrieve the top K most relevant chunks relative to a user query.


Figure 6: The data flow of building a RAG system and fine-tuning LLMs
If something doesn’t make sense, don’t worry. Throughout the course, we will zoom in on each component and explain why and how we did everything.

Now that we understand how the data flows, let's quickly examine the feature/training/inference (FTI) design we will use to build our Notion AI assistant.

5. Presenting the feature/training/inference (FTI) architecture
The pattern suggests that any AI/ML system can be boiled down to these three pipelines: feature, training, and inference.

Jim Dowling, CEO and Co-Founder of Hopsworks, introduced the pattern to simplify building production ML systems.

The feature pipelines take raw data as input and output features and labels to train our model(s).

The training pipeline takes the features and labels from the feature stored as input and outputs our trained model(s).

The inference pipeline inputs the features and labels from the feature store and the trained model(s) from the model registry. Using these two components, we can make predictions in batch or real-time mode and serve them to the client.


Figure 7: The feature/training/inference (FTI) architecture
Note that larger ML systems will have more than three pipelines. Thus, by convention, we name each pipeline based on its output artifact. That’s how we decide whether it’s a feature, training or inference pipeline.

To conclude, the most important thing you must remember about the FTI pipelines is their interface:

The feature pipeline takes in data and outputs features & labels saved to the feature store.

The training pipelines query the features store for features & labels and output a model to the model registry.

The inference pipeline uses the features from the feature store and the model from the model registry to make predictions.

It doesn’t matter how complex your ML system gets. These interfaces will remain the same.

There is a lot more to the FTI architecture. To learn more, consider reading the following article from Decoding ML:

Building ML systems the right way using the FTI architecture
Building ML systems the right way using the FTI architecture
Paul Iusztin
·
2024년 8월 10일
Read full story
6. Architecting our Second Brain AI assistant
Now that we have laid out all the foundations, such as understanding the data flow involved in building an AI assistant and the FTI architecture, let’s design our AI system.

As seen in Figure 8, we have 5 significant components that we have to understand:

The data pipelines

The feature pipelines

The training pipeline

The inference pipelines

The observability pipeline

You might wonder why we have five pipelines instead of three, as the FTI architecture suggests.

The data engineering team often owns the data pipeline, which prepares the data required to build AI systems.

Meanwhile, the observability pipeline is implemented on top of the FTI architecture to monitor and evaluate the system. Having eyes and ears all over your system is critical for success, especially in LLM systems, which are highly non-deterministic.

An ideal strategy is to implement an end-to-end workflow of your app quickly, plus the observability pipeline. Then, you use the metrics from the evaluation pipeline and logs from the prompt monitoring pipeline as clear signals on what works and what doesn’t.


Figure 8: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.
Let’s zoom in on each.

The data pipelines
As the name suggests, the data pipelines collect data from Notion, clean it, standardize it, and load it to a NoSQL database as our clean and normalized data snapshot before feeding it into the feature pipelines.

By storing the standardized and cleaned data in a NoSQL database, such as MongoDB, we collect the data once (as a backup), which we can use to experiment and build our AI system.

In larger AI systems, this is usually done with a data warehouse or lake, but a NoSQL database is lighter and does the job for us.

The data pipeline is split into two significant components.

The data collection pipeline: It uses Notion’s API to retrieve the data programmatically, where each Notion data entry is standardized to Markdown format and saved as JSON along with necessary metadata. To decouple the data collection step from the rest of the system, we save everything to a public S3 bucket to avoid giving public access to our Notion workspace.

The ETL pipeline: It extracts the raw Notion documents from S3, finds all the embedded links within the documents, crawls them, and standardizes them into Markdown format. It also computes a quality score/document using LLMs. Ultimately, it saves all the documents and metadata into a MongoDB NoSQL database.

The feature pipelines
The feature pipelines leverage the standardized and clean data from the MongoDB NoSQL database for two things:

To populate a MongoDB vector database for doing RAG.

To generate a summarization instruct dataset for fine-tuning an LLM.

These are two standard feature pipelines you will see in the GenAI world.

Within the RAG feature pipeline, we will implement advanced pre-retrieval RAG techniques, such as Contextual Retrieval, proposed by Antrophic. To implement it, we will require a summarization LLM and hybrid search.

Ultimately, we chunk the documents, embed them, and loaded them into a MongoDB vector database.

For more theory on advanced RAG and how a naive RAG system can be optimized, you can read our article from Decoding ML:

Your RAG is wrong: Here's how to fix it
Your RAG is wrong: Here's how to fix it
Paul Iusztin
·
1월 2일
Read full story
Using APIs such as OpenAI for summarization can get costly (which we need for Contextual Retrieval), so we fine-tune a summarization open-source LLM. To do this, we require a custom summarization dataset.

Thus, we will leverage distillation techniques for the dataset generation pipeline to create a high-quality summarization instruction dataset based on our documents.

We will save the generated dataset to Hugging Face’s data registry. As an example, you can check our generated dataset: pauliusztin/second_brain_course_summarization_task

The training pipeline
The training pipeline reads the instruct dataset from the data registry and uses Unsloth to fine-tune a Llama 3.1 8B LLM. We use Comet to log the metrics and hyperparameters between multiple experiments, compare them, and pick the best one.

After deciding on the best model, we load it into Hugging Face’s model registry. As an example, you can check our fine-tuned LLM: pauliusztin/Meta-Llama-3.1-8B-Instruct-Second-Brain-Summarization

ZenML orchestrates and manages the data, feature, and training pipelines, helping us run the pipelines with a clearly defined structure and configuration. As illustrated in Figure 9, we can track the progress, status and history of each pipeline in a beautiful UI.


Figure 9: Visualizing our ML pipelines from ZenML’s dashboard.
The inference pipelines
We have two inference pipelines.

The summarization inference pipeline, which contains only the fine-tuned LLM, is deployed as a real-time inference endpoint on Hugging Face’s Dedicated Endpoints serverless service.

The agentic inference pipeline is our AI assistant, which takes as input requests from a user and provides answers leveraging the data from the MongoDB vector database.

We implemented it using Hugging Face’s smolagents Python framework, which allows us to build agents without hiding too much of what is going on behind the scenes. We attached a retriever tool that interacts with the vector database and a summarization tool to help us synthesize answers.

We will also attach the agentic inference pipeline to a Gradio UI to completely simulate the experience of an AI assistant, as shown in Figure 10.


Figure 10: Screenshot of our AI assistant Gradio UI asking it to write a paragraph on optimizing LLMs during inference. We can visualize how it used the MongoDB vector search tool to get more context from our custom data until it reached the maximum allowed number of steps.
The observability pipeline
the last piece of the puzzle is the observability pipeline, which consists of two main components:

Prompt monitoring

LLM evaluation

For both, we will use Opik, which provides a beautiful dashboard for monitoring complex prompt traces, as seen in Figure 11.

It also provides a Python SDK to help us evaluate agentic and RAG applications, track the results and compare them (similar to experiment tracking, but for evaluating LLM applications).


Figure 11: Screenshot from Opik on monitoring the trace of an agent. We can visualize the user’s prompt and all the hidden steps until it reaches its final answer.
7. Offline vs. online ML pipelines
One last architectural decision we have to highlight is the difference between the offline and online ML pipelines.

Offline pipelines are batch pipelines that run on a schedule or trigger. They usually take input data, process it, and save the output artifact in another type of storage. From there, other pipelines or clients can consume the artifact as they see fit.

Thus, in our AI system, the offline ML pipelines are the

Data collection pipeline

ETL data pipeline

RAG feature pipeline

Dataset generation feature pipeline

Training pipeline

These are all independent processes that can run one after the other or on different schedules. They don’t have to run in sequence, as they are entirely decoupled through various storages: a NoSQL database, a vector database, a data registry or a model registry.

Because of their nature, we will orchestrate all the offline pipelines using ZenML, a popular ML orchestrator that allows us to schedule, trigger, configure, or deploy each pipeline.


Figure 12: Offline vs. online ML pipelines
On the other hand, we have online pipelines that directly interact with a client. In this setup, a client (e.g., a user or other software) requests a prediction in real or near real-time. Thus, the system has to be online 24/7, process the request, and return the answer.

In our use case, the online pipelines are the following:

Agentic inference pipeline

Summarization inference pipeline

Observability pipeline

Because of their request-answer nature, online pipelines do not need orchestration. Instead, they adopt a strategy similar to deploying RESTful APIs from the software engineering world.

It is critical to highlight that the offline and online pipelines are entirely different processes and often entirely different applications.

Seeing these LangChain PoCs, where the RAG ingestion, retrieval and generation are in the same Notebook, can be deceiving. You never (or almost never) want to ingest the data at query time; you want to do it offline. Thus, when the user asks a question, the vector database is already populated and ready for retrieval.

To clearly reflect this aspect, our codebase decoupled the offline and online pipelines into two different Python applications, as shown in Figure 13.


Figure 13: The folder structure of the offline and online Python applications.
The last step is to say a few words about how you can run the code.

8. Running the code
This lesson was only an overview of what you will learn in the following five lessons. Thus, there is no specific code attached to this lesson.

However, if you want to test our code without going through the following lessons, we have provided end-to-end instructions on how to do so in our GitHub repository.

Thus, you can choose your learning journey: go through our lessons or directly try out the code.

Enjoy!

Conclusion
This lesson taught you what you will build and learn throughout the “Building Your Second Brain AI Assistant Using LLMs and RAG” open-source course.

In this lesson, we’ve laid out the foundations by presenting the data flow of the AI assistant and the FTI architecture.

Next, we’ve shown how to apply the FTI architecture on top of our data flow to architect a production-ready AI assistant.

Lesson 2 (WIP) will focus on implementing the ETL data pipeline. While building it, we will learn how to use ZenML to orchestrate offline ML pipelines, crawl custom URLs, parse them to Markdown, and compute a quality score/document using LLMs.

💻 Explore all the lessons and the code in our freely available GitHub repository.

If you have questions or need clarification, feel free to ask. See you in the next session!

Whenever you’re ready, there are 3 ways we can help you:
Perks: Exclusive discounts on our recommended learning resources

(live courses, self-paced courses, learning platforms and books).

The LLM Engineer’s Handbook: Our bestseller book on mastering the art of engineering Large Language Models (LLMs) systems from concept to production.

Free open-source courses: Master production AI with our end-to-end open-source courses, which reflect real-world AI projects, covering everything from system architecture to data collection and deployment.

References
Decodingml. (n.d.). GitHub - decodingml/second-brain-ai-assistant-course. GitHub. https://github.com/decodingml/second-brain-ai-assistant-course

Iusztin, P. (2024a, August 10). Building ML system using the FTI architecture. Decoding ML. https://decodingml.substack.com/p/building-ml-systems-the-right-way

Iusztin, P. (2024b, August 31). RAG Fundamentals first. Decoding ML. https://decodingml.substack.com/p/rag-fundamentals-first

Iusztin, P. (2025a, January 2). Advanced RAG Blueprint: Optimize LLM retrieval Systems. Decoding ML. https://decodingml.substack.com/p/your-rag-is-wrong-heres-how-to-fix",,,
6,https://decodingml.substack.com/,2025.02.13,https://decodingml.substack.com/p/data-pipelines-for-ai-assistants?utm_source=substack&utm_medium=email,Data pipelines for AI assistants,"Data pipelines for AI assistants
The backbone of successful AI systems
Paul Iusztin
Feb 13, 2025

The second lesson of the open-source course Building Your Second Brain AI Assistant Using Agents, LLMs and RAG — a free course that will teach you how to architect and build a personal AI research assistant that talks to your digital resources.

A journey where you will have the chance to learn to implement an LLM application using agents, advanced Retrieval-augmented generation (RAG), fine-tuning, LLMOps, and AI systems techniques.

Lessons:
Lesson 1: Build your Second Brain AI assistant

Lesson 2: Data pipelines for AI assistants

Lesson 3: Generate high-quality fine-tuning datasets (WIP)

Lesson 4: Fine-tune and deploy open-source LLMs (WIP)

Lesson 5: RAG feature pipelines for building AI assistants (WIP)

Lesson 6: Agents and LLMOps (WIP)

🔗 Learn more about the course and its outline.

Data pipelines for AI assistants
Welcome to Lesson 2 of Decoding ML’s Building Your Second Brain AI Assistant Using Agents, LLMs and RAG open-source course, where you will learn to architect and build a production-ready Notion-like AI research assistant.

Every data and AI system starts with data. If you don’t have data, you don’t have the raw material to work with. You can have the most fancy algorithms, but without data, they are still like a car without fuel.

Hence, this lesson will teach us to architect and build the data pipelines that fuel our Second Brain AI assistant, such as the Notion data collection and ETL data pipelines.

While implementing the data pipelines, we will learn the following:

Use an MLOps framework such as ZenML to manage the ML pipelines.

Read documents from Notion.

Structure and validate our documents using Pydantic.

Crawl ~400 links found in the Notion documents using Crawl4AI.

Compute quality scores for each document using a two-stage design based on heuristics and LLMs.

Normalize all the documents to Markdown.

Store all the standardized documents in a NoSQL MongoDB.


Figure 1: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.
Let’s get started. Enjoy!

Podcast version of the lesson
Table of contents:
Architecting data pipelines for AI systems

Understanding MLOps frameworks for managing ML pipelines

Exploring the collected Notion data

Looking into requirements for crawling

Implementing crawling

Computing the quality score

Loading the standardized data to a NoSQL database

Running the code

1. Architecting data pipelines for AI systems
In most use cases, a data pipeline starts with raw data collection, undergoes some transformation steps, and ultimately loads the transformed data into storage.

To build our Second Brain AI assistant, we must collect data from Notion, crawl all the links found in the Notion documents, and standardize everything in Markdown so downstream processes can easily process the documents.

To implement that, we split the data pipelines into two main components, making the architecture flexible and scalable.

The data collection pipeline
Which uses Notion’s API to access our personal data programmatically. Then, it extracts all the links from the crawled documents and adds them to their metadata. Ultimately, it standardizes the document into Markdown, saves it as JSON, and loads the results in S3.

Forcing people to use their Notion collections wasn’t a scalable solution or a good user experience, and neither was making our Notion collection public.

Thus, we decided to save our processed Notion collection into a public S3 bucket, which everyone can access effortlessly.

To draw a parallel to the industry, the S3 bucket could be seen as a data lake, where multiple teams from the organization push raw data that can be used within the company.

The course will start with reading our custom Notion collection from S3 and the next steps in processing. However, if you want a personalized experience, we provide the code and instructions for collecting your Notion collections.

Thus, let’s dig into the ETL data pipeline, where most of our focus will be.

The ETL data pipeline
ETL stands for “Extract, Transform, Load,” a popular data engineering pattern applied to most data pipelines.

The pattern is simple. You have to extract data from a source, apply some transformations, and ultimately load it into storage that makes the processed data accessible.

Here is how it looks to us:

Extract storage: S3

Transformations: crawling, standardization to Markdown, computing a quality score

Load storage: NoSQL MongoDB

Now, let’s dig into the details of each step.

Once the data has been downloaded from S3, we want to enrich it by crawling all the links within the Notion documents. This makes sense for two core reasons:

When you chat with your AI assistant, you are primarily curious about what’s inside the link, not the link itself.

We have more data for fine-tuning our summarization LLM.

After the documents are crawled, they are standardized to Markdown format (as well) and added to our existing collection.

We track the source of each document within the metadata, and because the documents are in Markdown format regardless of their source, we can treat the Notion and crawled documents equally and store them in the same collection. This will make our lives 100 times easier during future processing steps.

Formatting everything in Markdown is critical because it ensures the data is standardized and more straightforward to process downstream in the pipeline. This standardization is very important because the data will later be used for RAG (Retrieval Augmented Generation) and to fine-tune a summarization large language model (LLM).


Figure 2: The architecture of the ETL data pipeline
Now that the data has been augmented, ensuring its quality is essential. We compute a quality score for each document. This quality score is a number between 0 and 1, which you can use to filter the documents based on the target quality you are looking for.

You know the famous saying regarding AI systems: “Trash in, trash out.”

Hence, this is a critical step for fine-tuning high-quality LLMs and doing RAG correctly.

We want to filter the data differently depending on whether we use it for RAG or fine-tuning. Thus, we store all documents with the quality score in the metadata. This allows downstream pipelines to decide what to filter.

Ultimately, the standardized data is loaded into a NoSQL database and ready for consumption by the feature pipelines, which will further process it for RAG and fine-tuning.

It is a good practice to create a snapshot of the data between the data and AI layers. This decouples the two, allowing you to run the data pipeline once and experiment further with your AI architecture. You can extend this by versioning your data sources and making the data available across your company (instead of just your project).

As before, to draw a parallel to the industry, this is similar to how a data warehouse, such as Big Query, connects the dots between the data and AI systems.

The last piece of the puzzle is our MLOps framework. For our use case, we picked ZenML. Through ZenML, we will manage all our offline pipelines, such as the data collection, ETL pipeline and feature engineering pipelines.

But why do we need an MLOps framework in the first place? Let’s dig into this in the next section.

2. Understanding MLOps frameworks for managing ML pipelines
MLOps frameworks are powerful tools that allow you to easily manage, schedule, track, and deploy your ML pipelines.

Metaflow and ZenML are popular MLOps frameworks. They are optimized for long-running ML jobs, tracking metadata and output artifacts for reproducibility and setting up complex environments required for training and inference.

One core component of these MLOps frameworks is orchestrating offline ML pipelines. There is a fine line between data and ML orchestrators. Popular data orchestrators include Airflow and Prefect, which are optimized for running multiple small units in parallel.

However, these data engineering tools are not built with ML as their first citizen. For example, they don’t include robust features for tracking and versioning output artifacts. In reality, they started rolling out ML-related features, but you will quickly realize they are forced and don’t fit naturally into their SDKs.

For our course, we chose ZenML because it can quickly run locally in dev mode, has a beautiful UI, and has an intuitive Python SDK.

It also supports all the requirements for most ML projects, such as model management and tracking configuration, metadata, and output artifacts per pipeline.

It also supports infrastructure management for all the popular cloud services such as AWS, GCP, Azure, and more. They recently introduced new features that allow you to quickly deploy all your pipelines using Terraform (Infrastructure as Code) or directly deploying them from their dashboard with zero code involved in the process (the heaven for Data Scientists).

To conclude, we need an MLOps framework to easily track, reproduce, schedule, and deploy all our offline ML pipelines without reinventing the wheel.

Enough with the theory. Let’s quickly take a look at how ZenML works.

For example, Figure 3 shows ZenML’s dashboard with all the pipelines we’ve run in their latest state.


Figure 3: ZenML’s pipelines dashboard.
If we zoom in, for example, in the ETL pipeline, we can see all the previous runs with essential details, such as the infrastructure (“Stack”) in which they ran, as seen in Figure 4.


Figure 4: ZenML’s ETL pipeline dashboard.
Ultimately, if we click on a specific pipeline run, we can see the whole Directed Acyclic Graph (DAG) with all its steps. If it fails, we can see the steps it failed at. Also, as seen in Figure 5, we can easily visualize and track the output of each step.


Figure 5: Visualising the ETL pipeline run in ZenML.
There is much more to ZenML. However, to avoid creating a section that sounds like documentation, we will highlight its other features while building our AI assistant.

As we said in the previous section, this course will start with the ETL data pipeline, which will use our precomputed Notion dataset. Hence, let’s implement it in ZenML.

What does our ETL data pipeline look like when implemented in ZenML?
The heart of our pipeline is the etl() function, decorated with ZenML's @pipeline decorator (found at pipelines/etl.py). This function orchestrates the entire data flow, accepting configuration parameters that control its behavior - from specifying data directories to controlling parallel processing and quality scoring settings:

@pipeline
def etl(
    data_dir: Path,
    load_collection_name: str,
    to_s3: bool = False,
    max_workers: int = 10,
    quality_agent_model_id: str = ""gpt-4o-mini"",
    quality_agent_mock: bool = True,
) -> None:
The pipeline's workflow begins by setting up the data paths. We establish two key directories: one for reading the raw Notion data and another for storing the processed results:

    notion_data_dir = data_dir / ""notion""
    crawled_data_dir = data_dir / ""crawled""
The primary data processing flow consists of three major steps. First, we read the documents from the disk. Then, we crawl every link found in each document. Next, we enhance the documents with quality scores:

    documents = read_documents_from_disk(
        data_directory=notion_data_dir, nesting_level=1
    )
    crawled_documents = crawl(documents=documents, max_workers=max_workers)
    enhanced_documents = add_quality_score(
        documents=crawled_documents,
        model_id=quality_agent_model_id,
        mock=quality_agent_mock,
        max_workers=max_workers,
    )
The final stage of our pipeline handles data persistence. We save the enhanced documents to disk, optionally upload them to S3, and finally load them into MongoDB for downstream processing:

    save_documents_to_disk(documents=enhanced_documents, output_dir=crawled_data_dir)
    if to_s3:
        upload_to_s3(
            folder_path=crawled_data_dir,
            s3_prefix=""second_brain_course/crawled"",
            after=""save_documents_to_disk"",
        )
    ingest_to_mongodb(
        models=enhanced_documents,
        collection_name=load_collection_name,
        clear_collection=True,
    )
In Figure 6, we can see what the ETL pipeline looks like in ZenML:


Figure 6: An ETL pipeline run from ZenML.
In future sections of the article, we will zoom in on each step and understand how it works.

One last key feature of ZenML is that it can be configured through YAML configuration files (one per pipeline). This allows you to easily configure each pipeline run without touching the code. Most importantly, you can track and version the configuration of each pipeline run, which is critical for reproducibility and debugging.

Let’s look at it in more detail (found under configs/etl.yaml):

parameters:
  data_dir: data/
  load_collection_name: raw
  to_s3: false
  max_workers: 4
  quality_agent_model_id: gpt-4o-mini
  quality_agent_mock: false
As you can see, it’s a YAML file that is one-on-one with the pipeline Python function parameters. As the pipeline function acts as the entry point to your application, it makes sense to be able to configure it from a clean YAML file that can be easily tracked by git instead of tweaking the values from the CLI.

If you are curious to learn more about ZenML, they have some fantastic guides while also learning production MLOps and LLMOps:

More on ZenML

3. Exploring the collected Notion data
In Lesson 1, we explored how our data looks directly in Notion. However, as we start working with it only after our data collection pipeline collects it, we have to visualize how the data we ingest from S3 looks like, as that is what we will work with.

The data is stored in JSON, containing the content of the Notion document in Markdown, along with its metadata and embedded URLs. Here is one sample:

{
    ""id"": ""8eb8a0ed6afffaa581ef6dff9b3eec17"",
    ""metadata"": {
        ""id"": ""8eb8a0ed6afffaa581ef6dff9b3eec17"",
        ""url"": ""https://www.notion.so/Training-Fine-tuning-LLMs-8eb8a0ed6afffaa581ef6dff9b3eec17"",
        ""title"": ""Training & Fine-tuning LLMs"",
        ""properties"": {
            ""Type"": ""Leaf""
        }
    },
    ""parent_metadata"": {
        ""id"": ""6cfa25bcea00377355cfe21f7dfaadff"",
        ""url"": """",
        ""title"": """",
        ""properties"": {}
    },
    ""content"": ""# Resources [Community]\n\n\t<child_page>\n\t# Number of samples for fine-tuning based on general, domain, task-specific 
... # The rest of the document in Markdown format
""
    ""content_quality_score"": null,
    ""summary"": null,
    ""child_urls"": [
        ""https://github.com/huggingface/trl/"",
        ""https://www.linkedin.com/company/liquid-ai-inc/"",
        ""https://github.com/unslothai/unsloth/"",
        ""https://arxiv.org/abs/2106.09685/"",
        ""https://paperswithcode.com/sota/code-generation-on-humaneval/"",
        ""https://github.com/axolotl-ai-cloud/axolotl/"",
        ""https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/"",
        ""https://arxiv.org/abs/2305.14314/""
}
Every document is stored in its own JSON following the same structure:

metadata: containing the source URL plus other details about the source

parent_metadata: containing the parent’s URL, plus other details about the parent (if empty, it has no parent)

content: the actual content in Markdown format

child_urls: all the URLs that were found in the document’s content (notice how diverse the links are)

The next step is to load this data into Python while ensuring that each JSON file is valid (has the expected structure and data types). The preferred method for this is using Pydantic.

How do we model this data into a Pydantic class?
Let’s see how we can model our data using Pydantic, the go-to Python package for defining modern data structures in Python. But first, let’s make a quick analogy to LangChain.

When working with LangChain, one of the fundamental building blocks is the Document class. Let's explore how we can implement our own version that builds upon LangChain's concept while respecting our custom functionality and needs.

Our Document class maintains LangChain's core principle of combining content with metadata while extending it with additional features. Each document has its unique identifier, content, and structured metadata, plus we've added support for document hierarchies, quality assessment, and summarization.

class Document(BaseModel):
    id: str = Field(default_factory=lambda: utils.generate_random_hex(length=32))
    metadata: DocumentMetadata
    parent_metadata: DocumentMetadata | None = None
    content: str
    content_quality_score: float | None = None
    summary: str | None = None
    child_urls: list[str] = Field(default_factory=list)

    @classmethod
    def from_file(cls, file_path: Path) -> ""Document"":
        json_data = file_path.read_text(encoding=""utf-8"")
        return cls.model_validate_json(json_data)

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, Document):
            return False
        return self.id == other.id

    def __hash__(self) -> int:
        return hash(self.id)
While LangChain uses a simple dictionary for metadata, we've created a dedicated DocumentMetadata class. This structured approach ensures consistent metadata across our pipeline and provides better type safety:

class DocumentMetadata(BaseModel):
    id: str
    url: str
    title: str
    properties: dict
By storing the source URL and the parent’s source URL within the metadata while doing RAG, we can show the user the source used as context and where the source originates from. For example, we can show the user from which Notion database the link was accessed and what link was crawled.

One last thing to highlight is that storing the parent_metadata and child_urls fields can extend this to multi-hoping algorithms simulating a GraphDB structure. We won’t do that in our course, but it’s good to know that you don’t necessarily need a GraphDB to do GraphRAG. In cases where you need parents or children from only 2-3 levels relative to your data point, modeling your data using relationships is good enough to get started.

Before we discuss the implementation, we need to review some basic requirements for crawling.

4. Looking into requirements for crawling
So, how does crawling work? Essentially, it's about automatically visiting web pages, extracting the content, and following links to discover more pages. This process can be very complex because every website has a different structure, and there are many ways to present content. You'll need to ensure you're able to handle this complexity.

Before crawling any website, you must check the site’s crawling limitations. You can do that by adding /robots.txt to the end of the website's URL. This file tells you which parts of the website are off-limits for web crawlers. Respecting these rules is essential since they protect the website from overloading and ensure you’re not crawling sensitive information.


Figure 7: Result after accessing “https://www.youtube.com/robots.txt”
Now, if you want to crawl a website, you’ll also need to know all the pages you need to visit. You might think you can start from a home page and follow all the links. However, you'll soon discover this approach can be inefficient and may not capture all website pages.

That’s why many websites provide a sitemap that lists all their pages. The sitemap is usually added to better index the site for search engines (which also crawl it), but we can also leverage it to get a list of recurrent links we can crawl easily.

You can usually find this sitemap by adding /sitemap.xml to the end of the website's URL. This file gives you a structured list of all the website’s sub-URLs, which makes it a lot easier to do a recursive crawl, which means you can follow all the links on the website.


Figure 8: Result after accessing “https://neptune.ai/sitemap.xml”
Now, you need a tool to do all this.

We will use Crawl4AI for crawling, an open-source web crawling framework specifically designed to scrape websites and format the output for LLMs to understand.

The tool has built-in HTML to Markdown conversion, which is perfect for our needs. Crawl4AI is designed to be efficient, fast, and easy to set up. It can handle things like proxies, session management, and removing irrelevant content, which are not easy to handle.

5. Implementing crawling
How can we apply these principles to our crawling algorithm?

Our code is educative and harmless. Hence, to keep things simple, we will skip checking the robots.txt file. But it’s super important to remember this when working with real-world products.

Also, to avoid working with too many links, we will skip checking the sitemap.xml file and stick to the links found directly on our Notion pages. However, we could easily augment our dataset by accessing the sitemap.xml file of each link we use from Notion, expanding our dataset exponentially.

With that in mind, let’s dig into the implementation.

At the heart of our crawling step, we have a ZenML step that orchestrates the crawling process (which is called from the ZenML pipeline):

@step
def crawl(
    documents: list[Document], max_workers: int = 10
) -> Annotated[list[Document], ""crawled_documents""]:
    crawler = Crawl4AICrawler(max_concurrent_requests=max_workers)
    child_pages = crawler(documents)

    augmented_pages = documents.copy()
    augmented_pages.extend(child_pages)
    augmented_pages = list(set(augmented_pages))
To track our crawling progress and provide insights into the pipeline's performance, we add metadata about the number of documents processed:

    step_context = get_step_context()
    step_context.add_output_metadata(
        output_name=""crawled_documents"",
        metadata={
            ""len_documents_before_crawling"": len(documents),
            ""len_documents_after_crawling"": len(augmented_pages),
            ""len_documents_new"": len(augmented_pages) - len(documents),
        },
    )

    return augmented_pages
This metadata is attached to the crawled_documents output artifact and can be visualized from ZenML’s dashboard. As shown in Figure 9, it helps us monitor and debug each pipeline run.


Figure 9: Metadata of the `crawled_documents` artifact.
Now, let's look at our Crawl4AICrawler class implementation, which leverages the powerful features of Crawl4AI. This crawler is designed to handle concurrent web crawling efficiently while providing clean, LLM-ready output in Markdown format.

The crawler class under the hood uses Crawl4AI's AsyncWebCrawler with its sophisticated browser and crawler configurations. As the class is called from the ZenML step, which runs in a Python synchronous process, and the crawler uses an async method, we must define and manage an async loop internally. Using async, it’s a powerful mechanism in Python to manage concurrently I/O dependent processes such as crawling or API requests:

class Crawl4AICrawler:
    def __init__(self, max_concurrent_requests: int = 10) -> None:
        self.max_concurrent_requests = max_concurrent_requests

    def __call__(self, pages: list[Document]) -> list[Document]:
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            return asyncio.run(self.__crawl_batch(pages))
        else:
            return loop.run_until_complete(self.__crawl_batch(pages))
The core crawling logic happens in the __crawl_batch method. We use Crawl4AI's CacheMode.BYPASS to ensure fresh content:

    async def __crawl_batch(self, pages: list[Document]) -> list[Document]:
        process = psutil.Process(os.getpid())
        start_mem = process.memory_info().rss
        
        semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        all_results = []

        async with AsyncWebCrawler(cache_mode=CacheMode.BYPASS) as crawler:
            for page in pages:
                tasks = [
                    self.__crawl_url(crawler, page, url, semaphore)
                    for url in page.child_urls
                ]
                results = await asyncio.gather(*tasks)
                all_results.extend(results)

        successful_results = [result for result in all_results if result is not None]
Finally, each URL is processed by the __crawl_url method, which utilizes Crawl4AI's powerful content extraction and Markdown generation capabilities. As we kick off all the jobs simultaneously, we control how many concurrent requests we can run simultaneously using the semaphore Python object. This is useful for managing your computer’s resources or API limits:

    async def __crawl_url(
        self,
        crawler: AsyncWebCrawler,
        page: Document,
        url: str,
        semaphore: asyncio.Semaphore,
    ) -> Document | None:
        async with semaphore:
            result = await crawler.arun(url=url)
            await asyncio.sleep(0.5)  # Rate limiting

            if not result or not result.success or result.markdown is None:
                return None

            child_links = [
                link[""href""]
                for link in result.links[""internal""] + result.links[""external""]
            ]
            title = result.metadata.pop(""title"", """") if result.metadata else """"

            return Document(
                id=utils.generate_random_hex(length=32),
                metadata=DocumentMetadata(
                    id=document_id,
                    url=url,
                    title=title,
                    properties=result.metadata or {},
                ),
                parent_metadata=page.metadata,
                content=str(result.markdown),
                child_urls=child_links,
            )
Crawl4AI is particularly well-suited for our needs as it's designed to be LLM-friendly. It generates clean Markdown output directly and handles all the formatting complexity behind the scenes.

The next step from our ETL pipeline is to compute the quality score.

6. Computing the quality score
Assessing content quality is crucial when processing documents in data pipelines. RAG systems can be incredibly powerful as long they use high-quality context. Also, when it comes to fine-tuning LLMs, using high-quality samples is the most critical aspect in which you should invest.

Thus, let's explore a sophisticated two-stage system that combines quick heuristic rules with an advanced LLM-based evaluation approach.

As parsing the document using an LLM can quickly increase the latency and costs of your system, we try first to do our best and compute the quality score using a set of heuristics. The key is to use heuristics where we are 100% sure we can score the documents. Next, for more complex and nuanced scenarios, we use the LLM.

Our ZenML pipeline step orchestrates the quality scoring process, starting with fast heuristics and falling back to the LLM-based method when needed:

@step
def add_quality_score(
    documents: list[Document],
    model_id: str = ""gpt-4o-mini"",
    mock: bool = False,
    max_workers: int = 10,
) -> Annotated[list[Document], ""scored_documents""]:
    heuristic_quality_agent = HeuristicQualityAgent()
    scored_documents: list[Document] = heuristic_quality_agent(documents)

    scored_documents_with_heuristics = [
        d for d in scored_documents if d.content_quality_score is not None
    ]
    documents_without_scores = [
        d for d in scored_documents if d.content_quality_score is None
    ]

    quality_agent = QualityScoreAgent(
        model_id=model_id, mock=mock, max_concurrent_requests=max_workers
    )
    scored_documents_with_agents: list[Document] = quality_agent(
        documents_without_scores
    )
The heuristic agent provides a quick first pass by analyzing URL density - a simple yet effective way to filter out low-quality documents that are mostly just collections of links:

class HeuristicQualityAgent:
    def __call__(self, documents: list[Document]) -> list[Document]:
        ...

    def __score_document(self, document: Document) -> Document:
        if len(document.content) == 0:
            return document.add_quality_score(score=0.0)

        url_based_content = sum(len(url) for url in document.child_urls)
        url_content_ratio = url_based_content / len(document.content)

        if url_content_ratio >= 0.7:
            return document.add_quality_score(score=0.0)
        elif url_content_ratio >= 0.5:
            return document.add_quality_score(score=0.2)
        return document
For more nuanced evaluation, we define a structured response format to ensure consistent scoring from our LLM:

class QualityScoreAgent:
    SYSTEM_PROMPT_TEMPLATE = """"""You are an expert judge tasked with evaluating the quality of a given DOCUMENT.

Guidelines:
1. Evaluate the DOCUMENT based on generally accepted facts and reliable information.
2. Evaluate that the DOCUMENT contains relevant information and not only links or error messages.
3. Check that the DOCUMENT doesn't oversimplify or generalize information in a way that changes its meaning or accuracy.

Analyze the text thoroughly and assign a quality score between 0 and 1, where:
- **0.0**: The DOCUMENT is completely irrelevant containing only noise such as links or error messages
- **0.1 - 0.7**: The DOCUMENT is partially relevant containing some relevant information checking partially guidelines
- **0.8 - 1.0**: The DOCUMENT is entirely relevant containing all relevant information following the guidelines

It is crucial that you return only the score in the following JSON format:
{{
    ""score"": <your score between 0.0 and 1.0>
}}

DOCUMENT:
{document}
""""""
The QualityScoreAgent implements sophisticated batch processing with concurrency control and rate limiting (similar to what you have seen in the crawler class). Calling OpenAI’s API in batch mode can quickly hit its request limits. To find a balance between process time and successfully scoring each document, we first called the API for each document using a shorter wait time between API calls. Next, for each API request that failed, we retry it with a longer wait period:

    async def __get_quality_score_batch(
        self, documents: list[Document]
    ) -> list[Document]:
        scored_documents = await self.__process_batch(documents, await_time_seconds=7)
        documents_with_scores = [
            doc for doc in scored_documents if doc.content_quality_score is not None
        ]
        documents_without_scores = [
            doc for doc in scored_documents if doc.content_quality_score is None
        ]

        # Retry failed documents with increased await time
        if documents_without_scores:
            retry_results = await self.__process_batch(
                documents_without_scores, await_time_seconds=20
            )
            documents_with_scores += retry_results

        return scored_documents
For each document, we format the input prompt, make the API request, and wait for a given period to avoid API request limits:

    async def __get_quality_score(
        self,
        document: Document,
        semaphore: asyncio.Semaphore | None = None,
        await_time_seconds: int = 2,
    ) -> Document | None:
        async def process_document() -> Document:
            input_user_prompt = self.SYSTEM_PROMPT_TEMPLATE.format(
                document=document.content
            )

            response = await acompletion(
                model=self.model_id,
                messages=[
                    {""role"": ""user"", ""content"": input_user_prompt},
                ],
                stream=False,
            )
            await asyncio.sleep(await_time_seconds)  # Rate limiting

            raw_answer = response.choices[0].message.content
            quality_score = self._parse_model_output(raw_answer)
            return document.add_quality_score(score=quality_score.score)
The last step is to check that the LLM’s output follows our desired format by leveraging a Pydantic class:

class QualityScoreResponseFormat(BaseModel):
    """"""Format for quality score responses from the language model.

    Attributes:
        score: A float between 0.0 and 1.0 representing the quality score.
    """"""

    score: float


def _parse_model_output(
        self, answer: str | None
    ) -> QualityScoreResponseFormat | None:
        if not answer:
            return None

        try:
            dict_content = json.loads(answer)
            return QualityScoreResponseFormat(
                score=dict_content[""score""],
            )
        except Exception:
            return None
Here are two important observations we still have to point out:

Instead of directly using OpenAI’s API, we used litellm, a wrapper over multiple popular LLM APIs, such as OpenAI, Antrophic, Cohere, and more. We recommend using them, as they allow you to experiment easily with various providers without touching the code.

To further optimize the system by reducing costs and the chance of request-limit errors, you can use OpenAI’s batch API. In this way, you can send multiple documents per request.

The last step is to see how we can load the processed documents to our NoSQL MongoDB.

7. Loading the standardized data to a NoSQL database
When building data pipelines, you often need a reliable way to store and retrieve your processed documents. This type of storage is often known as the data warehouse.

In our use case, out of simplicity, we used a NoSQL MongoDB database, which is not a data warehouse by the book, but for text data, it gets the job done well.

We implemented the MongoDBService class to interact with MongoDB. This class provides a generic interface for handling any Python structure that follows a Pydantic model structure.

The class is designed to be flexible, using Python's generic typing to work with any Pydantic model. This means you can use it to store different types of Python data structures while maintaining type safety and validation:

T = TypeVar(""T"", bound=BaseModel)

class MongoDBService(Generic[T]):
    def __init__(
        self,
        model: Type[T],
        collection_name: str,
        database_name: str = settings.MONGODB_DATABASE_NAME,
        mongodb_uri: str = settings.MONGODB_URI,
    ) -> None:
The ingest_documents() method is where the magic happens. It takes your Pydantic models and safely stores them in MongoDB. The method includes validation and proper error handling to ensure your data is stored correctly:

    def ingest_documents(self, documents: list[T]) -> None:
        try:
            if not documents or not all(
                isinstance(doc, BaseModel) for doc in documents
            ):
                raise ValueError(""Documents must be a list of Pycantic models."")

            dict_documents = [doc.model_dump() for doc in documents]

            # Remove '_id' fields to avoid duplicate key errors
            for doc in dict_documents:
                doc.pop(""_id"", None)

            self.collection.insert_many(dict_documents)
            logger.debug(f""Inserted {len(documents)} documents into MongoDB."")
        except errors.PyMongoError as e:
            logger.error(f""Error inserting documents: {e}"")
            raise
That’s it. We are finally done implementing our data pipeline.

The last step is to look at how we can run the code.

8. Running the code
The best way to set up and run the code is through our GitHub repository, where we have documented everything you need. We will keep these instructions only in our GitHub to avoid having the documentation scattered throughout too many places (which is a pain to maintain and use).

But to give a sense of the “complexity” of running the code, you have to run ONLY the following commands using Make:

make local-infrastructure-up    # 1. Spin up the infrastructure
make download-notion-dataset    # 2. Download the Notion dataset
make etl-pipeline               # 3. Run the ETL pipeline through ZenML
That’s all it takes to crawl and compute the quality score for all the documents.

While the ETL pipeline is running, you can visualize it on ZenML’s dashboard by typing in your browser: http://127.0.0.1:8237

Conclusion
This lesson taught you what it takes to build the data pipelines to implement a Second Brain AI assistant.

We walked you through the architecture of the data pipelines, what it takes to manage it with an MLOps framework such as ZenML, what the Notion data looks like, and what you should consider when crawling.

We learned how to crawl custom links at scale using Crawl4AI, compute a quality score for all your documents using heuristics and LLMs, and create a snapshot of the data in a NoSQL MongoDB database.

Lesson 3 (WIP) will teach you how to generate a high-quality summarization instruction dataset using distillation and how to load it to Hugging Face.

💻 Explore all the lessons and the code in our freely available GitHub repository.

If you have questions or need clarification, feel free to ask. See you in the next session!

Whenever you’re ready, there are 3 ways we can help you:
Perks: Exclusive discounts on our recommended learning resources

(live courses, self-paced courses, learning platforms and books).

The LLM Engineer’s Handbook: Our bestseller book on mastering the art of engineering Large Language Models (LLMs) systems from concept to production.

Free open-source courses: Master production AI with our end-to-end open-source courses, which reflect real-world AI projects, covering everything from system architecture to data collection and deployment.

References
Decodingml. (n.d.). GitHub - decodingml/second-brain-ai-assistant-course. GitHub. https://github.com/decodingml/second-brain-ai-assistant-course

MongoDB: the Developer Data platform. (n.d.). MongoDB. https://www.mongodb.com

Pydantic. (n.d.). GitHub - pydantic/pydantic: Data validation using Python type hints. GitHub. https://github.com/pydantic/pydantic

Unclecode. (n.d.). GitHub - unclecode/crawl4ai: 🚀🤖 Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. GitHub. https://github.com/unclecode/crawl4ai

ZenML - MLOps framework for infrastructure agnostic ML pipelines. (n.d.). https://zenml.io

Cole Medin. (2025, January 13). Turn ANY Website into LLM Knowledge in SECONDS [Video]. YouTube. https://www.youtube.com/watch?v=JWfNLF_g_V0",,,
7,https://techcrunch.com/,2025.02.11,https://techcrunch.com/2025/02/11/how-musks-97-4b-bid-could-gum-up-openais-for-profit-conversion/,How Musk’s $97.4B bid could gum up OpenAI’s for-profit conversion,"On Monday, Elon Musk, the world’s richest man, offered to buy the nonprofit that effectively governs OpenAI for $97.4 billion. The unsolicited buyout would be financed by Musk’s AI company, xAI, and a consortium of outside investors, per a letter sent to California and Delaware’s attorneys general.

OpenAI CEO Sam Altman quickly dismissed Musk’s bid, and took it as a chance to publicly dunk on him.

“no thank you, but we will buy Twitter for $9.74 billion if you want,” Altman wrote in a post on X just hours after reports emerged of Musk’s offer for OpenAI. Musk owns X, the social network formerly known as Twitter; he paid roughly $44 billion for it in October 2022. 

The two have a history. Musk is an OpenAI co-founder, and both he and xAI are currently involved in a lawsuit that alleges that OpenAI engaged in anticompetitive behavior, among other things.

But Altman’s rejection of a $97.4 billion takeover offer is more complicated than just saying “no thanks,” according to corporate governance experts who spoke with TechCrunch.

Stalling OpenAI’s nonprofit conversion
OpenAI CEO Sam Altman
OpenAI CEO Sam Altman visits “Making Money With Charles Payne” at Fox Business Network Studios on December 04, 2024 in New York City.
Image Credits:Mike Coppola / Getty Images
For background, OpenAI was founded as a nonprofit before transitioning to a “capped-profit” structure in 2019. The nonprofit is the sole controlling shareholder of the capped-profit OpenAI corporation, which retains formal fiduciary responsibility to the nonprofit’s charter. 

OpenAI is now in the process of restructuring — this time to a traditional for-profit company, specifically a public benefit corporation — in a bid to raise much more capital. But Musk — who is notorious for drowning his enemies in legal troubles — may have stalled the transition and raised the price of OpenAI’s nonprofit with his bid.

Delaware and California‘s attorneys general have requested more information from the ChatGPT maker about its plans to convert to a for-profit benefit corporation. The situation also forces it to consider outside bids seriously.

OpenAI’s board will almost certainly refuse the bid, but Musk has been setting the stage for future legal and regulatory battles. He’s already attempting to stall OpenAI’s for-profit conversion via an injunction, for instance. The bid appears to be an alternative offer, of sorts. 

Now, OpenAI’s board will have to demonstrate that it’s not underselling OpenAI’s nonprofit by handing the nonprofit’s assets, including IP from OpenAI’s proprietary research, to an insider (e.g. Sam Altman) for a steep discount.

“Musk is throwing a spanner into the works,” said Stephen Diamond, a lawyer who represented Musk’s opponents in corporate governance battles at Tesla, in an interview with TechCrunch. “He’s exploiting the fiduciary obligation of the nonprofit board to not undersell the asset. [Musk’s bid] is something OpenAI has to pay attention to.”

OpenAI is said to be gearing up for a funding round that would value its for-profit arm at $260 billion. The Information reports that OpenAI’s nonprofit is slated to get a 25% stake in OpenAI’s for-profit.

With his bid, Musk has signaled there’s at least one group of investors willing to pay a sizable premium for OpenAI’s nonprofit wing. That puts the board of directors in a tight spot. 

Grounds for rejection
Still, just because Musk threw out an eye-popping offer doesn’t mean that OpenAI’s nonprofit has to accept.

Corporate law gives tremendous authority to incumbent boards to protect against unsolicited takeover bids, according to David Yosifon, a Santa Clara University professor of corporate governance law.

OpenAI could make the case that Musk’s bid is a hostile takeover attempt given that Musk and Altman aren’t the best of friends.

The company could also argue that Musk’s offer isn’t credible because OpenAI is already in the midst of a corporate restructuring process.

Another approach OpenAI could take would be challenging Musk on whether he has the funds. As The New York Times notes, Musk’s wealth is largely tied to his Tesla stock, meaning that Musk’s investment partners would have to supply much of the $97.4 billion total.

OpenAI’s board may need to review Musk’s offer to fully asses whether it aligns with the nonprofit’s mission, not just specific financial or strategic goals, according to Scott Curran, the former general counsel to the Clinton Foundation. That means Musk’s offer could be weighed against OpenAI’s mission: “to ensure that artificial general intelligence – AI systems that are generally smarter than humans – benefits all of humanity.”

“When Altman posted that response [on X], that was probably done without legal guidance,” Yosifon said. “It’s not good for a regulator to see that kind of dismissive, knee-jerk tweet.”

Raising the value for OpenAI assets
The board is likely to side with Altman. Nearly all the directors joined after Altman was briefly fired, then rehired, by the nonprofit’s board in late 2023. Altman himself is also a board member.

If nothing else, Musk’s bid may raise the potential market value of the OpenAI nonprofit’s assets. That could force OpenAI to raise more capital than it originally anticipated, and complicate talks with the startup’s existing backers. It could also dilute the value of stakes held by OpenAI investors in the for-profit arm, including major partners such as Microsoft. 

That’s sure to anger Altman, who’s been working with investors for months to determine how to fairly compensate the nonprofit.

The gist is: OpenAI’s corporate restructuring plans just got more complex.

TechCrunch has an AI-focused newsletter! Sign up here to get it in your inbox every Wednesday.

Topics

AI
AI
ChatGPT
Elon Musk
nonprofit
OpenAI
sam altman
Startups",,,
8,https://www.gpters.org/,2025.01.22,https://www.gpters.org/nocode/post/issuing-inhouse-certificates-one-bve9DC05JG2LJus,클릭 한번으로 사내 증명서 발급하기 (1),"소개
직원수가 많은 회사에서 아르바이트 하다보니 각종 증명서를 발급하는 담당직원이 매번 각종 정보를 복사하거나 타이핑 하는걸 보면서 조금 편하게 해줄까 하는 생각이 들었습니다. 한번 만들어😁 🥰 두면 다양하게 활용이 가능하겠더군요.

(글쓰는 재주가 없어 정보 위주로 정리할게요)

진행 방법
사용도구: chatGPT와 구글 시트, 독스, 앱스 스크립트를 사용

gpt에게 구글시트와 독스 문서ID를 포함해주고 스크립트를 요청


사본
앱스 스크립트를 이용하여 아래 작성된 구글 독스와 시트에서 데이터를 입력받아 pdf를 구글 드라이브에 저장하는 자동화를 만들거야

아래 내용을 반영하여 앱스 스크립트를 작성하는 방법을 단계별로 상세하게 설명해줘 
 
구글 독스 1v-YXJ0R----------------KrH61xKs
구글 시트 1QFNF-------------------oELQnQ

구글 시트의 a열에서 체크박스에 체크가 입력되면 시트 행에 해당하는 내용을 구글 독스에 반영하여 pdf로 저장되도록 트리거 설정해줘  

구글 독스에는 이름, 생년월일은 표에 입력하고 
소속란에는 사업부와 부서를 입력
직급, 입사일자, 제출처를 그대로 입력하고 
증명서 하단 날짜는 파일 작성날짜를 입력해줘. 
작성시 준비한 시트와 문서는 아래 그림과 같습니다.

한국어 문자가 적힌 스프레드시트
ALT

한국어 비자 신청서 한국어 비자 신청서 한국어 비자 신청서 한국어 비자 신청서 한국어
ALT

스크립트에 있는 구글 시트와 독스 id를 확인하는 방법은 아래와 같습니다.


수정단계

gpt 답변을 단계별로 진행하면서 다양한 오류를 경험하고 수정하면서 하나씩 고쳐나갔습니다.

😢 그러나... 계속 되는 에러를 고민하다 알게된 것이 아웃풋인 pdf 파일을 저장할 공간을 지정해주지 않았더군요(혼자 바보라고 외쳤습니다 😮). 구글 드라이브 폴더 정보를 지정해 주어야 해당 폴더에 파일이 저장되더군요. 아래는 폴더 아이디를 확인하는 방법입니다.

한국사이트 스크린샷
ALT

폴더 아이디까지 추가한 후에야 파일이 생성되기 시작하더군요^^;;; 한 시간 넘게 걸쳐 수정을 반복하고, 트리거 조정하고 하면서 조금씩 완성이 되어갔습니다.

아래는 트리거 설정방법입니다

한국어로 된 Google 검색 페이지의 스크린샷
ALT

완료단계

약 두시간에 걸친 작업 끝에 완성된 pdf 파일입니다.


홍길동 (1).pdf
51.18KB


마지막으로 완성된 앱스 스크립트 코드 첨부합니다.


사본
const SHEET_ID = '-----------'; // 구글시트 iD
const DOC_TEMPLATE_ID = '--------'; //구글독스 양식 ID
const FOLDER_ID = '--------'; // PDF 저장 폴더 ID 주소중 project/.../edit ...부분


function onEdit(e) {
  if (!e) {
    Logger.log('onEdit 함수는 직접 실행되지 않습니다.');
    return;
  }

  const sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet();
  const range = e.range;

  Logger.log(`수정된 셀: ${range.getA1Notation()}`);
  Logger.log(`수정된 값: ${range.getValue()}`);

  if (range.getColumn() === 1 && range.getValue() === true) {
    Logger.log('체크박스가 선택되었습니다.');
    const row = range.getRow();
    const rowData = sheet.getRange(row, 2, 1, 7).getValues()[0];
    Logger.log('해당 행 데이터: ' + rowData.join(', '));

    const [name, dob, businessUnit, department, position, joinDate, submitto] = rowData;

    try {
      const doc = createDocument(name, dob, businessUnit, department, position, joinDate, submitto);
      const pdf = convertToPDF(doc, name);
      Logger.log(`PDF 파일 생성 완료: ${pdf.getName()}`);
    } catch (error) {
      Logger.log(`오류 발생: ${error.message}`);
    }
  }
}

function createDocument(name, dob, businessUnit, department, position, joinDate, submitto) {
  Logger.log('문서 생성 중...');
  const template = DriveApp.getFileById(DOC_TEMPLATE_ID);
  const newDoc = template.makeCopy(`증명서_${name}`, DriveApp.getFolderById(FOLDER_ID)).getId();
  Logger.log(`복사된 문서 ID: ${newDoc}`);
  const doc = DocumentApp.openById(newDoc);
  const body = doc.getBody();

  body.replaceText('{{이름}}', name);
  body.replaceText('{{생년월일}}', dob);
  body.replaceText('{{소속}}', `${businessUnit} / ${department}`);
  body.replaceText('{{직급}}', position);
  body.replaceText('{{입사일자}}', formatDate(new Date(joinDate)));
  body.replaceText('{{제출처}}', submitto);
  body.replaceText('{{작성일자}}', formatDate(new Date()));

  doc.saveAndClose();
  return doc;
}

function convertToPDF(doc, name) {
  Logger.log('PDF 변환 중...');
  const docFile = DriveApp.getFileById(doc.getId());
  const pdfBlob = docFile.getAs('application/pdf');
  const folder = DriveApp.getFolderById(FOLDER_ID);
  const pdfFile = folder.createFile(pdfBlob.setName(`${name}.pdf`));
  docFile.setTrashed(true);
  return pdfFile;
}

function formatDate(date) {
  if (Object.prototype.toString.call(date) === '[object Date]') {
    return Utilities.formatDate(date, Session.getScriptTimeZone(), 'yyyy-MM-dd');
  }
  return date;
}
결과와 배운 점
이전에 클로드를 이용할 때와 비교하면, gpt가 코딩에서는 설명과 에러수정에서 조금 부족한 느낌입니다. 여유되시면 스크립트는 클로드 추천합니다 😄

다음주는 증명서 신청을 받는 구글폼을 추가하고 이후에는 발행되는 증명서를 이메일로 발송하는 부분까지 추가해 보려합니다.",,,
9,https://www.nb-data.com/,2025.02.07,https://www.nb-data.com/p/enhance-rag-accuracy-with-corrective?utm_source=post-email-title&publication_id=37262&post_id=156582633&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email,Enhance RAG Accuracy with Corrective-RAG (CRAG),"Retrieval-augmented generation (RAG) is a system that combines the data retrieval technique and LLM generation to produce an accurate response. Thus, RAG output will depend on the retrieval and generation parts.

RAG output accuracy depends on data quality, the retrieval module, and the generation model. Often, the crucial part that could make or break the RAG system is the retrieved document itself.

In the previous article, we discussed a few techniques that could enhance retrieval. We have previously discussed many techniques, which you can read below.

Although the retrieval process can still result in irrelevant or erroneous data, the Corrective-RAG (CRAG) framework emerged to help address these limitations.

Corrective-RAG introduces a mechanism for error detection and correction within the retrieval-augmented generation pipeline by identifying inaccurate retrieved documents.

This article will explore building a simple CRAG that evaluates the retrieved documents. The system will follow the diagram below, and the code is in the repository.

Enhance RAG Accuracy with Corrective-RAG (CRAG)

Introduction
As mentioned above, Corrective-RAG, or CRAG, is a framework for improving RAG results using error detection and correction steps. It was first introduced by Yan et al. (2024) paper, which explores where the lightweight retrieval evaluator can be used to assess the overall quality of retrieved documents and improve the robustness of generation.

Often, these steps are only performed within the retrieval steps to detect the error documents and perform correction steps. However, they can be extended to the generation step as well.

The technique is based upon a feedback loop that continuously evaluates the quality of retrieved documents and provides evaluation. Basically, we pass the document to the evaluator and perform a correction step if it doesn’t pass the evaluation.

Enhance RAG Accuracy with Corrective-RAG (CRAG)
The evaluation step can leverage advanced techniques such as confidence scoring, consistency checks, and contextual validation to detect potential errors.

Confidence scoring is a mechanism for evaluating the reliability or trustworthiness of a retrieved document or generated response by assigning a numerical score.

Consistency checks ensure that the retrieved information and the generated response are logically coherent and free from contradictions.

Contextual validation ensures that the retrieved information and generated response are accurate and contextually appropriate.

But how can we employ the evaluation technique above in the RAG system? There are many ways to do that, but one of the most used is the LLM-as-a-Judge evaluator.

The previous article taught us how the LLM-as-a-Judge works, but you can refresh the concept by reading the following article.

Of course, you can use a more rigid evaluation metric such as ROUGE, BLEU, or any score metrics that can be used. What is important in the CRAG framework is that there is an evaluation and a corrective step that we perform.

So, what are the benefits of using the CRAG framework? There are a few notable benefits, including but not limited to:

Improved Accuracy: By detecting and correcting errors in real time, CRAG significantly improves the factual accuracy of generated responses.

Better contextual response: CRAG ensures the generated responses are accurate and contextually appropriate.

Enhanced User Trust: By delivering more accurate, reliable, and contextually appropriate outputs, CRAG builds greater trust with users.

Those are some benefits you can expect by using the CRAG framework. Now, let’s try to build a simple CRAG framework.

Building CRAG
In the next step, we will see the CRAG pipeline that evaluates the context of the retrieved document and performs the correction step. Note that for simplicity purposes, we don’t use a feedback loop here to improve the retrieval result; instead, we perform the corrections step by searching the web.

We will use the one we performed in the following article to build the simple RAG system.


Non-Brand Data
Simple RAG Implementation With Contextual Semantic Search
Hi everyone! Cornellius here, back with another Lite series. This time, we’ll explore the advanced techniques and production methods of Retrieval-Augmented Generation (RAG)—tools that will be helpful for your use cases. I will make it into a long series, so stay tuned…
Read more
23 days ago · 11 likes · 2 comments · Cornellius Yudha Wijaya
Let’s start building the CRAG pipeline by adding an evaluation step. In this step, I will only use a simple LLM-as-a-Judge to evaluate whether the document is relevant to the query. The output will be either “yes” or “no”.

def grade_document(query, document):
    #Uses the Gemini model to decide if a document is relevant to the query.
    prompt = f""""""Query: {query}
Document: {document}
Is this document relevant to the query? Answer with ""yes"" or ""no"".""""""
    response = completion(
        model=""gemini/gemini-1.5-flash"",
        messages=[{""content"": prompt, ""role"": ""user""}],
        api_key=GEMINI_API_KEY
    )
    answer = response['choices'][0]['message']['content'].strip().lower()
    return ""yes"" if ""yes"" in answer else ""no""
For the correction step, we will only use a simple web search to retrieve additional context using Internet data.

def supplementary_retrieval(query):
    #Performs a web search using DuckDuckGo and returns the result as a string.
    search_tool = DuckDuckGoSearchRun()
    web_result = search_tool.invoke(query)
    return web_result
Lastly, we build the CRAG pipeline using all the components we have previously constructed. We have built our simple CRAG pipeline by combining the semantic search, evaluator, and correction.

def corrective_rag(query, top_k=2):
    # The main CRAG pipeline:
    #   1. Retrieve documents using semantic search.
    #   2. Grade each document using the evaluator.
    #   3. If no relevant document is found, perform a web search.

    # Step 1: Retrieve documents
    results = semantic_search(query, top_k=top_k)
    retrieved_docs = results.get(""documents"", [])
    print(""Initial retrieved documents:"")
    for doc in retrieved_docs:
        print(doc)

    # Step 2: Grade each document for relevance (Evaluation step)
    relevant_docs = []
    for doc in retrieved_docs:
        grade = grade_document(query, doc)
        print(f""Grading document (first 60 chars): {doc[:60]}... => {grade}"")
        if grade == ""yes"":
            relevant_docs.append(doc)

    # Step 3: If no relevant document is found, perform supplementary retrieval (Correction step)
    if not relevant_docs:
        print(""No relevant documents found; performing supplementary retrieval via web search."")
        supplementary_doc = supplementary_retrieval(query)
        relevant_docs.append(supplementary_doc)
    else:
        print(""Using relevant documents from the vector store."")

    # Ensure all elements in relevant_docs are strings
    context = ""\n"".join(["" "".join(doc) if isinstance(doc, list) else doc for doc in relevant_docs])

    # Step 4: Generate final answer using the combined context.
    final_answer = generate_response(query, context)
    return final_answer
You can then try to test the CRAG pipeline using the following code.

query = ""What is the insurance for car?""
final_answer = corrective_rag(query)
print(""Final Answer:"")
print(final_answer)
The result will depend on the retrieved document and the generation model.

As mentioned, you can build the CRAG pipeline even further by adding a more strenuous feedback loop and applying it in the generation module.

That’s all for now! I hope you all learn more about the CRAG framework.

Is there anything else you’d like to discuss? Let’s dive into it together!",,,
10,https://marvelousmlops.substack.com/,2025.02.05,https://marvelousmlops.substack.com/p/unicorn-and-rainbows-the-reality?utm_source=post-email-title&publication_id=1746193&post_id=156526517&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email,Unicorns and Rainbows: The Reality of Implementing AI in a Corporate,"Unicorns and Rainbows. Is it a metaphor? Is it a reality? Maybe both. Think of an unicorn dancing on top of a radiant rainbow. But, in fact, what does it mean?


Image generated by AI
Humanity has always been drawn to utopia — a perfect, idealized future where all problems are solved. Believing that the world is steadily marching toward this vision is tempting. In the AI landscape, the unicorn (you have noticed the 5th leg, right?) represents the elevated promises, wild imagination, and relentless hype that paint a picture of transformative, almost magical technology.

Marvelous MLOps Substack is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.


Upgrade to paid

The rainbow, however, represents the real world: entire potential but riddled with imperfections, inconsistencies, and systemic barriers.

Just like the stock market, AI has its declines and flows. Everything might seem to skyrocket, but a slight shift — technical debt, regulatory burdens, or enterprise realities — can send it crashing back to earth. The question is not whether AI is a transformative force (there is no doubt it is!) but whether we’re being realistic about its trajectory.

This article will discuss the reality of using AI in the enterprise environment, address technical debt, bridge knowledge gaps, and understand the herd effect that fuels the AI bubble. We aim to offer a realistic roadmap for businesses navigating the complex AI landscape by critically analyzing these factors.

1. The AI bubble
We have been in Data & AI for over 10 years. The AI bubble has never been so big. We have AI everywhere on our laptops, phones, and websites. The CEOs of Nvidia, Microsoft, Meta, and OpenAI are spreading a lot of news about revolutionary AI technology, how AI agents will replace humans, how we will reach AGI soon, and how we will have AI everywhere. We live in an AI bubble, and even though the technology is accurate, it is nontrivial to apply it to actual use cases and drive business value than advertised.

The technological advancements in the AI field are significant, and the value AI generates is real. However, there are still many gaps that people who try to build AI products see clearly. Two factors contribute to the AI bubble: knowledge and the herd effect. Somehow, they are tangential but different.

2. The Role of Knowledge Gaps
The gap between AI insiders and the general public is one of the hinds of the AI bubble. The saying “Knowledge is power” remains valid for AI within its development and implementation context.

People who are deeply invested in the development of AI are fully aware of the nuances, challenges, and limitations that come with the implementation of AI-based solutions.

On the other hand, AI outsiders are constantly awe-struck by the marketing terms associated with AI which presents an entirely different world of possibilities. This knowledge gap enables misconceptions to spread at an alarming rate, therefore making the hype of AI take precedence over the reality of what AI systems can offer.

3. Herd Effect: Fear of Missing Out (FOMO)
Another significant factor driving the AI hype is the herd effect or the Fear of Missing Out (FOMO).

As more companies invest in AI and tout their successes, others feel compelled to follow suit, fearing they’ll fall behind if they don’t adopt AI technologies. This rush often leads to deploying AI solutions without a thorough understanding of their applicability or potential ROI, further inflating the AI bubble. The result is a market saturated with AI buzzwords and solutions that may not deliver the promised transformative impact.

AI models (under AI models, we understand foundation models) are used everywhere, where a standard ML model should be used instead. This adds complexity and decreases reliability.

4. Back to basics
Most companies can not reliably bring standard machine learning models to production and lack monitoring practices.

Despite what many people think, workflows that include AI models are, on average, more complex to bring into production and monitor — even if you take the simplest scenario without RAG or finetuning involved — just call a 3rd party API.

In too many cases, we seem to have forgotten the basic principles of machine learning and blindly rely on what that API outputs. This is the danger of AI hype: AI has become accessible to everyone, and many software engineers treat it as just another API call.

What could go wrong? The data model has not changed, the code has not changed (and neither has the environment where it gets executed), and the version of the API has not changed. But this is the beauty of machine learning: even if everything in your control has not changed, the model can start performing poorly unexpectedly because data distribution has changed.

This does not just happen with standard machine learning models, it also happens with AI models — we just have less means to impact that behavior, and prompt finetuning becomes an essential part of the process.

5. Real-world, 2025
Experts say that 2025 will be the year of AI agents. But is it really true?

While the AI hype machine continues to boom, real-world adoption tells a different story. The promise of autonomous AI agents seamlessly operating across enterprises remains largely aspirational. The reality? AI in enterprise is still a work in progress — complex, expensive, and often misaligned with actual business needs.

Take BBVA, the Spanish bank that went all in on OpenAI’s technology. They deployed over 2,900 AI models to enhance productivity, yet integrating them into their existing systems turned out to be a logistical nightmare. AI doesn’t operate in a vacuum; it needs to connect with legacy infrastructure, existing workflows, and strict regulatory requirements. And that’s where reality bites — scaling AI across an enterprise is exponentially harder than rolling out a chatbot.

The UK government’s attempt to integrate AI into its welfare system faced significant limitations. At least six AI prototypes, designed to enhance staff training, improve job center services, and streamline disability benefit processing, were discontinued due to issues in scalability, reliability, and insufficient testing. Officials acknowledged several “frustrations and false starts,” highlighting the complexities involved in deploying AI within public services.

A study highlighted several obstacles in developing and deploying AI agents within enterprises. Security concerns were identified as a top challenge by leadership (53%) and practitioners (62%). Other significant challenges included data governance, performance issues, and integration complexity. These findings underscore the multifaceted difficulties organizations face in implementing AI agents effectively.

Reflecting on these examples, it’s evident that the widespread adoption of AI agents in enterprise settings faces significant limitations. While 2025 may usher in extensive research, proofs of concept (POCs), and minimum viable products (MVPs), the path to full-scale integration remains complex.

6. AI in a corporate environment
Big companies operate under strict rules, structured workflows, and a constant focus on ROI. Unlike agile startups that can adapt on the fly, large organizations have to deal with complex approval processes, compliance checks, and risk management. All this makes adopting AI a slower process, and the idea of rapid transformation often feels more like a distant dream than something achievable.

Chip Huyen references the most common LLM applications in her AI engineering book. Enterprises are risk-averse and prefer to deploy internal-facing applications first. From what we have seen so far, even though there is initial support from the leadership to deploy such applications, not enough funding goes to those projects (and unlikely will) as they do not generate direct business value. We are not saying there is no value — there is, but it is challenging to convince stakeholders.


Image reinterpreted from Huyen, C. (2025). AI Engineering: Building Applications with Foundation Models. Available on Amazon
In enterprises, the most common use cases with direct business generation are related to customer service (forwarding customers to the right agents/ processes) and reviewing contracts. These use cases have been there for a while, and have historically been NLP-heavy, and AI models helped to improve these projects.

Some companies have tried to use LLMs for recommendations and chatbots, and the world has seen enough failures. Here are some examples:

DPD’s customer-facing chatbot, “Ruby,” was designed to assist customers with their inquiries. However, due to insufficient safeguards, a user was able to provoke the bot into swearing and composing a poem criticizing the company itself. This incident underscores the importance of implementing strict content moderation protocols and regularly updating AI systems to prevent such occurrences.

Similarly, Pak’nSave’s AI meal planner app, intended to provide innovative recipe suggestions, malfunctioned and recommended a combination of ingredients that would produce chlorine gas, labeling it as an “aromatic water mix.” This highlights the critical need for rigorous testing and validation of AI outputs, especially in applications directly impacting consumer health and safety.

It feels like not everyone has learned from it, and we regularly see companies launching AI applications without clear business value with poor guardrails, mainly for “marketing purposes”. Let’s hope it will not turn out to be bad marketing, as users will try to make the app do things it is not supposed to do “just for fun”.

There are exceptions. Some companies created nice LLM-powered recommendations, for example, Zalando. It has well-implemented guardrails and is useful for the customers (it helps to find items that are otherwise hard to find via search). In October 2024, Zalando expanded its AI-powered assistant to all 25 markets, supporting local languages. This expansion aims to provide customers with personalized fashion advice and insights into emerging local trends, thereby enhancing the shopping experience.

7. Areas of attention & conclusions
There is great potential to leverage AI in a corporate setting. However, to hope for enterprise adoption, we must consider security gaps, controlled environments, transparency and traceability, and a way to monitor and evaluate AI systems.

In enterprise ecosystems, AI systems need large volumes of data, including personal and proprietary information. Their role is to enhance workflows and boost efficiency, but they need access to critical systems, which can be considered a security risk. Organizations must focus on preventing unapproved access to data, breaches, and compliance issues.

Threat actors can deploy malware that mimics AI behavior to breach networks, skew decisions, or steal secrets. AI agents act autonomously, making them harder to detect and control. This creates a major challenge: real-time oversight of AI systems.

Monitoring is a persistent issue. Few companies have proper systems in place, and AI’s complexity makes it even harder. Owners must fully understand every decision their AI makes

AI’s transformative potential is undeniable, but the path from hype to reality is complex and challenging. Rather than chasing unicorns and rainbows, organizations must take a grounded, strategic approach — one that prioritizes real business needs, robust security frameworks, and a deep understanding of AI’s limitations. The road ahead is uncertain, but one thing is clear: the way we answer these questions will determine whether AI becomes a lasting force for good or just another passing bubble.",,,
11,https://modulabs.co.kr/,2025.02.05,https://modulabs.co.kr/blog/deepseek-security-caution?utm_source=stibee&utm_medium=moduletter&utm_campaign=blog_curation&utm_content=deepseek_security&utm_term=none,"DeepSeek 보안, 개인정보 처리방침의 숨겨진 위험성 분석","챗GPT 같은 인공지능 챗봇 등이 대세가 되면서 대다수의 사람들이 인공지능 사용하면서 생활을 하고 있습니다.
요근래 중국 회사 DeepSeek에서 만든 챗봇도 인기로 자리잡았으며 특히 R1 성능이 OpenAI에서 만드는 o1에 맞먹는 성능을 자랑한다는 결과까지 나왔습니다.
그런데 혹시 DeepSeek, 꼼꼼하게 따져보고 쓰고 계신가요?
오늘은 DeepSeek의 개인정보 처리방침을 샅샅이 파헤쳐 보면서, DeepSeek 보안에 문제가 없는지 그리고 사용할때 주의해야 할 부분은 없는지 알아보겠습니다.

중국 기업의 DeepSeek 보안, 내 정보는 어디로?


DeepSeek의 가장 큰 특징은, 데이터를 관리하는 회사가 중국에 있다는 점입니다.
항저우와 베이징에 있는 DeepSeek 법인들이 우리 정보를 쥐고 있어 DeepSeek 보안 문제가 우려됩니다.

일단, 우리가 DeepSeek에 입력하는 모든 정보는 중국 서버에 저장되고, 중국 법의 적용을 받게 되는데 중국은 국가 안보나 공공질서라는 이유로 정부가 데이터에 접근할 수 있는 권한이 엄청나게 넓습니다.
특히 우리를 비롯한 중국에 살지 않는 외국인들의 DeepSeek 보안이 더욱 취약할 수 있습니다.

회사에서 중요한 사업 전략이나 기술 정보를 DeepSeek으로 이야기했다면 그 내용이 중국 정부 기관에 넘어갈 수도 있습니다.
또, 중국의 사이버보안법이나 데이터보안법에 따르면, 정부가 중요하다고 판단하는 정보는 즉시 제공해야 할 의무가 있어서, 사용자 입장에서는 내 정보에 대한 권리를 제대로 행사하기 어려울 수 있기 때문에 DeepSeek 보안 측면에서 좋은 선택지라 볼 수 없습니다.

너무 많은 정보를 가져가는 DeepSeek 보안의 현실
DeepSeek은 개인 정보를 너무 많이 수집한다는 점에서 DeepSeek 보안 정책의 문제점이 드러납니다.
특히 이런 부분들까지 전부 수집하는 것이 더 큰 문제입니다.

타이핑 습관까지…?:
키보드를 누르는 패턴이나 리듬 같은 생체 정보를 가져가는데, 이걸 왜 수집하는지, 어디에 쓰는지 명확하게 설명해주지 않습니다.
이렇게 되면 우리가 키보드를 어떻게 두드리는지 분석해서, 우리 행동 패턴을 알아낼 수도 있습니다.

“사용하는 기능 및 취하는 행동”…?:
이건 또 무슨 의미일까요? 너무 애매하게 정의된 사용 정보를 수집해서, 우리를 감시하려는 건 아닌지 우려가 됩니다.
서비스를 제공하는 데 꼭 필요한 정보만 수집해야 하는 원칙에 어긋난다고 볼 수 있습니다.

광고 파트너에게까지…?:
심지어 광고 파트너나 다른 회사로부터 우리가 DeepSeek 밖에서 뭘 하는지까지 정보를 가져옵니다.
이건 AI 챗봇 서비스에서 기대하는 수준을 넘어서는 거죠. 사용자의 온라인 활동을 샅샅이 사찰한다는 생각이 들 수 밖에 없습니다,

내 대화 내용까지 수집하는 DeepSeek 보안 문제
information-thief

DeepSeek은 정보를 “서비스 제공 및 관리, 서비스 개선 및 개발, 안전, 보안 및 안정성 유지” 같은 애매한 목적으로 사용한다고만 밝히고 있습니다.
특히 챗봇은 우리가 대화한 내용을 AI가 학습하는 데 쓸 가능성이 큰데, 이 부분에서 DeepSeek 보안이 취약하다는 것이 큰 문제입니다.

AI가 학습하는 과정에서 이런 위험들이 생길 수 있습니다:

내 이름이나 주소 같은 개인 정보가 AI 모델에 저장돼서, 다른 사람이랑 대화할 때 갑자기 튀어나올 수도 있습니다.

회사 기밀이나 중요한 업무 내용이 모델에 학습돼서 경쟁 회사에 넘어갈 수 있습니다.

개인적인 고민 상담이나 건강 정보가 학습 데이터로 쓰일 수 있습니다.

DeepSeek 보안과 개인정보 공유의 위험성
DeepSeek은 광고나 분석 파트너와 사용자 정보를 공유할 수 있습니다. 이는 DeepSeek 보안에 심각한 허점을 만들 수 있습니다:

우리가 뭘 좋아하는지, 어떤 행동을 하는지 광고에 활용될 수 있습니다.

민감한 대화 내용이 다른 회사에 넘어가서 광고에 쓰일 수도 있습니다.

개인 정보가 여러 회사에 공유되면서 우리의 모든 정보가 하나로 합쳐질 수도 있습니다.

게다가 중국 법 때문에 법 집행 기관이나 공공 기관에 정보를 줘야 할 의무도 있어서 정보의 안정성까지 담보하지 못합니다.

내 권리는 어디에…? DeepSeek 보안의 사각지대
DeepSeek은 우리가 사는 지역에 따라서 개인 정보 보호 권리와 DeepSeek 보안 정책을 다르게 적용하고 있어요.
이건 글로벌 서비스로서는 심각한 문제가 있는 거죠.

유럽은 괜찮고, 한국은…?: 유럽에 사는 사람들은 GDPR 덕분에 개인 정보 보호를 비교적 잘 받을 수 있지만, 아시아나 아프리카처럼 관련 법이 약한 곳에 사는 사람들은 제대로 보호받기 어렵습니다. 중국에 사는 사람들은 중국 법 때문에 더 제약이 많을 거고요.

삭제해달라고 하면 서비스 이용 제한…?: 개인 정보를 삭제하거나 사용을 막아달라고 하면 서비스 이용을 제한할 수 있습니다. 이건 우리 권리를 포기하라는 압박처럼 느껴집니다. 특히 DeepSeek에 의존하는 사람들은 자기 권리를 제대로 행사하기 어려울 것 같습니다.

불명확한 정보 보관 기간, DeepSeek 보안의 또 다른 문제점
DeepSeek의 데이터 보관 정책에도 문제가 있습니다:

“필요한 기간 동안”?: 얼마나 오랫동안 보관하는지 명확하게 알려주지 않습니다. “합법적인 사업적 이익”이라는 주관적인 기준으로 보관 기간을 결정한다고 하니, 언제 삭제되는지 알 수 없습니다.

오래될수록 위험한 정보: 예전에 나눴던 대화나 민감한 정보가 계속 서버에 남아있을 수도 있습니다. 시간이 지날수록 정보가 유출되거나 잘못 사용될 위험도 커지며 잊혀질 권리를 제대로 행사하기 어려울 수 있습니다.

청소년 보호 측면의 DeepSeek 보안 실태
DeepSeek은 청소년 보호 정책도 보완해야 할 부분이 많아요:

나이 확인은 어떻게…?: 14세 미만은 사용하면 안 된다고 하지만, 실제로 나이를 확인하는 절차가 없으며 거짓으로 입력해도 쓸 수 있다는겁니다. 부모님 동의를 받는 시스템도 없습니다.

청소년 맞춤 보호는…?: 14~18세 청소년들을 위한 특별한 보호 장치도 부족합니다. 청소년에게 광고를 하거나 정보를 수집하는 걸 막는 규정도 없고, 유해한 콘텐츠를 걸러내는 시스템도 부족해 보입니다.

결론: DeepSeek 보안, 개선이 시급하다
지금까지 DeepSeek의 개인 정보 처리 방침을 꼼꼼하게 살펴봤으며 여러 면에서 개선해야 할 부분이 많다는 것을 알 수 있습니다.
특히 중국 법의 적용을 받는다는 점, 너무 많은 정보를 수집한다는 점, 불분명한 정보 활용 목적, 제한적인 사용자 권리 같은 문제들은 시급하게 해결해야 합니다.

사용자입장에서 이 부분을 알지 못한 채로 DeepSeek의 챗봇 시스템의 성능만 보고 이용한다면 문제될 수 있으니.. 꼭 이 부분 명심하고 사용하시길 부탁드립니다.",,,
12,https://blog.bytebytego.com/,2025.02.14,https://blog.bytebytego.com/p/non-functional-requirements-the-backbone?utm_source=post-email-title&publication_id=817132&post_id=157015041&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email,Non-Functional Requirements: The Backbone of Great Software - Part 1,"Non-functional requirements (NFRs) are as critical as functional requirements because they define a system's qualities and operational parameters.

While functional requirements specify what a software product should do (for example, “users must be able to log in”), non-functional requirements define how well it must accomplish these tasks under real-world conditions (for example, “the login process should respond within two seconds under peak load” or “all user credentials must be encrypted and stored securely”).

Together, functional and non-functional requirements create a foundation for building great software systems. 

NFRs are essential for the following reasons:

Quality of Service: NFRs like response time, availability, and usability directly affect the user’s perception of quality. A system that fulfills its functional requirements but is slow, constantly crashes, or is difficult to use can undermine user trust and satisfaction.

System Stability: Requirements such as reliability, fault tolerance, and recoverability help maintain stable operation even when part of the system fails. Without these, unhandled errors can escalate into large-scale outages.

Security and Compliance: Security-related NFRs dictate how data is protected, how access is controlled, and how audits are conducted. Neglecting these can lead to breaches, legal consequences, or reputational damage.

Scalability and Performance: Requirements for throughput, capacity, and resource utilization ensure the software can handle growth in users or data. If not addressed from the start, scaling can become prohibitively expensive or technically challenging later on.

Maintenance and Evolution: Maintainability, testability, and modularity requirements determine how easily bugs can be fixed, features added, or adaptations made to changing environments. Overlooking them can lead to ballooning technical debt, slowing down future development.

In short, non-functional requirements are not mere “nice-to-haves” but essential components that ensure a software system truly meets user expectations and withstands real-world challenges. 

In this article (Part 1), we’ll look at the differences between functional and non-functional requirements. Then, we’ll explore the various trade-offs in NFRs and their architectural impact on building systems.

",,,
13,https://www.llmwatch.com/,2025.02.08,https://www.linkedin.com/pulse/massive-progress-reasoning-models-pascal-biese-abguf/,Massive Progress in Reasoning Models,"In this issue:
Beating OpenAI with Open-Source
99% performance with only 1% data
Chain-of-Associated-Thoughts (CoAT)


For those of you that enjoy the Linkedin version of LLM Watch, that's great, it's why I publish it here. However, I've heard numerous times by now that people weren't even aware of my Substack - or what Substack is, at all. 

You don't have to use the website or the app itself if you don't want to, you can simply subscribe to me there and get all of my updates straight to your e-mail. This includes additional content that I only publish on Substack.

 Click here for the full experience

1. s1: Simple test-time scaling
Watching: s1 (paper/code)


What problem does it solve? Current approaches to test-time scaling—using additional compute during inference to boost performance—often rely on opaque or complex methodologies, as seen in OpenAI’s proprietary ""o1"" model. This lack of transparency and simplicity hinders reproducibility and practical adoption. The article addresses this gap by proposing an accessible, minimalistic framework for test-time compute scaling, focusing on enhancing model reasoning without intricate architectural changes.

How does it solve the problem? The authors combined two key innovations: a compact, high-quality dataset (s1K) and a novel ""budget forcing"" mechanism. s1K was curated via rigorous criteria (difficulty, diversity, quality) to maximize fine-tuning efficiency with minimal data. Budget forcing dynamically controls test-time compute by either truncating the model’s reasoning to limit resources or appending ""Wait"" tokens to prompt self-correction, effectively simulating iterative refinement. The approach was applied via supervised fine-tuning on the Qwen2.5-32B-Instruct model.

What are the key findings? The model (s1-32B) outperformed OpenAI’s o1-preview by up to 27% on competition math benchmarks (MATH, AIME24) and extrapolated beyond its base performance with budget forcing, improving from 50% to 57% on AIME24. The results demonstrate that controlled test-time compute interventions, even with a small dataset, yield significant gains in reasoning tasks. The open-source release of the model, data, and code further underscores reproducibility.

Why does it matter? This work democratizes test-time scaling by proving its viability through simple, transparent methods—contrasting with proprietary ""black-box"" approaches. Budget forcing introduces a lightweight, adaptive mechanism to optimize compute use during reasoning, applicable across domains like education or coding. By open-sourcing their framework, the authors enable broader community adoption and innovation, advancing equitable access to high-performance LLM capabilities.

2. LIMO: Less is More for Reasoning
Watching: LIMO (paper)


What problem does it solve? The article challenges the prevailing belief that eliciting complex reasoning—especially in mathematical domains—requires an enormous amount of training data (often exceeding 100,000 examples). It questions the long-held assumption that large-scale supervised fine-tuning (SFT) is necessary for fostering sophisticated reasoning skills in large language models (LLMs).

How does it solve the problem? The authors propose a model called LIMO (Less-Is-More Reasoning) that challenges the conventional wisdom. They demonstrate that complex mathematical reasoning abilities can be effectively elicited in LLMs with surprisingly few examples. By using only 817 carefully curated training samples, LIMO achieves remarkable performance on mathematical reasoning tasks, outperforming previous models trained on significantly larger datasets. Key innovations include iterative distillation of examples emphasizing intermediate cognitive steps ""borrowed"" from human solvers, combined with gradient-aware example pruning to maximize template efficiency.

What are the key findings? LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, a substantial improvement from previous SFT-based models' 6.5% and 59.2% respectively, while using only 1% of the training data. LIMO also demonstrates exceptional out-of-distribution generalization, achieving a 40.5% absolute improvement across 10 diverse benchmarks, surpassing models trained on 100x more data. These results challenge the notion that SFT leads to memorization rather than generalization.

Why does it matter? They fundamentally reframe how we approach specialized reasoning in LLMs - not as data-hungry pattern recognition tasks, but as knowledge-unlocking challenges. This reduces computational costs and democratizes development of specialized models. The discovery also offers new perspectives on AI cognition, suggesting foundation models may possess ""latent reasoning muscles"" that require targeted activation rather than brute-force training. For practitioners, it enables mathematical reasoning deployment scenarios where large labeled datasets are unavailable.

3. CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning
Watching: CoAT (paper)


What problem does it solve? Current LLM inference predominantly relies on a ""fast thinking"" approach, where models generate outputs in a single pass without iterative refinement. While effective for many tasks, this methodology lacks mechanisms to dynamically integrate new information or revisit earlier reasoning steps—key aspects of human-like ""slow thinking."" This limitation becomes pronounced in complex scenarios requiring adaptability, multi-step reasoning, or incorporation of evolving context.

How does it solve the problem? The authors introduced Chain-of-Associated-Thoughts (CoAT), blending Monte Carlo Tree Search (MCTS) with a dynamic ""associative memory"" system. MCTS enables structured exploration of diverse reasoning pathways similar to human brainstorming, while associative memory acts as a real-time knowledge repository. This combination allows LLMs to iteratively update their reasoning by retrieving and cross-referencing stored insights, mimicking the human ability to pause, reflect, and refine earlier conclusions.

What are the key findings? CoAT significantly outperformed conventional inference methods across generative and reasoning tasks, scoring higher in accuracy, coherence, and output diversity. The framework’s iterative refinement—enabled by MCTS-guided exploration and associative memory integration—produced outputs better aligned with complex problem-solving requirements. Notably, the system demonstrated strong context retention even as search spaces expanded dynamically.

Why does it matter? These results address a critical gap in LLM capabilities: the inability to self-correct or incorporate new insights mid-reasoning. By aligning model reasoning closer to human cognitive processes, the CoAT framework paves the way for developing AI systems that are not only more accurate but also better equipped to handle complex, real-world tasks that demand flexibility and contextual adaptability.

Papers of the Week:
Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models
The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking
Reward-Guided Speculative Decoding for Efficient LLM Reasoning
Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment
SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling
Pheromone-based Learning of Optimal Reasoning Paths
Jackpot! Alignment as a Maximal Lottery
Can We Predict the Effect of Prompts?
Partially Rewriting a Transformer in Natural Language
SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs
Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",,,
14,https://the-decoder.com/,2025.02.16,https://the-decoder.com/chatgpt-passes-turing-test-for-psychotherapy-study-says/,"ChatGPT passes Turing test for psychotherapy, study says","A recent study reveals that people struggle to differentiate between therapeutic responses from ChatGPT and human therapists, with the AI's answers often rated as more empathetic than those from professionals.

The classic Turing test, developed by computer science pioneer Alan Turing, measures whether humans can identify if they're interacting with a machine or another person. Researchers recently applied this concept to psychotherapy, asking 830 participants to differentiate between responses from ChatGPT and human therapists.

According to research published in PLOS Mental Health, participants performed only slightly better than random guessing when trying to identify the source of therapeutic responses. They correctly identified human therapist responses 56.1 percent of the time and ChatGPT responses 51.2 percent of the time. The researchers examined 18 couples therapy case studies, comparing responses from 13 experienced therapists against those generated by ChatGPT.

The human factor still influences perception
The study found that ChatGPT's responses actually outperformed human experts in measures of therapeutic quality, scoring higher in therapeutic alliance, empathy, and cultural competence.

Several factors contributed to ChatGPT's strong performance. The AI system consistently produced longer responses with a more positive tone, and used more nouns and adjectives in its answers. These characteristics likely made its responses appear more detailed and empathetic to readers.

The research uncovered an important bias: when participants believed they were reading AI-generated responses, they rated them lower - regardless of whether humans or ChatGPT actually wrote them. This bias worked both ways: AI-generated responses received their highest ratings when participants incorrectly attributed them to human therapists.

The researchers acknowledge important limitations in their work. Their study relied on brief, hypothetical therapy scenarios rather than real therapy sessions. They also question whether their findings from couples therapy would apply equally well to individual counseling.

Still, as evidence grows for AI's potential benefits in therapeutic settings and its likely future role in mental health care, the researchers emphasize that mental health professionals need to understand these systems. They stress that responsible clinicians must carefully train and monitor AI models to maintain high standards of care.

Growing evidence supports AI's therapeutic potential
This isn't the first study to demonstrate AI's capabilities in advisory roles. Research from the University of Melbourne and the University of Western Australia found that ChatGPT provided more balanced, comprehensive, and empathetic advice on social dilemmas compared to human advice columnists, with preference rates between 70 and 85 percent.",,,
15,https://techcrunch.com/,2025.02.16,https://techcrunch.com/2025/02/16/openai-tries-to-uncensor-chatgpt/,OpenAI tries to ‘uncensor’ ChatGPT,"OpenAI is changing how it trains AI models to explicitly embrace “intellectual freedom … no matter how challenging or controversial a topic may be,” the company says in a new policy.

As a result, ChatGPT will eventually be able to answer more questions, offer more perspectives, and reduce the number of topics the AI chatbot won’t talk about.

The changes might be part of OpenAI’s effort to land in the good graces of the new Trump administration, but it also seems to be part of a broader shift in Silicon Valley and what’s considered “AI safety.”

On Wednesday, OpenAI announced an update to its Model Spec, a 187-page document that lays out how the company trains AI models to behave. In it, OpenAI unveiled a new guiding principle: Do not lie, either by making untrue statements or by omitting important context.

In a new section called “Seek the truth together,” OpenAI says it wants ChatGPT to not take an editorial stance, even if some users find that morally wrong or offensive. That means ChatGPT will offer multiple perspectives on controversial subjects, all in an effort to be neutral.

For example, the company says ChatGPT should assert that “Black lives matter,” but also that “all lives matter.” Instead of refusing to answer or picking a side on political issues, OpenAI says it wants ChatGPT to affirm its “love for humanity” generally, then offer context about each movement.

“This principle may be controversial, as it means the assistant may remain neutral on topics some consider morally wrong or offensive,” OpenAI says in the spec. “However, the goal of an AI assistant is to assist humanity, not to shape it.”

The new Model Spec doesn’t mean that ChatGPT is a total free-for-all now. The chatbot will still refuse to answer certain objectionable questions or respond in a way that supports blatant falsehoods.

These changes could be seen as a response to conservative criticism about ChatGPT’s safeguards, which have always seemed to skew center-left. However, an OpenAI spokesperson rejects the idea that it was making changes to appease the Trump administration.

Instead, the company says its embrace of intellectual freedom reflects OpenAI’s “long-held belief in giving users more control.”

But not everyone sees it that way.

Conservatives claim AI censorship

Trump’s closest Silicon Valley confidants — including David Sacks, Marc Andreessen, and Elon Musk — have all accused OpenAI of engaging in deliberate AI censorship over the last several months. We wrote in December that Trump’s crew was setting the stage for AI censorship to be a next culture war issue within Silicon Valley.

Of course, OpenAI doesn’t say it engaged in “censorship,” as Trump’s advisers claim. Rather, the company’s CEO, Sam Altman, previously claimed in a post on X that ChatGPT’s bias was an unfortunate “shortcoming” that the company was working to fix, though he noted it would take some time.

Altman made that comment just after a viral tweet circulated in which ChatGPT refused to write a poem praising Trump, though it would perform the action for Joe Biden. Many conservatives pointed to this as an example of AI censorship.

While it’s impossible to say whether OpenAI was truly suppressing certain points of view, it’s a sheer fact that AI chatbots lean left across the board.

Even Elon Musk admits xAI’s chatbot is often more politically correct than he’d like. It’s not because Grok was “programmed to be woke” but more likely a reality of training AI on the open internet. 

Nevertheless, OpenAI now says it’s doubling down on free speech. This week, the company even removed warnings from ChatGPT that tell users when they’ve violated its policies. OpenAI told TechCrunch this was purely a cosmetic change, with no change to the model’s outputs.

The company seems to want ChatGPT to feel less censored for users.

It wouldn’t be surprising if OpenAI was also trying to impress the new Trump administration with this policy update, notes former OpenAI policy leader Miles Brundage in a post on X.

Trump has previously targeted Silicon Valley companies, such as Twitter and Meta, for having active content moderation teams that tend to shut out conservative voices.

OpenAI may be trying to get out in front of that. But there’s also a larger shift going on in Silicon Valley and the AI world about the role of content moderation.

Generating answers to please everyone

Newsrooms, social media platforms, and search companies have historically struggled to deliver information to their audiences in a way that feels objective, accurate, and entertaining.

Now, AI chatbot providers are in the same delivery information business, but arguably with the hardest version of this problem yet: How do they automatically generate answers to any question?

Delivering information about controversial, real-time events is a constantly moving target, and it involves taking editorial stances, even if tech companies don’t like to admit it. Those stances are bound to upset someone, miss some group’s perspective, or give too much air to some political party.

For example, when OpenAI commits to let ChatGPT represent all perspectives on controversial subjects — including conspiracy theories, racist or antisemitic movements, or geopolitical conflicts — that is inherently an editorial stance.

Some, including OpenAI co-founder John Schulman, argue that it’s the right stance for ChatGPT. The alternative — doing a cost-benefit analysis to determine whether an AI chatbot should answer a user’s question — could “give the platform too much moral authority,” Schulman notes in a post on X.

Schulman isn’t alone. “I think OpenAI is right to push in the direction of more speech,” said Dean Ball, a research fellow at George Mason University’s Mercatus Center, in an interview with TechCrunch. “As AI models become smarter and more vital to the way people learn about the world, these decisions just become more important.”

In previous years, AI model providers have tried to stop their AI chatbots from answering questions that might lead to “unsafe” answers. Almost every AI company stopped their AI chatbot from answering questions about the 2024 election for U.S. president. This was widely considered a safe and responsible decision at the time.

But OpenAI’s changes to its Model Spec suggest we may be entering a new era for what “AI safety” really means, in which allowing an AI model to answer anything and everything is considered more responsible than making decisions for users.

Ball says this is partially because AI models are just better now. OpenAI has made significant progress on AI model alignment; its latest reasoning models think about the company’s AI safety policy before answering. This allows AI models to give better answers for delicate questions.

Of course, Elon Musk was the first to implement “free speech” into xAI’s Grok chatbot, perhaps before the company was really ready to handle sensitive questions. It still might be too soon for leading AI models, but now, others are embracing the same idea.

Shifting values for Silicon Valley

Mark Zuckerberg made waves last month by reorienting Meta’s businesses around First Amendment principles. He praised Elon Musk in the process, saying the owner of X took the right approach by using Community Notes — a community-driven content moderation program — to safeguard free speech.

In practice, both X and Meta ended up dismantling their longstanding trust and safety teams, allowing more controversial posts on their platforms and amplifying conservative voices.

Changes at X may have hurt its relationships with advertisers, but that could have more to do with Musk, who has taken the unusual step of suing some of them for boycotting the platform. Early signs indicate that Meta’s advertisers were unfazed by Zuckerberg’s free speech pivot.

Meanwhile, many tech companies beyond X and Meta have walked back from left-leaning policies that dominated Silicon Valley for the last several decades. Google, Amazon, and Intel have eliminated or scaled back diversity initiatives in the last year.

OpenAI may be reversing course, too. The ChatGPT-maker seems to have recently scrubbed a commitment to diversity, equity, and inclusion from its website.

As OpenAI embarks on one of the largest American infrastructure projects ever with Stargate, a $500 billion AI datacenter, its relationship with the Trump administration is increasingly important. At the same time, the ChatGPT maker is vying to unseat Google Search as the dominant source of information on the internet.

Coming up with the right answers may prove key to both.",,,
16,https://medium.com,,https://medium.com/towards-data-science/deepseek-v3-explained-1-multi-head-latent-attention-ed6bee2a67c4,DeepSeek-V3 Explained 1: Multi-head Latent Attention,"This is the first article of our new series “DeepSeek-V3 Explained”, where we will try to demystify DeepSeek-V3 [1, 2], the latest model open-sourced by DeepSeek.

In this series, we aim to cover two major topics:

Major architecture innovations in DeepSeek-V3, including MLA (Multi-head Latent Attention) [3], DeepSeekMoE [4], auxiliary-loss-free load balancing [5], and multi-token prediction training.
Training of DeepSeek-V3, including pre-training, finetuning and RL alignment phases.
This article mainly focuses on Multi-head Latent Attention, which was first proposed in the development of DeepSeek-V2 and then used in DeepSeek-V3 as well.

Here is the link to other articles in this series:

DeepSeek-V3 Explained 2: DeepSeekMoE
Table of contents:

Background: we start from the standard MHA, explain why we need Key-Value cache at inference stage, how MQA and GQA try to optimize it, and how RoPE works, etc.
Multi-head Latent Attention: An in-depth introduction to MLA, including its motivations, why decoupled RoPE is needed, and its performance.
References.
Background
To better understand MLA and also make this article self-contained, we will revisit several related concepts in this section before diving into the details of MLA.

MHA in Decoder-only Transformers
Note that MLA is developed to speedup inference speed in autoregressive text generation, so the MHA we are talking about under this context is for decoder-only Transformer.

The figure below compares three Transformer architectures used for decoding, where (a) shows both the encoder and decoder proposed in the original “Attention is All You Need” paper. Its decoder part is then simplified by [6], leading to a decoder-only Transformer model shown in (b), which is later used in many generation models like GPT [8].

Nowadays, LLMs are more commonly to choose the structure shown in (c) for more stable training, with normalization applied on the input rather then output, and LayerNorm upgraded to RMS Norm. This will serve as the baseline architecture we will discuss in this article.


Figure 1. Transformer architectures. (a) encoder-decoder proposed in [6]. (b) Decoder-only Transformer proposed in [7] and used in GPT [8]. (c) An optimized version of (b) with RMS Norm before attention. [3]
Within this context, MHA calculation largely follows the process in [6], as shown in the figure below:


Figure 2. Scaled dot-product attention vs. Multi-Head Attention. Image from [6].
Assume we have n_h attention heads, and the dimension for each attention head is represented as d_h, so that the concatenated dimension will be (n_h · d_h).

Given a model with l layers, if we denote the input for the t-th token in that layer as h_t with dimension d, we need to map the dimension of h_t from d to (h_n · d_h) using the linear mapping matrices.

More formally, we have (equations from [3]):


where W^Q, W^K and W^V are the linear mapping matrices:


After such mapping, q_t, k_t and v_t will be split into n_h heads to calculate the scaled dot-product attention:


where W^O is another projection matrix to map the dimension inversely from (h_n · d_h) to d:


Note that the process described by Eqn.(1) to (8) above is just for a single token. During inference, we need to repeat this process for each newly generated token, which involves a lot of repeated calculation. This leads to a technique called Key-Value cache.

Key-Value Cache
As suggested by its name, Key-Value cache is a technique designed to speedup the autoregressive process by caching and reusing the previous keys and values, rather than re-computing them at each decoding step.

Note that KV cache is typically used only during the inference stage, since in training we still need to process the entire input sequence in parallel.

KV cache is commonly implemented as a rolling buffer. At each decoding step, only the new query Q is computed, while the K and V stored in the cache will be reused, so that the attention will be computed using the new Q and reused K, V. Meanwhile, the new token’s K and V will also be appended to the cache for later use.

However, the speedup achieved by KV cache comes at a cost of memory, since KV cache often scales with batch size × sequence length × hidden size × number of heads, leading to a memory bottleneck when we have larger batch size or longer sequences.

That further leads to two techniques aiming at addressing this limitation: Multi-Query Attention and Grouped-Query Attention.

Multi-Query Attention (MQA) vs Grouped-Query Attention (GQA)
The figure below shows the comparison between the original MHA, Grouped-Query Attention (GQA) [10] and Multi-Query Attention (MQA) [9].


Figure 3. MHA [6], GQA [10] AND MQA [9]. Image from [10].
The basic idea of MQA is to share a single key and a single value head across all query heads, which can significantly reduce memory usage but will also impact the accuracy of attention.

GQA can be seen as an interpolating method between MHA and MQA, where a single pair of key and value heads will be shared only by a group of query heads, not all queries. But still this will lead to inferior results compared to MHA.

In the later sections, we will see how MLA manages to seek a balance between memory efficiency and modeling accuracy.

RoPE (Rotary Positional Embeddings)
One last piece of background we need to mention is RoPE [11], which encodes positional information directly into the attention mechanism by rotating the query and key vectors in multi-head attention using sinusoidal functions.

More specifically, RoPE applies a position-dependent rotation matrix to the query and key vectors at each token, and uses sine and cosine functions for its basis but applies them in a unique way to achieve rotation.

To see what makes it position-dependent, consider a toy embedding vector with only 4 elements, i.e., (x_1, x_2, x_3, x_4).

To apply RoPE, we firstly group consecutive dimensions into pairs:

(x_1, x_2) -> position 1
(x_3, x_4) -> position 2
Then, we apply a rotation matrix to rotate each pair:


Figure 4. Illustration of the rotation matrix applied to a pair of tokens. Image by author.
where θ = θ(p) = p ⋅ θ_0​, and θ_0​ is a base frequency. In our 4-d toy example, this means that (x_1, x_2) will be rotated by θ_0​, and (x_3, x_4) will be rotated by 2 ⋅ θ_0.

This is why we call this rotation matrix as position-dependent: at each position (or each pair), we will apply a different rotation matrix where the rotation angle is determined by position.

RoPE is widely used in modern LLMs due to its efficiency in encoding long sequences, but as we can see from the above formula, it is position-sensitive to both Q and K, making it incompatible with MLA in some ways.

Multi-head Latent Attention
Finally we can move on to the MLA part. In this section we will first layout the high-level idea of MLA, and then dive deeper into why it needs to modify RoPE. Finally, we present the detailed algorithm of MLA as well as it performance.

MLA: High-level Idea
The basic idea of MLA is to compress the attention input h_t into a low-dimensional latent vector with dimension d_c, where d_c is much lower than the original (h_n · d_h). Later when we need to calculate attention, we can map this latent vector back to the high-dimensional space to recover the keys and values. As a result, only the latent vector needs to be stored, leading to significant memory reduction.

This process can be more formally described with the following equations, where c^{KV}_t is the latent vector, W^{DKV} is the compressing matrix that maps h_t’s dimension from (h_n · d_h) to d_c (here D in the superscript stands for “down-projection”, meaning compressing the dimension), while W^{UK} and W^{UV} are both up-projection matrices that map the shared latent vector back to the high-dimensional space.


Similarly, we can also map the queries into a latent, low-dimensional vector and then map it back to the original, high-dimensional space:


Why Decoupled RoPE is Needed
As we mentioned before, RoPE is a common choice for training generation models to handle long sequences. If we directly apply the above MLA strategy, that will be incompatible with RoPE.

To see this more clearly, consider what happens when we calculate attention using Eqn. (7): when we multiply the transposed q with k, the matrices W^Q and W^{UK} will appear in the middle, and their combination equivalents to a single mapping dimension from d_c to d.

In the original paper [3], the authors describe this as W^{UK} can be “absorbed” into W^Q, as a result we do not need to store W^{UK} in the cache, further reducing memory usage.

However, this will not be the case when we take the rotation matrix in Figure (4) into consideration, as RoPE will apply a rotation matrix on the left of W^{UK}, and this rotation matrix will end up in between the transposed W^Q and W^{UK}.

As we have explained in the background part, this rotation matrix is position-dependent, meaning the rotation matrix for each position is different. As a result, W^{UK} can no longer be absorbed by W^Q.

To address this conflict, the authors propose what they call “a decoupled RoPE”, by introducing additional query vectors along with a shared key vector, and use these additional vectors in the RoPE process only, while keeping the original keys kind of isolated with the rotation matrix.

The entire process of MLA can be summarized as below (equation numbers reused from Appendix C in [3]):


Figure 5. MLA process. Image edited by author based on equations in [3].
where

Eqn. (37) to (40) describe how to process query tokens.
Eqn. (41) and (42) describe how to process key tokens.
Eqn. (43) and (44) describe how to use the additional shared key for RoPE, be aware that the output of (42) is not involved in RoPE.
Eqn. (45) describes how to process value tokens.
During this process, only the blue variables with boxes need to be cached. This process can be illustrated more clearly with the flowchart blow:


Figure 6. Flowchart of MLA. Image from [3].
Performance of MLA
The table below compares the number of elements needed for KV cache (per token) as well as the modeling capacity between MHA, GQA, MQA and MLA, demonstrating that MLA could indeed achieve a better balance between memory efficiency vs. modeling capacity.

Interestingly, the modeling capacity for MLA even surpass that of the original MHA.


Table 1 from [3].
More specifically, the table below shows the performance of MHA, GQA and MQA on 7B models, where MHA significantly outperforms both MQA and GQA.


Table 8 from [3].
The authors of [3] also conduct analysis between MHA vs. MLA, and results are summarized in the table below, where MLA achieves better results overall.


Table 9 in [3].
References
[1] DeepSeek
[2] DeepSeek-V3 Technical Report
[3] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
[4] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models
[5] Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts
[6] Attention Is All You Need
[7] Generating Wikipedia by Summarizing Long Sequences
[8] Improving Language Understanding by Generative Pre-Training
[9] Fast Transformer Decoding: One Write-Head is All You Need
[10] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
[11] RoFormer: Enhanced Transformer with Rotary Position Embedding",,,
17,https://medium.com,,https://medium.com/@isaakmwangi2018/a-simple-guide-to-deepseek-r1-architecture-training-local-deployment-and-hardware-requirements-300c87991126,"A Simple Guide to DeepSeek R1: Architecture, Training, Local Deployment, and Hardware Requirements","DeepSeek’s Novel Approach to LLM Reasoning

DeepSeek has introduced an innovative approach to improving the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL), detailed in their recent paper on DeepSeek-R1. This research represents a significant advancement in how we can enhance LLMs’ ability to solve complex problems through pure reinforcement learning, without relying heavily on supervised fine-tuning.

Before we proceed if you like this topic and you want to support me:

Clap my article 10 times; that will help me out.👏
Follow me on Medium to get my latest articles 🫶
Technical Overview of DeepSeek-R1

Model Architecture:

DeepSeek-R1 is not a singular model but a family of models, encompassing: DeepSeek-R1-Zero and DeepSeek-R1

Let me clarify the key differences between DeepSeek-R1 and DeepSeek-R1-Zero:

The Primary Distinction

DeepSeek-R1-Zero represents the team’s initial experiment using pure reinforcement learning without any supervised fine-tuning. They started with their base model and applied reinforcement learning directly, letting the model develop reasoning capabilities through trial and error. While this approach achieved impressive results (71% accuracy on AIME 2024), it had some significant limitations, particularly in readability and language consistency. It features 671 billion parameters, utilizing a mixture-of-experts (MoE) architecture where each token activates parameters equivalent to 37 billion. This model showcases emergent reasoning behaviors, such as self-verification, reflection, and long chain-of-thought (CoT) reasoning.

DeepSeek-R1, in contrast, uses a more sophisticated multi-stage training approach. Instead of pure reinforcement learning, it begins with supervised fine-tuning on a small set of carefully curated examples (called “cold-start data”) before applying reinforcement learning. This approach addresses the limitations of DeepSeek-R1-Zero while achieving even better performance. This model also maintains the 671 billion parameter count but achieves better readability and coherence in responses.

The Training Process Comparison
Training Methodology:

Reinforcement Learning: Unlike traditional models that predominantly rely on supervised learning, DeepSeek-R1 uses RL extensively. The training leverages group relative policy optimization (GRPO), focusing on accuracy and format rewards to enhance reasoning capabilities without the need for extensive labeled data.
Distillation Techniques: To democratize access to high-performing models, DeepSeek has also released distilled versions of R1, ranging from 1.5 billion to 70 billion parameters. These models are based on architectures like Qwen and Llama, showing that complex reasoning can be encapsulated in smaller, more efficient models. The distillation process involves fine-tuning these smaller models with synthetic reasoning data generated by the full DeepSeek-R1, thus preserving high performance at reduced computational cost.
DeepSeek-R1-Zero’s training process is straightforward:

Start with base model
Apply reinforcement learning directly
Use simple rewards based on accuracy and format
DeepSeek-R1’s training process has four distinct stages:

Initial supervised fine-tuning with thousands of high-quality examples
Reinforcement learning focused on reasoning tasks
Collection of new training data through rejection sampling
Final reinforcement learning across all types of tasks
Performance Metrics:

Reasoning Benchmarks: DeepSeek-R1 has shown impressive results on various benchmarks:
AIME 2024: Achieved a 79.8% pass rate, compared to 79.2% by OpenAI’s o1–1217.
MATH-500: Scored an impressive 97.3%, slightly ahead of o1–1217’s 96.4%.
SWE-bench Verified: Outperformed in programming tasks, showcasing its coding proficiency.
Cost Efficiency: The API for DeepSeek-R1 is priced at $0.14 per million input tokens for cache hits, making it significantly cheaper than comparable models like OpenAI’s o1.
Limitations and Future Work

The paper acknowledges several areas for improvement:

The model sometimes struggles with tasks requiring specific output formats
Performance on software engineering tasks could be enhanced
There are challenges with language mixing in multilingual contexts
Few-shot prompting consistently degrades performance
Future work will focus on addressing these limitations and expanding the model’s capabilities in areas like function calling, multi-turn interactions, and complex role-playing scenarios.

Deployment and Accessibility
Open Source and Licensing: DeepSeek-R1 and its variants are released under the MIT License, promoting open-source collaboration and commercial use, including model distillation. This move is pivotal for fostering innovation and reducing the entry barriers in AI model development.
Model Formats:
Both models and their distilled versions are available in formats like GGML, GGUF, GPTQ, and HF, allowing flexibility in how they are deployed locally.
1. Web Access via DeepSeek Chat Platform:
The DeepSeek Chat platform provides a user-friendly interface to interact with DeepSeek-R1 without any setup requirements.

Steps to Access:
Navigate to the DeepSeek Chat platform
Register for an account or log in if you already have one.
After logging in, select the “Deep Think” mode to experience DeepSeek-R1’s step-by-step reasoning capabilities.

DeepSeek Chat Platform
2. Access via DeepSeek API:
For programmatic access, DeepSeek offers an API compatible with OpenAI’s format, allowing integration into various applications.

Steps to Use the API:

a. Obtain an API Key:

Visit the DeepSeek API platform to create an account and generate your unique API key.
b. Configure Your Environment:

Set the base_url to https://api.deepseek.com/v1.
Use your API key for authentication, typically via Bearer Token in the HTTP header.
c. Make API Calls:

Utilize the API to send prompts and receive responses from DeepSeek-R1.
Detailed documentation and examples are available in the DeepSeek API Docs.

DeepSeek API call example
3. Running DeepSeek-R1 Locally:
Both Models (R1 and R1-Zero):

Hardware Requirements: The full models require significant hardware due to their size. A GPU with substantial VRAM (like Nvidia RTX 3090 or higher) is recommended. For CPU use, you’d need at least 48GB of RAM and 250GB of disk space, although performance would be slow without GPU acceleration.
Distilled Models: For local deployment with less resource-intensive hardware, DeepSeek provides distilled versions. These range from 1.5B to 70B parameters, making them suitable for systems with more modest hardware. For instance, the 7B model can run on a GPU with at least 6GB VRAM or on a CPU with about 4GB RAM for the GGML/GGUF format.
Software Tools for Local Running:

Ollama:
You can use Ollama to serve the models locally: (Ollama Is a tool for running open-source AI models locally on your machine. Grab it here: https://ollama.com/download)


Next, you’ll need to pull and run the DeepSeek R1 model locally.

Ollama offers different model sizes — basically, bigger models equal to smarter AI, but need better GPU. Here’s the lineup:

1.5B version (smallest):
ollama run deepseek-r1:1.5b

8B version:
ollama run deepseek-r1:8b

14B version:
ollama run deepseek-r1:14b

32B version:
ollama run deepseek-r1:32b

70B version (biggest/smartest):
ollama run deepseek-r1:70b
To begin experimenting with DeepSeek-R1, it is advisable to start with a smaller model to familiarize yourself with the setup and ensure compatibility with your hardware. You can initiate this process by opening your terminal and executing the following command:

ollama run deepseek-r1:8b

Image courtesy from Reddit, via r/macapps
Sending Requests to locally downloaded DeepSeek-R1 via Ollama:

Ollama provides an API endpoint to interact with DeepSeek-R1 programmatically. Ensure that the Ollama server is running locally before making API requests. You can start the server by running:

ollama serve
Once the server is active, you can send a request using curl as follows:

curl -X POST http://localhost:11434/api/generate -d '{
  ""model"": ""deepseek-r1"",
  ""prompt"": ""Your question or prompt here""
}'
Replace ""Your question or prompt here"" with the actual input you wish to provide to the model. This command sends a POST request to the local Ollama server, which processes the prompt using the specified DeepSeek-R1 model and returns the generated response.

Other methods to run/Access the models locally are:
vLLM/SGLang: Used for serving the models locally. Commands like vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B — tensor-parallel-size 2 — max-model-len 32768 — enforce-eager can be used for the distilled versions.


Courtesy: HuggingFace
llama.cpp: You can also use llama.cpp to run the models locally.

See What Others Are Building with DeepSeek-R1:
Running DeepSeek R1 across my 7 M4 Pro Mac Minis and 1 M4 Max MacBook Pro:

DeepSeek R1 1.5B running fully locally in your browser at 60 tok/ sec powered by WebGPU:


RAG app to chat with your PDF files using the DeepSeek R1 model, running locally on your computer.


Running DeepSeek R1 version 1.5B perfectly locally on phone:


Cracking complex math problems with ease! (Thought for ~3200 tokens in about 35 seconds on M4 Max with mlx-lm.):


Conclusions:

This progression from DeepSeek-R1-Zero to DeepSeek-R1 represents an important learning journey in the research. While DeepSeek-R1-Zero proved that pure reinforcement learning could work, DeepSeek-R1 showed how combining supervised learning with reinforcement learning could create an even more capable and practical model.

Collaborations 🤝: Have an interesting AI project in mind? Let’s team up! I’m available for collaboration on AI and machine learning initiatives, and keen to connect with other professionals in the field.",,,
18,https://medium.com/,,https://medium.com/ai-in-plain-english/deepseek-r1-understanding-grpo-and-multi-stage-training-5e0bbc28a281,DeepSeek R1: Understanding GRPO and Multi-Stage Training,"Artificial intelligence has taken a significant leap forward with the release of DeepSeek R1, an open model that challenges OpenAI’s o1 in advanced reasoning tasks. Developed using an innovative technique called Group Relative Policy Optimisation (GRPO) and a multi-stage training approach, DeepSeek R1 sets new benchmarks for AI models in mathematics, coding, and general reasoning.

What sets DeepSeek R1 apart is its ability to solve complex tasks with remarkable accuracy and reasoning depth while maintaining a streamlined training process. This blog dives into the foundational methods, the training pipeline, and the innovations that make DeepSeek R1 an exceptional model in AI research.


Source: OpenAI
Understanding Group Relative Policy Optimization (GRPO)
Group Relative Policy Optimisation (GRPO) is the core innovation driving DeepSeek R1’s exceptional reasoning abilities. Introduced in the DeepSeekMath paper, this reinforcement learning algorithm enhances model training by rethinking how rewards and optimisation are handled. GRPO replaces traditional methods like Proximal Policy Optimisation (PPO) with a simpler and more efficient approach tailored for large language models.

If you’re new to PPO and similar methods, you can check out my previous blogs to get an overview of what they are and how they work.

Key Features of GRPO
No Value Function Model: Unlike PPO, GRPO eliminates the need for a separate value function model. This simplifies training and reduces memory usage, making it more efficient.
Group-Based Advantage Calculation: GRPO leverages a group of outputs for each input, calculating the baseline reward as the average score of the group. This group-based approach aligns better with reward model training, especially for reasoning tasks.
Direct KL Divergence Optimisation: Instead of incorporating KL divergence into the reward signal (as in PPO), GRPO integrates it directly into the loss function, providing finer control during optimisation.
Glimpse of how GRPO Works
Sampling: The model generates multiple outputs for each prompt using the current policy.
Reward Scoring: Each output is scored using a reward function. These scores can be rule-based (e.g., format or accuracy) or outcome-based (e.g., correctness in math or coding).
Advantage Calculation: The average reward from the group serves as the baseline. The relative advantage of each output is calculated based on this baseline, with rewards normalised within the group.
Policy Optimisation: Using the calculated advantages, the policy updates itself to maximise performance. The KL divergence term is incorporated directly into the loss function, ensuring the model balances exploration and stability.

Deepdive into GRPO
If you’re just here for an overview, you can skip this part—the previous section should be enough. I don’t want you to feel overwhelmed, so no need to dive into this if it’s not necessary.

Group Relative Policy Optimisation (GRPO), a variant of Proximal Policy Optimisation (PPO), enhances mathematical reasoning abilities while concurrently optimising the memory usage of PPO.

Group Relative Policy Optimization: Comprehensive Explanation

1. Comparison Between PPO and GRPO
The key difference between Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) lies in their approach to advantage estimation and computational efficiency. While PPO relies on a separate value model, GRPO eliminates this dependency, replacing it with group-based relative advantage estimation, reducing memory and computation costs.


Source: DeepSeekMath
2. Diagram Overview
In the diagram:

PPO:

The policy model generates outputs O for a given input q.
A separate value model predicts a baseline v, used with Generalised Advantage Estimation (GAE) to compute advantages A.
The reward r includes a KL penalty term computed using a reference model and reward model.
This architecture results in significant resource overhead.
GRPO:

Multiple outputs {o1,o2,…,oG} are generated for each q, and their rewards {r1,r2,…,rG} are computed using the reward model.
Group computation normalises these rewards, providing relative advantages A1, A2,..., AG without a value model.
The KL divergence between the trained policy and reference model is added directly to the loss, simplifying training.
PPO: Mathematical Formulation
PPO is an RL algorithm that optimises a policy model by maximising a surrogate objective function while ensuring training stability through clipping-based constraints. The key aspects are described below:




Takeaways from PPO
Advantage Calculation: PPO uses the GAE to reduce the variance in At​, leveraging a learned value function Vψ​ as a baseline.
Clipping Regularization: Clipping in the surrogate objective ensures stability and prevents excessively large policy updates.
KL Divergence Regularization: The KL penalty in the reward discourages the policy from diverging too much from the reference model, promoting stable learning.
GRPO: Mathematical Formulation
Group Relative Policy Optimization (GRPO) simplifies PPO by removing the value model and using group-based relative rewards for baseline estimation. It is designed to efficiently fine-tune large language models (LLMs) while reducing computational overhead.



Takeaways from GRPO
Eliminates the Value Model: GRPO replaces the computationally expensive value model with group-based reward normalization, significantly reducing resource requirements.

Leverages Group Comparisons: By normalizing rewards within a group, GRPO aligns with the pairwise comparison nature of most reward models, ensuring better relative reward estimation.

Simplifies KL Regularization: GRPO directly regularizes the policy with a KL divergence term, avoiding the need for complex KL penalties in the reward.

Outcome Supervision RL with GRPO


Process Supervision RL with GRPO



GRPO training involves iteratively updating the policy and reward model to maintain alignment. The steps are:


Key Differences Between PPO and GRPO
Value Model: PPO uses a value model for advantage estimation, while GRPO eliminates it and relies on group-normalized rewards.
KL Regularization: PPO includes a KL penalty in the reward; GRPO directly regularizes the loss with a KL divergence term.
Reward Granularity: PPO computes token-level rewards directly, while GRPO leverages group-relative rewards normalized across sampled outputs.
Computational Efficiency: GRPO is more efficient due to the removal of the value model and simpler advantage estimation.
Multi-Stage Training of DeepSeek R1
Training an advanced reasoning model like DeepSeek R1 requires more than just raw computational power — it demands a carefully structured training pipeline. To achieve superior reasoning and coherence, the DeepSeek team designed a multi-stage training process that combines supervised fine-tuning (SFT) with reinforcement learning (RL) using GRPO. This approach overcomes challenges like early instability in RL training and ensures that the model excels in diverse tasks.

Stage 1: Base to Supervised Fine-Tuning (SFT)
The journey began with fine-tuning the DeepSeek V3 base model using high-quality, chain-of-thought (CoT) data.
Data Collection:

Generated up to 10k token-long reasoning completions (CoT) using the R1-zero model and human annotators.
Focus:

Enhance readability, coherence, and logical flow in the model’s outputs.
Outcome:

A solid foundation for reinforcement learning, reducing instability during subsequent training stages.
Stage 2: RL for Reasoning
GRPO was introduced to refine the model’s reasoning capabilities in tasks like mathematics, coding, and structured problem-solving.
Rule-Based Rewards:

Focused on accuracy (e.g., solving coding problems, verifying mathematical results).
Enforced formatting rules to ensure clarity, such as enclosing thought processes within specific tags (e.g., ‘reasoning’).
New Reward Signal:

A “language consistency” reward encouraged the model to maintain the same language throughout its outputs.
Outcome:

Significant improvements in reasoning performance, as evidenced by the AIME 2024 pass@1 score jump to 71.0%.
Stage 3: Rejection Sampling and SFT
To expand the model’s capabilities, a large synthetic dataset was generated using Rejection Sampling (RS).
Dataset Creation:

The model from Stage 2 generated 600k reasoning-related samples.
Additional 200k samples focused on general-purpose tasks like writing and role-playing.
Data sourced from DeepSeek V3’s SFT dataset or regenerated with chain-of-thought included.
Focus:

Broaden the model’s expertise beyond reasoning tasks into creative and general-purpose domains.
Outcome:

The model demonstrated greater versatility and coherence across a wider range of tasks.
Stage 4: RL for Helpfulness
In the final stage, GRPO was applied once again, but with a broader focus on helpfulness and harmlessness.
Combination of Reward Models:
Rule-based rewards ensured continued improvement in reasoning and accuracy.
Outcome-based rewards encouraged helpful and safe outputs.
Outcome:

A balanced model capable of handling complex reasoning tasks while maintaining clarity, safety, and user alignment.
Key Insights from Multi-Stage Training
Early SFT Stabilizes RL Training: Fine-tuning the base model before applying RL techniques reduces training instability and accelerates convergence.
Rule-Based Rewards Are Effective: Simple, targeted rewards (accuracy, format) often outperform complex reward models.
Rejection Sampling Improves Versatility: Synthetic datasets generated through rejection sampling enhance the model’s adaptability to varied tasks.
By strategically alternating between supervised fine-tuning and reinforcement learning, the DeepSeek team overcame the challenges of RL cold starts and task-specific overfitting. This multi-stage pipeline ensured that DeepSeek R1 could excel in both reasoning and broader applications.

Stay tuned, will share more insights on it soon!",,,
19,https://llm.extractum.io/static/llm-news/,,https://medium.com/@cognidownunder/gemini-2-0-googles-leap-into-the-agentic-era-of-ai-fc3390469f44,Gemini 2.0: Google’s Leap into the Agentic Era of AI,"Google has just pulled back the curtain on Gemini 2.0, and it’s not just another incremental update in the AI arms race. This is the search giant’s most ambitious foray into what they’re calling the “agentic era” of artificial intelligence. It’s a bold claim, but after diving into the details, it’s clear that Gemini 2.0 is poised to redefine our expectations of what AI can do.

The Dawn of Proactive AI
For years, we’ve been interacting with AI models that are essentially glorified question-answering machines. Ask them something, and they’ll spit out a response. But Gemini 2.0 is designed to flip that script entirely.

Thinking Ahead, So You Don’t Have To
The standout feature of Gemini 2.0 is its agentic capabilities. This isn’t just marketing fluff; the model is genuinely designed to “think multiple steps ahead.” It’s like having a digital assistant that doesn’t just wait for your commands but anticipates your needs and starts working on solutions before you even ask.

This proactive approach is a game-changer. Imagine an AI that doesn’t just tell you the weather but also suggests rescheduling your outdoor plans and offers alternative indoor activities. That’s the level of foresight we’re talking about here.

Multimodal Mastery
One of the most impressive aspects of Gemini 2.0 is its multimodal prowess. This isn’t just about understanding different types of input; it’s about seamlessly blending various forms of output.

Beyond Text: The Rich Tapestry of AI Responses
Gemini 2.0 supports native generation of images and audio. This means responses can be a rich mix of text, visuals, and even multilingual audio. It’s not just about conveying information; it’s about creating a more immersive and intuitive interaction.

The Need for Speed: Gemini 2.0 Flash
In the world of AI, speed is king, and Google knows it. Enter Gemini 2.0 Flash, an experimental version that’s pushing the boundaries of what’s possible in terms of performance.

Twice as Fast, Twice as Good
Not only does Flash outperform its predecessor, the 1.5 Pro model, on key benchmarks, but it does so while delivering responses twice as fast. This isn’t just incremental progress; it’s a quantum leap in AI responsiveness.

Native Tool Integration: The Swiss Army Knife of AI
One of the most exciting aspects of Gemini 2.0 is its native integration with a suite of Google tools. This isn’t just about having access to information; it’s about seamlessly executing tasks in the real world.

From Search to Maps: A World of Possibilities
Imagine asking your AI assistant to not just find a restaurant but to actually book a table, provide turn-by-turn directions, and even suggest menu items based on your dietary preferences. With Gemini 2.0’s integration of Google Search, Maps, and other tools, this level of comprehensive assistance is becoming a reality.

The Memory of an Elephant (Well, Almost)
One of the most frustrating aspects of current AI assistants is their goldfish-like memory. Gemini 2.0 takes a significant step forward in this regard.

10 Minutes of Recall: A New Era of Contextual Understanding
With up to 10 minutes of in-session recall, Gemini 2.0 can maintain context and personalization in a way that feels much more human. This isn’t just about remembering facts; it’s about understanding the flow of a conversation and adapting responses accordingly.

Research Prototypes: A Glimpse into the Future
Google isn’t just resting on its laurels with Gemini 2.0. They’re already exploring the next frontiers of AI through several ambitious research prototypes.

Project Astra: The Universal AI Assistant
Project Astra is Google’s vision of a truly universal AI assistant. Built on Gemini 2.0, it’s designed to seamlessly integrate with your daily life, providing real-time support and information across a wide range of tasks.

Project Mariner: Reimagining Web Interaction
Project Mariner is tackling one of the most common interfaces we use daily: the web browser. This prototype can understand and reason across various types of information in the browser, from text and code to images and forms.

Jules: Your AI Coding Companion
For developers, Jules represents a tantalizing glimpse into the future of coding. This AI-powered code agent can analyze, debug, and suggest solutions, potentially revolutionizing the development process.

Availability: Coming Soon to a Device Near You
While developers can already access Gemini 2.0 Flash through Google AI Studio and Vertex AI, the rest of us won’t have to wait long to experience its capabilities.

From Chat to Chrome: The Rollout Begins
A chat-optimized version of Gemini 2.0 is already available in the Gemini app, with plans to integrate it into more Google products early next year. This gradual rollout strategy allows Google to refine and optimize the model based on real-world usage.

The Bottom Line: A New Chapter in AI
Gemini 2.0 represents more than just an upgrade; it’s a fundamental shift in how we think about and interact with AI. By combining proactive thinking, multimodal capabilities, and seamless integration with real-world tools, Google is laying the groundwork for a future where AI is not just a tool we use, but a partner we collaborate with.

As we stand on the brink of this new era, one thing is clear: the line between human and artificial intelligence is becoming increasingly blurred. Gemini 2.0 isn’t just imitating human-like responses; it’s starting to think and act in ways that are uniquely its own. And that, perhaps, is the most exciting and challenging aspect of all.

FAQ
Q: How does Gemini 2.0 differ from previous AI models?
A: Gemini 2.0 introduces agentic capabilities, multimodal output, and native tool integration, making it more proactive and versatile than previous models.

Q: Can Gemini 2.0 generate images and audio?
A: Yes, Gemini 2.0 supports native generation of images and audio, allowing for more diverse and rich responses.

Q: How can developers access Gemini 2.0?
A: Developers can access Gemini 2.0 Flash through the Gemini API in Google AI Studio and Vertex AI.

Q: What is the context window size for Gemini 2.0?
A: Gemini 2.0 Flash and Flash-Lite have a context window of 1 million tokens, while the experimental Pro version boasts a 2 million token context window.

Q: How does Gemini 2.0 improve memory and contextual understanding?
A: Gemini 2.0 features up to 10 minutes of in-session recall, allowing for more personalized and contextually relevant interactions.

#Gemini2 #GoogleAI #ArtificialIntelligence #AIInnovation #TechAdvancement

“proactive AI assistant”, “multimodal AI capabilities”, “AI-powered code debugging”, “next-generation AI models”, “AI with extended memory recall”",,,
20,https://medium.com/,,https://medium.com/@sahin.samia/s1-32b-model-explained-beating-openais-o1-with-just-1-000-training-examples-8f1e90957c1b,"s1–32B Model Explained : Beating OpenAI’s o1 with Just 1,000 Training Examples","Over the past decade, AI progress has followed a simple rule: bigger is better — larger models and more data lead to better performance. But what if we could enhance reasoning without increasing model size?

This is exactly what s1–32B achieves. Inspired by OpenAI’s o1 model, which demonstrated exceptional reasoning but kept its methodology closed, researchers sought a simpler, more efficient approach. Instead of massive reinforcement learning pipelines, they fine-tuned an existing model using just 1,000 carefully selected examples and introduced a lightweight technique called budget forcing — a method that optimizes reasoning dynamically at test time.

The result? s1–32B surpasses OpenAI’s o1-preview on advanced math and science benchmarks while using a fraction of the compute and data. This breakthrough, known as test-time scaling, proves that we can push model performance even after training by refining how it reasons in real time.

In this blog, we’ll dive into the mechanics behind s1–32B, the power of budget forcing, and why this open-source approach is redefining AI reasoning. Based on the paper, s1: Simple Test-Time Scaling (Muennighoff et al., 2025), this work is fully open-source and available on GitHub.. Understanding Test-Time Scaling

What is Test-Time Scaling?
Traditionally, language model improvements have come from scaling up train-time compute — bigger datasets, larger models, and longer training times. However, test-time scaling flips this approach on its head. Instead of increasing compute during training, it allocates additional computation at inference time to improve reasoning performance.

Imagine a student given extra time to double-check their answers on an exam. Even with the same knowledge, they may perform significantly better just by thinking more carefully. Test-time scaling applies a similar concept to language models, allowing them to refine their answers after they’ve already been trained.

How Test-Time Scaling Works
Instead of treating inference as a one-shot process, test-time scaling introduces methods that enable models to reflect, verify, and refine their responses. Some common techniques include:

Iterative Reasoning: Encouraging the model to break down complex problems step by step.
Majority Voting (Parallel Scaling): Generating multiple answers and selecting the most frequent or highest-confidence response.
Monte Carlo Tree Search (MCTS): Using search algorithms to explore multiple reasoning paths before choosing the best one.
While these techniques have shown promise, they often require expensive compute resources or complex modifications to existing architectures. This is where s1–32B takes a different approach — by using a simple but effective decoding trick called budget forcing.

The s1 Approach: Minimal Data, Maximum Impact
For years, the prevailing belief in AI research has been that more data leads to better models. The largest and most capable language models today — such as GPT-4 and Gemini — are trained on massive datasets spanning trillions of tokens. However, s1–32B challenges this assumption by achieving state-of-the-art reasoning performance using just 1,000 carefully selected training examples.

The s1K Dataset: Quality Over Quantity
Instead of collecting a massive dataset, the researchers behind s1–32B focused on curating a small but highly effective dataset called s1K. This dataset consists of 1,000 reasoning-intensive questions, each paired with detailed step-by-step solutions.

To ensure maximum efficiency, the dataset was selected based on three key principles:

Quality — Only well-structured, high-quality examples were included, filtering out poorly formatted or ambiguous questions.
Difficulty — Questions were chosen specifically because they were challenging for existing models, ensuring the dataset would push the model’s reasoning abilities.
Diversity — The dataset spans 50+ domains, including advanced math, physics, logic puzzles, and standardized test problems.
This highly selective approach meant that the model learned to reason effectively with far fewer examples than traditional models. Rather than memorizing vast amounts of data, s1–32B learned from high-quality, reasoning-rich examples that forced it to develop structured thought processes.

Training: Fast, Efficient, and Cost-Effective
Training s1–32B on the s1K dataset took only 26 minutes on 16 H100 GPUs — a stark contrast to the weeks or months required to train state-of-the-art models on massive datasets. This efficiency makes s1–32B one of the most sample-efficient reasoning models ever built.

So how does it achieve better reasoning performance despite its small dataset? The answer lies in a novel test-time intervention technique called budget forcing — a simple yet powerful trick that improves reasoning without additional training.

Budget Forcing: A Simple Yet Powerful Decoding Trick
Improving a language model’s reasoning ability usually involves training on more data or fine-tuning with advanced techniques like reinforcement learning. However, s1–32B takes a different approach — instead of modifying the model itself, it changes how the model thinks at test time.

The key innovation? Budget forcing — a lightweight, test-time-only intervention that guides the model’s reasoning without requiring extra training data.

How Budget Forcing Works
At its core, budget forcing is a decoding-time control method that regulates how long the model spends reasoning before generating an answer. This is done in two ways:

Limiting Reasoning (Early Stopping)

If the model spends too much time thinking, budget forcing terminates the reasoning process by appending a forced stop token (e.g., “Final Answer:”).
This prevents the model from getting stuck in unnecessary loops or excessive computations.
Extending Reasoning (Encouraging More Thought)

If the model tries to stop too early, budget forcing prevents it from stopping and appends the word “Wait” to its reasoning process.
This forces the model to continue reflecting, often leading it to identify and correct mistakes before finalizing an answer.
Example: Self-Correction in Action


Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393
This simple yet highly effective strategy helps the model avoid common reasoning errors and produce more reliable outputs without additional training

Comparison to Other Test-Time Scaling Methods
While budget forcing is a straightforward solution, other test-time scaling methods exist. How does it compare?


Why Budget Forcing Works So Well
Unlike majority voting or MCTS, which require significantly more compute, budget forcing is a lightweight and controllable method that can be applied on-the-fly during inference.

Scalability: Works on both small and large models without retraining.
Efficiency: Requires no extra fine-tuning or RL — just a simple decoding trick.
Performance Gains: Helps s1–32B outperform OpenAI’s o1-preview on advanced math and reasoning benchmarks.
Performance and Benchmarks: How s1–32B Stacks Up
Now that we’ve explored how s1–32B is trained and how budget forcing enhances its reasoning, the next question is: How well does it actually perform?

To measure its effectiveness, s1–32B was evaluated on three widely used benchmarks for advanced reasoning:


Created using data from Paper
Key Insights from These Results:
✅ s1–32B outperforms OpenAI’s o1-preview by up to 27% on AIME24 — a highly competitive math benchmark.
✅ It achieves 93% accuracy on MATH500, proving its effectiveness in structured problem-solving.
✅ While slightly behind on GPQA Diamond, it still demonstrates strong scientific reasoning, despite training on just 1,000 samples.

Test-Time Scaling in Action
One of the biggest advantages of s1–32B is its ability to scale performance dynamically at test time. Thanks to budget forcing, the model’s accuracy increases when given more test-time compute.

Example: Scaling Performance on AIME24

Base performance (default reasoning time): 50% accuracy
With extended budget forcing (more reasoning steps): 57% accuracy
Why does this matter?

Most AI models have fixed performance after training.
s1–32B improves its answers dynamically — scaling its reasoning depth without retraining.
This bridges the gap between static fine-tuned models and more expensive reinforcement learning techniques.
Comparing s1–32B with Other Open Models

Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393
While OpenAI and Google’s latest models remain closed-source, s1–32B is fully open and reproducible.

How It Stands Against Other Open-Weight Models:

More sample-efficient than DeepSeek-R1, which required 800K+ reasoning samples to achieve similar results.
Trained in just 26 minutes, while other models often require weeks of GPU time.
Matches Google’s Gemini 2.0 Flash Thinking on AIME24, proving its effectiveness at structured reasoning.

Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393
The Future of Test-Time Scaling: What’s Next?
The success of s1–32B and budget forcing proves that test-time scaling is a powerful alternative to traditional training-based improvements. Instead of relying on massive datasets and computationally expensive reinforcement learning, test-time scaling allows us to enhance reasoning dynamically — even after a model has been trained.

But this is just the beginning. What’s next for test-time scaling?

Pushing the Boundaries of Test-Time Scaling
1️⃣ Improving Budget Forcing for Even Better Self-Correction
While budget forcing is simple and effective, there’s room for optimization.
🔹 Smarter prompts — Instead of just appending “Wait”, we could dynamically modify the reasoning process.
🔹 Adaptive stopping criteria — Can we predict the optimal amount of reasoning needed for different problems?
🔹 Avoiding loops — Sometimes, excessive budget forcing can lead to repetitive reasoning cycles. New techniques could help mitigate this.

2️⃣ Hybrid Approaches: Combining Parallel and Sequential Scaling
Parallel scaling methods like majority voting aggregate multiple model responses to find the best answer.
Sequential scaling, like budget forcing, extends reasoning iteratively within a single model run.
Combining both could lead to even better results — leveraging majority voting while keeping compute costs low.
3️⃣ Beyond Language Models: Can Test-Time Scaling Work for Multimodal AI?
Could test-time scaling improve image reasoning (e.g., visual question answering)?
Could budget forcing help reinforcement learning agents make better decisions by spending more time “thinking” before acting?
As AI expands into video understanding and robotics, test-time scaling could help models analyze situations more deeply before taking action.
Open-Source Innovation: Why s1–32B Matters
Unlike OpenAI’s o1 model or Google’s Gemini, s1–32B is fully open-source — allowing researchers to experiment, improve, and build on this work.

🔗 Try it yourself! 👉 GitHub: Simple Scaling

Here is how you can use this model from huggingface:

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

DEVICE = ""cuda"" if torch.cuda.is_available() else ""cpu""
model_name = ""simplescaling/s1-32B""

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=""auto"",
    device_map=""auto""
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = ""How many r in raspberry""
messages = [
    {""role"": ""system"", ""content"": ""You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.""},
    {""role"": ""user"", ""content"": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors=""pt"").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
Code source: GitHub: Simple Scaling

With more researchers exploring test-time reasoning enhancements, we could be on the verge of a new AI paradigm — one that prioritizes efficiency, adaptability, and reasoning depth over brute-force training scale.

Conclusion: The Shift from Bigger to Smarter AI
For years, AI progress has been driven by bigger models, more training data, and massive compute. But s1–32B proves that there’s another way — one that focuses on smarter reasoning instead of just larger models.

✅ A 1,000-sample dataset beats 800K+ data-heavy models.
✅ A simple decoding trick improves reasoning at test time.
✅ s1–32B rivals proprietary models while being fully open-source.

As AI evolves, test-time scaling could become a defining technique for future models — helping AI think more efficiently, dynamically, and intelligently.

Reference:
Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393",,,
21,https://news.hada.io/,,https://news.hada.io/topic?id=19190,DeepScaleR: RL을 활용한 1.5B 모델로 O1-Preview 능가하기,"▲
GN⁺: DeepScaleR: RL을 활용한 1.5B 모델로 O1-Preview 능가하기 (pretty-radio-b75.notion.site)
5P by neo 5일전 | ★ favorite | 댓글 1개
DeepScaleR-1.5B-Preview: Deepseek-R1-Distilled-Qwen-1.5B 모델을 강화 학습(RL)으로 미세 조정한 모델
AIME2024 Pass@1 정확도 43.1% 달성 (기본 모델 대비 +14.3% 향상),
→ OpenAI o1-preview 성능 능가!
3,800 A100 GPU 시간($4500)으로 훈련 → 70,000 A100 GPU 시간 대비 18.42배 효율적인 RL 스케일링
데이터셋, 코드, 훈련 로그 오픈소스 공개 → 누구나 RL을 활용한 지능 확장을 실험 가능
RL을 활용한 소형 모델 강화
Deepseek-R1은 OpenAI o1과 견줄 수 있는 오픈소스 모델이지만, 정확한 훈련 과정은 비공개
RL을 활용하여 적은 계산량으로 강력한 추론 모델을 개발하는 방법을 연구
기존 RL의 가장 큰 한계는 고비용:
→ Deepseek-R1의 실험을 재현하려면 최소 70,000 A100 GPU 시간 필요
해결책:
고성능 지식 증류(distillation) 모델 활용
RL을 점진적으로 확장하는 ""Iterative Lengthening"" 기법 도입 → 계산량 3,800 A100 GPU 시간으로 절감
데이터셋 구축
AIME(1984-2023) + AMC(2023 이전) + Omni-MATH + Still 데이터셋 사용

데이터 정제 과정:

정답 추출: gemini-1.5-pro-002를 활용해 공식 해설에서 정답 추출
중복 제거: sentence-transformers/all-MiniLM-L6-v2 임베딩 기반으로 유사 문제 제거
채점 불가능 문제 필터링: sympy를 활용한 자동 평가가 어려운 문제 제거
최종적으로 40,000개 문제-정답 쌍 확보, 향후 데이터 확장 예정

보상 함수(Reward Function)
Deepseek-R1과 동일하게 ""Outcome Reward Model (ORM)"" 적용:

1점: 올바른 형식의 정답 (sympy 검증 통과)
0점: 잘못된 정답, 형식 오류 (<think>...</think> 누락 등)
""과정 기반 보상(Process Reward Model, PRM)""을 사용하지 않는 이유:

보상 해킹(reward hacking) 방지 → 모델이 형식만 따라가려는 부작용 방지
""Iterative Lengthening"": RL 학습을 단계적으로 확장하는 기법
Step 1: 8K 컨텍스트로 RL 학습 시작
이유:
잘못된 답변은 평균 20,346 토큰, 정답은 6,395 토큰 → 긴 응답이 오답 가능성 증가
초기부터 긴 컨텍스트로 학습하면 비효율적 → 8K로 먼저 최적화
결과:
AIME Pass@1 28.9% → 33.9% (+5%) 향상
불필요한 토큰 수 감소 → 평균 응답 길이 10,484 토큰 감소
Step 2: 16K 컨텍스트로 확장
훈련 1,000스텝 이후, 모델이 더 길게 사고(추론)하려는 경향을 보임
하지만 8K 한계로 인해 학습 효과가 제한됨 → 16K로 확장
장점:
처음부터 16K로 훈련하는 것보다 2배 이상 빠름 (평균 응답 길이 3,000 → 9,000 토큰 방지)
AIME2024 정확도 38% 도달
Step 3: ""24K Magic"" - 최종 성능 향상
16K에서 성능이 정체 → 24K 컨텍스트로 마지막 확장
결과적으로 AIME2024 Pass@1 정확도 43.1% 도달, OpenAI o1-preview 능가!
최종 평가 결과
DeepScaleR 모델은 AIME, MATH 500, AMC 2023, Minerva Math, OlympiadBench 등 여러 수학 벤치마크에서 평가됨
AIME2024 기준, DeepScaleR-1.5B-Preview의 정확도는 43.1%로, OpenAI o1-preview 모델보다 우수함
MATH 500, AMC 2023 등에서도 1.5B 모델임에도 불구하고 7B 모델과 동등하거나 더 높은 성능을 기록
이전 연구(RL 기반 rStar, PRIME, SimpleRL)와 비교해도 최고의 효율성을 보여줌
핵심 요약 (Key Takeaways)
소형 모델에서도 RL 확장이 가능함

기존에는 RL이 대형 모델에만 효과적이라는 인식이 있었음
하지만 고품질 데이터로 미세 조정된 작은 모델도 RL을 통해 강력한 추론 능력을 학습 가능
DeepScaleR는 28.9% → 43.1% (AIME 정확도) 향상
""Iterative Lengthening"" 기법으로 효과적인 길이 확장 가능

기존 연구에서는 16K 이상 컨텍스트에서 성능 향상이 미미함을 보고
8K → 16K → 24K 점진적 확장을 통해 성능 최대화
결론: RL 스케일링의 대중화
DeepScaleR-1.5B-Preview는 O1-preview를 능가하는 최초의 오픈소스 RL 모델
3,800 A100 GPU 시간($4500)만으로도 고성능 모델 구축 가능 → 저비용 RL 연구의 가능성 증명
오픈소스 커뮤니티와 함께 RL 기반 추론 모델의 발전을 지속할 예정",,,
22,https://discuss.pytorch.kr/,,https://discuss.pytorch.kr/t/s1-test-time-scaling/6060,s1: 테스트 시점 스케일링(Test-Time Scaling)을 단순하게 구현하는 방법에 대한 연구,"이 연구에서는 가장 단순하면서도 효과적인 테스트 시점 스케일링 기법을 탐색하는 것을 목표로 하였습니다. 연구팀은 Budget Forcing이라는 개념을 도입하여, 최소한의 데이터와 연산량으로도 모델의 성능을 크게 향상시킬 수 있는지를 검증하였습니다. 특히, s1-32B 모델을 개발하여 단 1,000개의 고품질의 데이터로 구성된 s1K 데이터셋만을 사용하여 기존 모델보다 뛰어난 추론 성능을 기록하였습니다. 이는 소규모 데이터셋만으로도 테스트 시점 조정을 통해 성능을 극대화할 수 있음을 보여줍니다. 즉, 이 연구에서는 테스트 시점에서 연산량을 조절하여 모델의 성능을 최적화하는 전략이 실제로 효과적인지를 검증하고, 향후 모델 개발에 미칠 영향을 분석하고자 합니다. 추론 데이터셋 s1K 소개 및 생성 방법 s1 연구에서는 학습 데이터를 최소화하면서도 강력한 추론 능력을 학습할 수 있는가라는 질문을 중심으로 데이터셋을 설계하였습니다. 이를 위해 연구팀은 s1K라는 새로운 데이터셋을 구축하였으며, 이는 1,000개의 고품질 데이터로 이루어진 소규모 데이터셋입니다. 일반적으로 대형 언어 모델의 학습에는 수백만 개의 데이터가 필요하다고 알려져 있지만, 본 연구에서는 소량의 데이터를 효과적으로 선별하여 훈련하면 강력한 추론 능력을 학습할 수 있는지 검증하는 것을 목표로 하였습니다. 이미지 좌측: 추론 데이터셋 s1K 구성, 우측: s1-32B 모델과 다른 모델들의 샘플 효율성 비교(데이터셋 규모와 MATH500 정확도) 이미지 좌측: 추론 데이터셋 s1K 구성, 우측: s1-32B 모델과 다른 모델들의 샘플 효율성 비교(데이터셋 규모와 MATH500 정확도) 1248×642 117 KB s1K 데이터셋을 구축하기 위해 연구팀은 먼저 다양한 분야에서 59,029개의 문제를 수집하였습니다. 이 데이터는 수학, 과학, 논리, 언어학, 확률 등 다양한 도메인을 포함하며, 문제의 난이도와 유형도 광범위하게 분포되어 있습니다. 이후 연구팀은 **품질(Quality), 난이도(Difficulty), 다양성(Diversity)**이라는 세 가지 기준을 설정하여 데이터를 필터링하였습니다. 먼저 품질(Quality) 기준을 적용하여, 잘못된 형식의 데이터나 의미가 명확하지 않은 문제들을 제거하였습니다. 이후 난이도(Difficulty) 기준을 적용하여, 문제 해결을 위해 깊은 추론이 필요한 문제들을 선별하였습니다. 마지막으로 다양성(Diversity) 기준을 적용하여, 특정 유형의 문제에 치우치지 않도록 다양한 주제의 문제들을 균형 있게 포함하도록 구성하였습니다. 이러한 과정을 거쳐 최종적으로 1,000개의 고품질 문제(s1K)가 선정되었습니다. 본 연구에서 중요한 점은, s1K 데이터셋이 기존 대형 데이터셋보다 훨씬 적은 양의 데이터를 포함하고 있음에도 불구하고, 효과적인 테스트 시점 조정을 통해 모델의 성능을 크게 향상시킬 수 있다는 점 입니다. 이는 향후 AI 모델을 개발하는 과정에서 데이터 수집과 모델 훈련 비용을 줄이는 데 중요한 시사점을 제공합니다. 테스트 시점 스케일링(Test-Time Scaling)과 Budget Forcing 기존의 대형 언어 모델은 주어진 입력에 대해 즉각적인 응답을 생성하도록 설계되어 있습니다. 하지만 복잡한 문제를 해결할 때는 한 번의 계산만으로 최적의 답을 도출하는 것이 어렵기 때문에, 모델이 더 깊이 사고하고 다각적인 검토를 수행할 수 있도록 유도하는 방법이 필요합니다. 이러한 접근 방식 중 하나가 **테스트 시점 스케일링(Test-time Scaling)**이며, 이는 모델을 훈련하는 과정에서가 아니라, 테스트(추론) 시점에서 추가적인 연산을 수행하여 성능을 향상시키는 방법론을 의미합니다. 테스트 시점 스케일링에는 크게 두 가지 방식이 있습니다: 첫 번째는 **병렬 방식(Parallel Scaling)**으로, 모델이 여러 개의 독립적인 출력을 생성한 후 가장 적절한 답을 선택하는 방법입니다. 이는 다수의 출력을 비교하고 다수결 방식(Majority Voting)이나 후처리 알고리즘을 활용하여 최종 답을 결정하는 방식으로, 테스트 과정에서 여러 번의 독립적인 추론을 수행하는 것이 특징입니다. 하지만 이 방식은 계산량이 증가하며, 반드시 최적의 답을 도출하는 것은 아니기 때문에 한계가 존재합니다 . 두 번째 방식은 **순차 방식(Sequential Scaling)**으로, 모델이 하나의 답을 생성하는 과정에서 이전 사고 과정(Reasoning Trace)을 활용하여 점진적으로 더 나은 답을 찾아가는 방식입니다. 이 접근법은 단계적인 사고 과정이 필요한 문제에서 유용하게 작용할 수 있으며, 연구진은 이를 최적화하기 위해 Budget Forcing 기법을 도입하였습니다 . Budget Forcing 기법을 활용한 모델 추론 과정: 모델은 ""...is 2.""에서 출력을 완료하려고 했으나, 'Wait' 토큰을 추가하여 답을 스스로 개선하도록 함 Budget Forcing 기법을 활용한 모델 추론 과정: 모델은 ""...is 2.""에서 출력을 완료하려고 했으나, 'Wait' 토큰을 추가하여 답을 스스로 개선하도록 함 942×800 41.2 KB Budget Forcing은 모델의 사고 과정(Thinking Phase)을 인위적으로 조절하는 기법입니다. 일반적으로 언어 모델은 최대한 빨리 답을 생성하려는 경향이 있으며, 이는 복잡한 문제를 해결하는 데 있어 충분한 사고를 거치지 않고 성급한 결론을 내리는 문제를 초래할 수 있습니다. 연구팀은 이 문제를 해결하기 위해 두 가지 기법을 적용하였습니다. 첫째, 모델이 일정한 토큰 수 이상을 생성할 때까지 사고 과정을 유지하도록 설정하여, 모델이 충분한 사고를 거친 후 답을 생성하도록 유도하였습니다. 둘째, 모델이 답을 생성하려고 할 때 ""Wait""이라는 추가적인 신호를 입력하여, 모델이 지금까지의 출력을 한 번 더 검토할 기회를 제공하였습니다 . 이러한 방법을 통해, 모델이 문제를 더 철저히 분석하고 중간 과정에서 오류를 수정하는 능력이 향상되었습니다. 특히, Budget Forcing을 적용한 모델은 문제를 해결하는 과정에서 보다 체계적으로 사고하는 경향을 보였으며, 최종적으로 높은 정확도를 달성할 수 있었습니다. 연구팀은 Budget Forcing이 적용된 s1-32B 모델이 기존의 o1-preview 모델보다 더 나은 성능을 보이며, 특히 수학적 추론과 과학 문제 해결에서 뛰어난 성능을 발휘했음을 실험을 통해 확인하였습니다 . 실험 결과 및 성능 정리, Ablation AIME(좌측), MATH500(중앙), GPQA Diamond(우측) 벤치마크에서의 질문 및 s1-32B의 답변 예시 AIME(좌측), MATH500(중앙), GPQA Diamond(우측) 벤치마크에서의 질문 및 s1-32B의 답변 예시 1028×1319 220 KB 본 연구에서는 Budget Forcing을 적용한 s1-32B 모델을 다양한 벤치마크 데이터셋을 활용하여 평가하였습니다. 실험에 사용된 데이터셋은 다음과 같습니다. 첫째, AIME24는 미국 수학 올림피아드(AIME) 문제들로 구성된 데이터셋으로, 복잡한 수학적 사고가 요구되는 고난이도 문제들로 이루어져 있습니다. 둘째, MATH500은 다양한 수학 문제들을 포함한 일반적인 평가용 데이터셋입니다. 셋째, GPQA Diamond는 박사 수준의 과학 문제들을 포함한 데이터셋으로, 고난이도 질문들에 대한 모델의 성능을 측정하는 데 사용되었습니다. s1 모델과 o1, Gemini 2.0 Flash Think, Qwen, QwQ, r1 등 기존의 주요 모델들의 성능 비교: AIME24, MATH500, GPQA Diamond s1 모델과 o1, Gemini 2.0 Flash Think, Qwen, QwQ, r1 등 기존의 주요 모델들의 성능 비교: AIME24, MATH500, GPQA Diamond 1000×1268 140 KB 실험 결과, s1-32B 모델은 기존의 OpenAI o1-preview 모델보다 높은 성능을 기록하였습니다 . 특히 Budget Forcing을 적용한 경우, AIME24 문제에서 성능이 50%에서 56.7%로 상승하였으며, 이는 단순한 기법만으로도 테스트 시점에서 성능을 극대화할 수 있음을 보여줍니다 . 또한 MATH500과 GPQA Diamond에서도 기존 모델보다 뛰어난 성능을 보였으며, 이는 1,000개 샘플만을 사용한 미세 조정만으로도 모델의 추론 능력을 강화할 수 있음을 시사합니다. Ablation 연구 – s1K 데이터셋의 효과 분석 Ablation 연구 – s1K 데이터셋의 효과 분석 Budget Forcing(BF)과 다양한 스케일링 기법(Token/Step/Class-Conditional Control, Rejection Sampling) 비교 Budget Forcing(BF)과 다양한 스케일링 기법(Token/Step/Class-Conditional Control, Rejection Sampling) 비교 추가적인 Ablation 연구를 통해, 데이터의 양과 다양성이 모델 성능에 미치는 영향을 분석하였습니다. 연구팀은 모델을 훈련할 때 데이터의 수를 달리하여 실험을 진행하였으며, 이를 통해 적절한 난이도와 다양성을 가진 소규모 데이터셋(s1K)이 효과적인 모델 학습에 중요한 역할을 한다는 사실을 발견하였습니다 . 또한 Budget Forcing을 사용하지 않은 경우보다 사용한 경우에 더 높은 성능을 달성하였으며, 이는 모델이 충분한 사고 과정을 거칠 수 있도록 유도하는 것이 매우 중요하다는 것을 보여줍니다 . Budget Forcing을 적용한 순차(Sequential) 방식(a)과 다수결 투표의 병렬(Parallel) 방식(b)의 테스트 시점 스케일링 성능 비교 Budget Forcing을 적용한 순차(Sequential) 방식(a)과 다수결 투표의 병렬(Parallel) 방식(b)의 테스트 시점 스케일링 성능 비교 1245×678 150 KB 본 연구는 또한 Budget Forcing이 다른 테스트 시점 스케일링 기법들과 비교하여 어떤 이점을 가지는지 평가하였습니다. 다수결 방식(Majority Voting)과 같은 병렬 스케일링 기법과 비교한 결과, Budget Forcing을 활용한 순차적(Sequential) 스케일링 방식이 더 효과적인 것으로 나타났습니다. 이는 모델이 여러 번 생각하고 답을 수정하는 과정이 최종적인 성능 향상에 중요한 역할을 한다는 것을 의미합니다 . 논의, 향후 과제 및 결론 본 연구는 소규모 데이터 환경에서도 테스트 시점 스케일링을 활용하여 강력한 AI 모델을 개발할 수 있음을 입증하였습니다. 특히, 1,000개의 고품질 데이터셋(s1K)만을 활용하여도 효과적인 학습이 가능하며, 이를 통해 모델의 연산량을 최적화하면서도 높은 성능을 유지할 수 있음을 보였습니다. 이는 향후 AI 모델을 최적화하는 데 있어 중요한 시사점을 제공합니다. 하지만 본 연구에서도 몇 가지 한계점이 존재합니다: 첫째, Budget Forcing 기법이 모든 유형의 문제에서 동일한 효과를 발휘하는지에 대한 추가적인 연구가 필요합니다. 예를 들어, 특정한 도메인에서는 모델이 추가적인 사고 과정을 거친다고 해서 반드시 더 좋은 답을 생성하는 것이 아닐 수도 있습니다. 둘째, 더 긴 문맥을 다룰 수 있도록 모델을 확장하는 방법에 대한 연구가 필요합니다. 현재의 접근 방식은 일정한 토큰 수 내에서 사고 과정을 조정하는 것이기 때문에, 보다 긴 문맥을 다룰 수 있도록 설계된 새로운 모델 아키텍처가 필요할 가능성이 있습니다 . 향후 연구에서는 Budget Forcing을 더욱 정교하게 설계하여, 다양한 도메인에서의 적용 가능성을 넓히는 방향으로 확장될 수 있을 것입니다. 또한, 강화 학습과의 결합을 통해 Budget Forcing을 보다 효과적으로 활용하는 방법을 탐색할 수도 있습니다. 이러한 연구가 진행된다면, 보다 효율적인 테스트 시점 스케일링 기법이 개발될 가능성이 있으며, 이는 향후 인공지능 모델의 성능을 향상시키는 중요한 기술적 돌파구가 될 것입니다.",,,
23,https://www.aimodels.fyi/,,https://www.aimodels.fyi/papers/arxiv/tensorllm-tensorising-multi-head-attention-enhanced-reasoning,TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs,"Overview
Introduces TensorLLM, a novel approach to compress and enhance large language models
Uses tensor decomposition to improve multi-head attention mechanisms
Achieves 3.5x compression while maintaining model performance
Demonstrates enhanced reasoning capabilities on benchmark tasks
Provides new insights into attention head interactions
Plain English Explanation
Large language models are like massive pattern-recognition machines that require enormous computing power. TensorLLM makes these models more efficient by restructuring how they process information.

Think of traditional attention mechanisms as multiple spotlights examining different aspects of text. TensorLLM reorganizes these spotlights into a more compact form using mathematical techniques called tensor decomposition - similar to how a jpeg compresses an image while keeping the important details.

The researchers found that this reorganization not only saves space but actually helps the model think better. It's like reorganizing a messy desk - you end up with both more space and a better workflow.

Key Findings
Achieved 3.5x compression while maintaining 98% of original performance
Enhanced reasoning capabilities showed 5% improvement on complex tasks
Discovered new patterns in how attention heads collaborate
Reduced computational costs by 65% during inference
Maintained model accuracy across multiple benchmark tests
Technical Explanation
The core innovation lies in applying Tucker decomposition to multi-head attention layers. The attention mechanism is reshaped into a higher-dimensional tensor, then decomposed into smaller, more manageable components.

The process preserves crucial relationships between attention heads while eliminating redundant patterns. This restructuring allows for more efficient information flow and better feature extraction.

The implementation includes adaptive scaling factors that automatically adjust based on input complexity, ensuring optimal performance across different types of tasks.

Critical Analysis
The approach shows promising results but has some limitations. The compression ratio varies significantly depending on model size, with smaller models seeing less benefit. The method also requires careful tuning of decomposition parameters.

Some questions remain about long-term stability and performance on extremely long sequences. The paper doesn't fully address how the method scales to models larger than 7B parameters.

The transformer architecture modifications might need additional validation across different model families and tasks.

Conclusion
TensorLLM represents a significant advance in making large language models more practical and efficient. The ability to compress models while improving their reasoning capabilities could accelerate the deployment of AI systems in resource-constrained environments.

The findings suggest that current model architectures might have substantial redundancy that could be eliminated through careful restructuring. This opens new paths for developing more efficient AI systems.",,,
24,https://www.aimodels.fyi/,,https://www.aimodels.fyi/papers/arxiv/limo-less-is-more-reasoning,LIMO: Less is More for Reasoning,"Overview
• LIMO is a novel AI reasoning approach that achieves strong performance with minimal training data

• Demonstrates better reasoning capabilities compared to larger models while using fewer resources

• Builds upon LIMA (Less Is More for Alignment) but focuses specifically on enhancing reasoning skills

• Challenges conventional wisdom that more training data always leads to better AI performance

Plain English Explanation
LIMO shows that AI models can learn to reason well without massive amounts of training data. Think of it like teaching a student - sometimes a few clear, well-chosen examples work better than overwhelming them with information.

The researchers found that carefully selecting high-quality training examples produces better results than using enormous datasets. This approach is similar to how humans often learn better from a few detailed explanations rather than skimming through volumes of material.

What makes LIMO special is its focus on teaching the AI to think step-by-step, similar to how humans solve complex problems. Rather than force-feeding the model with endless examples, it learns from a smaller set of carefully chosen reasoning patterns.

Key Findings
Logic and reasoning capabilities can be developed with significantly less training data than previously thought. The model showed:

• Superior performance on reasoning tasks compared to larger models

• More consistent and reliable outputs when solving complex problems

• Better ability to explain its thinking process

• Reduced computational requirements and training time

Technical Explanation
The LIMO architecture builds on previous work in mathematical reasoning but introduces several key innovations. The model uses a specialized training approach that emphasizes quality over quantity in the training data.

The system employs a selective data curation process that identifies and prioritizes examples that demonstrate clear reasoning patterns. This approach differs from traditional methods that rely on massive datasets.

Rational meta-reasoning is integrated into the model's architecture, allowing it to evaluate and improve its own problem-solving strategies.

Critical Analysis
While LIMO shows promising results, several limitations deserve attention:

• The approach may not scale well to certain types of problems that require broader knowledge

• The criteria for selecting training examples could introduce unintended biases

• More research is needed to validate performance across diverse reasoning tasks

The evaluation methodology could be expanded to include a wider range of reasoning challenges and real-world applications.

Conclusion
LIMO represents a significant shift in how we think about training AI systems to reason. The success of this ""less is more"" approach challenges the dominant paradigm of ever-larger training datasets.

This research opens new possibilities for developing more efficient and effective AI systems, particularly in resource-constrained environments. The findings suggest that future AI development might benefit from focusing more on the quality and structure of training data rather than just increasing its quantity.",,,
25,https://www.aimodels.fyi/,,https://www.aimodels.fyi/papers/arxiv/coat-chain-associated-thoughts-framework-enhancing-large,CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning,"Overview
Introduces Chain of Associated Thoughts (CoAT) framework to enhance LLM reasoning
Leverages associative memory mechanisms similar to human cognition
Demonstrates improved performance on complex reasoning tasks
Integrates episodic and semantic memory concepts
Shows 8-12% improvement over baseline models
Plain English Explanation
The Chain of Associated Thoughts framework works like human memory and thinking patterns. When humans solve problems, we often connect different memories and ideas together. CoAT helps AI models do the same thing.

Think of it like solving a mystery. Rather than just looking at clues in isolation, CoAT helps the AI connect related information, just like a detective connecting evidence from different sources. The system stores important information and recalls it when needed, similar to how we remember relevant past experiences.

The framework creates two types of memory: quick temporary memories for the current task (like your working memory) and longer-lasting knowledge (like facts you've learned over time). This combination helps AI models think more thoroughly and solve problems more effectively.

Key Findings
The research demonstrates several important results:

10% average improvement in reasoning accuracy
Better performance on complex multi-step problems
More consistent and logical explanations
Reduced repetition and circular reasoning
Enhanced ability to handle contradictory information
Technical Explanation
The CoAT architecture consists of three main components: an associative memory module, a reasoning engine, and a memory consolidation system. The memory module stores both episodic (experience-based) and semantic (fact-based) information.

The system uses a novel attention mechanism to retrieve relevant memories and incorporate them into the reasoning process. This approach differs from traditional chain-of-thought prompting by actively maintaining and updating a memory state throughout the reasoning process.

Performance testing used standard benchmarks including GSM8K, MATH, and BIG-Bench Hard. The framework showed particular strength in problems requiring multi-step reasoning or integration of multiple concepts.

Critical Analysis
While CoAT shows promise, several limitations exist:

Memory capacity constraints may limit scalability
Performance depends heavily on initial knowledge base quality
May struggle with novel scenarios outside training distribution
Computational overhead from memory operations
The framework's effectiveness could be further validated through more diverse testing scenarios and real-world applications.

Conclusion
CoAT represents a significant step toward more human-like reasoning in AI systems. The integration of associative memory mechanisms offers a promising direction for improving LLM capabilities. Future work could focus on scaling the approach and reducing computational requirements while maintaining performance benefits.

The results suggest that mimicking human cognitive processes may be a valuable approach for advancing AI reasoning capabilities. This could lead to more reliable and explainable AI systems for complex decision-making tasks.",,,
26,https://www.aimodels.fyi/,,https://www.google.com/search?q=Syntriever%3A+How+to+Train+Your+Retriever+with+Synthetic+Data+from+LLMs&oq=Syntriever%3A+How+to+Train+Your+Retriever+with+Synthetic+Data+from+LLMs&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg6MgYIAhBFGD0yBggDEEUYPdIBBzUyOGowajSoAgCwAgE&sourceid=chrome&ie=UTF-8,Syntriever: How to Train Your Retriever with Synthetic Data from LLMs,"Overview
Introduces Syntriever, a novel approach for training text retrievers using synthetic data from large language models
Demonstrates improved retrieval performance across multiple domains without task-specific training
Achieves state-of-the-art results on various retrieval benchmarks
Uses LLMs to generate high-quality training data through passage synthesis
Implements a two-stage training process for knowledge distillation and contrastive learning
Plain English Explanation
Syntriever works like a smart librarian who learns to find relevant information by studying artificially created examples. Instead of learning from real-world data, it learns from carefully crafted synthetic passages generated by large language models.

Think of it like teaching someone to find books in a library by first creating practice scenarios. The system generates pairs of related texts, like a question and its answer, then learns to recognize what makes them connected. This approach is similar to how humans might learn to spot relationships between different pieces of information.

The training happens in two main steps. First, the system learns from synthetic examples created by advanced AI models. Then, it practices matching related pieces of information using a technique called contrastive learning - like learning to spot differences and similarities between various texts.

Key Findings
Text retrieval systems trained with synthetic data outperform traditional methods across multiple benchmarks. The research shows:

2-8% improvement over previous state-of-the-art systems
Consistent performance across different types of retrieval tasks
Effective generalization to new domains without additional training
Reduced dependency on task-specific training data
Superior performance in zero-shot scenarios
Technical Explanation
The training process involves two distinct stages. Stage one focuses on knowledge distillation through passage synthesis, where an LLM generates diverse, high-quality training examples. Stage two implements contrastive learning using these synthetic passages.

The architecture employs a bi-encoder framework with shared weights between query and passage encoders. This design enables efficient similarity computation through dense vector representations. The system uses a combination of hard negative mining and carefully curated positive examples to enhance training effectiveness.

Implementation details reveal careful consideration of computational efficiency, with optimizations in both the training pipeline and inference process. The model architecture balances performance with practical deployment considerations.

Critical Analysis
While the results are impressive, several limitations deserve attention:

Dependency on LLM quality for synthetic data generation
Computational costs associated with large-scale training
Potential biases inherited from the underlying LLMs
Limited evaluation on non-English languages
Need for further investigation of scalability limits
The research methodology could benefit from more extensive ablation studies and deeper analysis of failure cases. Future work might explore multilingual applications and more efficient training techniques.

Conclusion
Syntriever represents a significant advancement in training retrievers without task-specific data. The success of synthetic training data suggests a promising direction for developing more robust and versatile retrieval systems. This approach could significantly impact information retrieval applications across various domains, from search engines to question-answering systems.

The findings point toward a future where retrieval systems can be trained more efficiently and effectively, potentially reducing the need for large amounts of human-annotated training data. This development has broad implications for improving information access and search technology.",,,
27,https://www.chatpaper.ai/,,https://www.chatpaper.ai/dashboard/paper/6e627325-cad1-46a2-97cf-c7d04184d29d,LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention,"Paper Overview
This paper introduces a method to transfer knowledge from a large pre-trained model to a smaller one using Enhanced Cross-Attention. The methodology involves utilizing a large pre-trained model for input query representations and a smaller model with Enhanced Cross-Attention layers for response generation. Experimental validation shows improved generation quality and reduced computational costs.

Core Contribution
The key innovation lies in the Enhanced Cross-Attention mechanism that facilitates knowledge transfer from a large pre-trained model to a smaller one efficiently.

Research Context
This study addresses the challenge of transferring knowledge from large models to smaller ones effectively, aiming to enhance response generation quality while reducing computational overhead.

Keywords
Transfer Learning, Enhanced Cross-Attention, Pre-trained Models, Response Generation, Knowledge Transfer

Background
The research focuses on the transfer of knowledge from a large pre-trained model to a smaller one to improve response generation quality and efficiency. The rationale stems from the need to leverage the capabilities of large models in smaller, more resource-efficient settings.

Research Gap
Existing literature lacks efficient methods for transferring knowledge from large pre-trained models to smaller ones for response generation tasks.

Technical Challenges
Challenges include designing a mechanism for effective knowledge transfer, ensuring compatibility between models of different sizes, and maintaining response coherence during generation.

Prior Approaches
Previous solutions have primarily focused on fine-tuning or distillation methods, which may not fully address the efficiency and quality requirements of knowledge transfer in this context.

Methodology
The methodology involves utilizing a large pre-trained model for input query representations and a smaller model with Enhanced Cross-Attention layers for response generation.

Theoretical Foundation
Theoretical basis includes the concept of knowledge transfer, Enhanced Cross-Attention mechanism, and model integration for response generation tasks.

Technical Architecture
The architecture comprises a modified large pre-trained model, a smaller response generation model, and a combined model integrating both components for efficient knowledge transfer.

Implementation Details
Implementation involves Linear projections for dimension conversion, Adapter Block for non-linear transformation, and Gating Mechanism for blending original and external knowledge.

Innovation Points
The innovation lies in the Enhanced Cross-Attention mechanism, which enhances knowledge transfer efficiency and response generation quality.

Experimental Validation
Experimental validation includes comparative analysis with models like DeepSeek, Qwen2, and GPT-Neo, showcasing improved generation quality and reduced computational costs.

Setup
The experimental setup includes the utilization of the Bespoke-Stratos-17k dataset, dynamic data preparation techniques like padding, filtering, and shuffling.

Metrics
Evaluation metrics involve reduction in training and validation loss, coherence of generated responses, and task-specific quality assessments like arithmetic problems.

Results
Results demonstrate a reduction in loss metrics, enhanced coherence in responses, and improved generation quality compared to existing models.

Comparative Analysis
Comparative analysis with DeepSeek, Qwen2, and GPT-Neo models highlights the superiority of the proposed approach in terms of efficiency and quality.

Impact and Implications
The proposed method shows significant improvements in response generation quality, reduced computational costs, and adaptability to specific tasks, paving the way for practical applications.

Key Findings
The key findings include enhanced response generation quality, reduced computational overhead, and adaptability to task-specific requirements.

Limitations
Limitations may include constraints in scaling the approach to extremely large models and potential challenges in integrating diverse model types.

Future Directions
Future research opportunities include exploring different model integrations, optimizing Cross-Attention layer configurations, and applying the method to specialized business tasks.

Practical Significance
The method's practical significance lies in its potential to improve response generation in various applications, reduce computational expenses, and adapt models to specific task requirements.",,,
28,https://discuss.pytorch.kr/tag/tool-for-llm,,https://discuss.pytorch.kr/t/yek-rust-llm/5926,yek: Rust 기반 LLM 데이터 처리 도구,"yek 소개
yek은 빠르고 효율적인 방식으로 프로젝트의 텍스트 기반 파일을 처리하여 대규모 언어 모델(LLM)이 쉽게 소비할 수 있도록 데이터를 구조화하고 직렬화하는 도구입니다. Rust로 작성되어 빠른 속도와 높은 안정성을 제공하며, 특히 파일 우선순위를 지정하고 .gitignore를 활용하여 불필요한 파일을 건너뛸 수 있는 기능이 돋보입니다.

yek은 주로 다음과 같은 작업을 자동화할 수 있는 CLI 도구입니다:

프로젝트 내 텍스트 파일을 처리하고, 원하는 크기나 토큰 개수로 청크(Chunk)로 나눕니다.

.gitignore 규칙 및 Git 히스토리를 활용하여 중요한 파일만 선택적으로 처리합니다.

결과 데이터를 임시 디렉토리에 저장하거나 스트리밍 방식으로 출력합니다.

Rust 기반으로 작성된 만큼 성능이 뛰어나며, 특히 대규모 파일이 있는 프로젝트에서도 빠른 처리 속도를 보장합니다. 기본적으로 프로젝트의 모든 파일을 10MB 단위로 나누어 LLM이 쉽게 처리할 수 있는 형태로 제공합니다. LLM 학습 및 추론 워크플로우에서 yek은 데이터 준비 시간을 획기적으로 단축할 수 있습니다.

또한, yek은 비슷한 도구인 Repomix와 비교해 최대 230배 빠른 성능을 자랑합니다. Rust의 병렬 처리와 효율적인 I/O 작업을 통해 속도 면에서 큰 장점을 보유하고 있습니다. 특히 다음과 같은 기능에서 차별화됩니다:

.gitignore 및 Git 히스토리를 활용한 우선순위 처리.

기본적으로 바이너리 파일 및 대용량 파일을 무시.

사용자 지정 가능성이 높은 설정 파일(yek.toml) 제공.

yek의 주요 기능
파일 우선순위 지정: Git 기록 및 사용자 정의 규칙을 기반으로 더 중요한 파일을 마지막에 출력하여 LLM의 집중도를 높임.

청크 크기 조절: 기본 10MB 크기, 최대 토큰 수 등을 지정 가능.

스트리밍 출력 지원: 파이프를 통해 다른 명령어로 결과를 전달 가능.

다중 디렉토리 처리: 여러 디렉토리를 한 번에 처리 가능.

사용자 정의 설정: yek.toml 파일을 통해 추가적인 무시 패턴 및 우선순위 규칙 설정 가능.

라이선스
yek 프로젝트는 MIT 라이선스로 공개된 오픈소스 프로젝트입니다.

:github: yek GitHub 저장소
https://github.com/bodo-run/yek



",,,
29,https://discuss.pytorch.kr/tag/tool-for-llm,,https://discuss.pytorch.kr/t/toolgen/5327,ToolGen: 도구 검색 및 호출을 위한 대규모 언어 모델 통합 시스템,"ToolGen 소개
ToolGen은 도구 지식을 대형 언어 모델(LLM)에 직접 통합하도록 설계된 시스템입니다. 이 시스템은 도구를 고유한 토큰으로 표현함으로써, 언어 생성 작업 중에 도구 호출을 원활하게 수행할 수 있게 합니다. 이러한 통합 접근 방식은 API 호출이나 데이터 검색과 같은 외부 도구 접근이 필요한 복잡한 작업에서 언어 모델이 동적으로 도구를 검색하고 호출할 수 있도록 지원합니다.

ToolGen의 주요 목표는 LLM이 도구를 쉽게 통합하고 필요할 때마다 특정 도구를 호출할 수 있도록 하는 것입니다. 이 기능은 다양한 API 호출이나 데이터 검색이 요구되는 복잡한 작업에서 특히 유용합니다.

주요 기능
ToolGen: 도구 검색 및 호출을 위한 대규모 언어 모델 통합 시스템

통합된 도구 토큰화: 도구들을 고유한 토큰으로 통합하여 자연어 생성과 툴 호출을 함께 처리할 수 있습니다.
API 자동화: 도구 사용을 위한 API 키를 불러와 툴 호출 과정을 자동화합니다.
단순화된 도구 사용: 도구 호출에 필요한 복잡한 설정 없이 바로 사용할 수 있는 간편한 시스템.

:scroll: ToolGen 논문
https://arxiv.org/pdf/2410.03439

:hugs: ToolGen 모델
huggingface.co

ToolGen - a reasonwang Collection
We’re on a journey to advance and democratize artificial intelligence through open source and open science.

:hugs: ToolGen 데이터셋 다운로드
huggingface.co

reasonwang/ToolGen-Datasets · Datasets at Hugging Face
We’re on a journey to advance and democratize artificial intelligence through open source and open science.

:github: ToolGen GitHub 저장소
https://github.com/Reason-Wang/ToolGen",,,
30,https://discuss.pytorch.kr/tag/tool-for-llm,,https://discuss.pytorch.kr/t/crawl4ai-llm-ai-crawler/5282,Crawl4AI: LLM 및 AI 애플리케이션을 위한 오픈소스 웹 크롤러(Crawler),"Crawl4AI 소개
Crawl4AI는 대규모 언어 모델(LLM)을 위한 웹 크롤링과 데이터 추출을 비동기적으로 처리하는 라이브러리입니다. 비동기 기능을 통해 여러 URL을 동시에 처리할 수 있고, 웹 페이지의 메타데이터, 미디어 태그, 링크 등을 손쉽게 추출할 수 있습니다. 이 도구는 Playwright와 같은 크롤러를 활용하여 성능을 극대화하고, 크롤링 대상 페이지에서 Javascript 실행 및 CSS 셀렉터를 통한 데이터 추출도 지원합니다.

기존의 동기 크롤링 방식은 웹 페이지 로드 시간과 실행 시간에 큰 영향을 받는 반면, Crawl4AI는 비동기 구조를 통해 빠르고 대규모의 데이터를 효율적으로 수집할 수 있습니다. 이를 통해 AI 응용 프로그램에 필요한 대량의 데이터를 신속하게 확보할 수 있습니다.

Crawl4AI의 주요 기능
무료 및 오픈소스: 누구나 자유롭게 사용할 수 있는 무료 오픈소스 프로젝트입니다.
빠른 성능: 상용 서비스보다 빠른 성능을 자랑합니다.
다양한 출력 형식 지원: JSON, HTML, Markdown 등의 LLM 친화적인 형식으로 데이터를 출력합니다.
다중 URL 크롤링: 여러 URL을 동시에 처리하여 효율적인 데이터 수집이 가능합니다.
미디어 태그 추출: 이미지, 오디오, 비디오 태그를 모두 추출합니다.
세션 관리 및 프록시 지원: 복잡한 다중 페이지 시나리오에 유용하며 프록시를 통해 개인 정보 보호 및 접근성을 강화합니다.
고급 데이터 추출: CSS 셀렉터 및 JavaScript 실행을 통해 정확한 데이터 추출이 가능하며, 세부 추출 전략도 지원합니다.
설치 방법
Crawl4AI는 Python 패키지로 설치하거나 Docker로 사용할 수 있습니다.

pip를 이용한 설치
pip install crawl4ai
기본적으로 Playwright를 사용하여 비동기 크롤링이 가능하며, 필요한 경우 Playwright를 수동으로 설치할 수 있습니다.

playwright install
Playwright에 문제가 발생한 경우, 다음의 명령어를 실행하면 해결이 되는 경우도 있습니다:

python -m playwright install chromium
동기 버전 설치
pip install crawl4ai 명령어는 자동으로 비동기(Asynchronous)로 동작하는 크롤러를 설치합니다. 동기식(Synchronous)으로 동작하는 크롤링을 원할 경우 Selenium을 사용한 동기 버전을 설치할 수 있습니다.

pip install crawl4ai[sync]
사용 방법
간단한 사용 예시는 다음과 같습니다:

import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url=""https://www.nbcnews.com/business"")
        print(result.markdown)

if __name__ == ""__main__"":
    asyncio.run(main())
이 예시는 NBC 뉴스의 비즈니스 페이지를 크롤링하여 내용을 Markdown 형식으로 출력합니다.

Crawl4AI 라이선스
Crawl4AI 프로젝트는 Apache 2.0 License로 공개 및 배포되고 있습니다.

:github: Crawl4AI GitHub 저장소
https://github.com/unclecode/crawl4ai?tab=readme-ov-file

:google_cloud: Crawl4AI를 Google Colab에서 바로 사용해보기

colab.research.google.com

Google Colab
:books: Crawl4AI 공식 문서

crawl4ai.com
Home - Crawl4AI Documentation (v0.4.3bx)
🚀🤖 Crawl4AI, Open-source LLM-Friendly Web Crawler & Scraper",,,
31,https://news.hada.io/,,https://news.hada.io/topic?id=19051,"2025년을 위한 도구들 : Oils, Radicle, Simplex Chat","저자는 Oils, Radicle, Simplex Chat 3가지가 더욱 널리 사용되면 좋겠다고 생각
Oils: 기존 POSIX 쉘(Bash 등)을 대체
Radicle: 분산형 Git 호스팅 솔루션으로 Github/GitLab을 대체 가능
Simplex Chat: 이메일을 대체하거나 혁신 가능
보통 새로운 도구를 발견하면, 그 도구의 기본개념 이해하는데 10분, 시작하는데 5일까지 걸리게 됨
대부분은 기본 개념을 이해하고 나면 흥미가 끝나버리기에, 정말 근본적인 혁신과 일관된 철학이 있어야 계속 관심을 가질 수 있음
예를 들어 Pijul 같은 프로젝트가 흥미로웠지만 Git에 비교해 적용 이점이 크게 체감되지 않아 일상적으로 밀고 나가지는 못했음
아래 소개할 세 가지 툴은 현재 대안으로서의 가치와 실제로 쓰일 만한 완성도를 갖췄다고 봄
Oils for unix
Bash 쉘을 새롭게 구현한 프로젝트로, POSIX 호환성을 갖추면서도 오래된 쉘 환경의 문제점을 해결해 나가는 중
KornShell 등 기존 대안이 있었지만, 유의미한 개선 없이 오랜 기간 정체 상태였음
Nushell 처럼 비슷한 구문으로 새로운 언어를 만드는 것도 아님
Oils는 Bash 구현을 바탕으로, 정말로 깨진 부분만 점진적으로 고치고, 스크립트도 단계적으로 마이그레이션 가능하도록 함
기존 POSIX 셸의 문제점(예: 암시적 글로빙, 잘못된 산술 연산 등)을 해결
현재 Bash 호환 부분은 안정된 베타 단계에 있으며, 올해 1.0 버전 출시가 목표
Oils 시도해보기 : Alpine Linux Edge 버전에서 oils-for-unix 패키지를 이용해 간단히 도커 환경에서 테스트 가능
Radicle
Git 저장소를 탈중앙 방식으로 호스팅/공유하는 솔루션
Gitlab이나 Github보다 우수하며, 소스 코드의 가용성을 높이기 위해 토렌트와 유사한 방식을 사용
로컬 호스팅임에도 불구하고 기존의 서버-클라이언트 상호작용과 유사한 직관적인 UX를 제공
새로운 개념을 쉽게 배울 수 있도록 문서화에 많은 노력을 기울여서, 완전히 새로운 워크플로라도 쉽게 적응 가능
ActivityPub 기반 ForgeFed와 달리, Radicle은 Git 데이터에 최적화되어 더 높은 가용성을 기대할 수 있음
Radicle 시도해보기 : 내 서버에 래디클 노드를 만들고 이 블로그에 소스를 게시해둠. 공개된 사용자 가이드를 따르거나, 그냥 내 블로그를 피어링으로 클론하는 것도 가능
Simplex Chat
겉보기에는 또 하나의 채팅 앱이지만, 주소(Address) 개념이 근본적으로 다름
생성한 프로필에 여러 주소를 추가할 수 있고, 이 주소는 특정 대화 연결에만 사용되거나 스팸 발생 시 즉시 폐기 가능함
""주소는 1회용이거나, 삭제하기 전까지 재사용 가능""
""2개의 프로필 간의 모든 연결은 고유한 1대1 주소를 사용""
결과적으로 “하나의 프로필에 여러 주소를 유동적으로 연결/분리”할 수 있어, 스팸에 대응하거나 목적별로 주소를 사용하기 좋음
재사용 가능한 주소를 삭제하면 새로운 연결만 차단되며, 기존 연락처를 삭제하면 해당 연락처의 접근이 차단됨
즉 ""주소를 추상화하고, 주소와 프로필을 디커플링 하는 것. 마치 사서함 처럼""
이메일 시스템이 이런 방식을 채택한다면, 주소를 새로 발급하고 필요 시 버리는 과정을 간편화할 수 있음
E2E 암호화, 완전한 프라이버시도 지원해, 메일 대신 이런 프로토콜이 확산되길 바라는 마음이 큼
아직 인지도가 낮지만, “또 다른 메신저”라고 지나치기엔 근본적 아이디어가 참신함
Simplex 시도해보기 : 피드백용 그룹을 생성해 둠. 익명 모드로 그룹에 참여 가능하며, 간단한 프로필 생성 후 바로 대화를 체험할 수 있음
뭘 해야할까?
우리가 사용하는 도구가 곧 미래의 표준이 됨
Oils, Radicle, Simplex Chat 모두 기존의 관행적 도구를 넘어서는 가능성을 지닌 프로젝트임
이런 새로운 프로젝트를 직접 시도해보고, 더 나은 실무 표준을 만들어나가기를 제안함

▲
dbs0829 5일전  [-]
oils는 너무 일반적인 단어라서 괜히 정감이 안가네요. 저만 그럴지 모르겠는데, 딱 검색했을 때 해당 서비스가 안나오는 경우 은근 거부감이 들어요.

답변달기
▲
xguru 5일전  [-]
Oil - 새로운 Unix 쉘
제가 2021년에 소개했었고, 예전 이름이 Oil 이었는데 2025년부터 Oils 로 이름이 변경되었습니다.
Radicle - P2P 방식의 GitHub 대체제
SimpleX - 사용자 ID가 없는 최초의 메신저",,,
32,https://news.hada.io/,,https://news.hada.io/topic?id=19179,Show GN: Moonlight v1.0.0 - 논문읽기를 위한 AI PDF 리더,"논문읽기를 위한 AI PDF 리더
연구자라면 누구나 겪어본 어려움이 있습니다.

수식의 맥락 및 변수 파악에 시간이 걸리고,
이미지나 테이블의 의미를 빠르게 파악하기 어려우며,
하이퍼링크를 클릭하고 다시 돌아오는 번거로운 과정에 지치고,
인용 논문을 하나하나 찾아보는 과정이 불편하고 시간이 걸립니다.
작년에 시작한 Moonlight AI PDF 논문 리더의 v1.0.0을 소개합니다.
연구자들이 논문을 더 쉽게 탐색하고 이해할 수 있도록 도와주는 AI PDF 리더로, 논문을 효과적으로 다룰 수 있도록 지원합니다. GPT와 같은 AI 기술을 학술 논문을 읽는 흐름에 직접 적용하여 업무의 효율을 높이고자 합니다.

주요 기능

텍스트 및 이미지 설명 - 선택한 텍스트 / 이미지들을 가독성 좋게 구조화된 설명을 제공합니다.
수식 설명 - 논문에 있는 수식 설명, 변수 정의 및 추가적인 질의를 통해 수식을 효율적으로 이해할 수 있습니다.
수식 복사 - pdf 로부터 LaTeX 형식으로 바로 복사해서 notion, overleaf, github, tex 등의 정리 매체로 쉽게 옮길 수 있습니다.
인용 카드 - 참고 문헌을 논문 안에서 클릭 한번으로 확인할 수 있습니다.
번역 - 모르는 부분, 혹은 전체 페이지를 번역해서 볼 수 있습니다.
자동 하이라이트 - 논문의 주요 내용들을 자동으로 하이라이트 해주어 읽을 때 가이드 역할을 해줍니다.
지금 바로 사용해보세요!

크롬 익스텐션 설치하기
데모 비디오",,,
33,https://betaai.substack.com/,,https://betaai.substack.com/p/2-ai-0f1,Multimodal Self-Instruct: 추상 이미지 이해력 강화,"Multimodal Self-Instruct: 추상 이미지 이해력 강화

LMM의 한계: 기존 멀티모달 모델은 지도, 차트, 다이어그램 등의 추상 이미지 해석과 시각적 추론에 취약.

합성 데이터 활용: LLM과 코드 기능을 이용해 8개 시각적 시나리오에서 11,193개 지시문을 포함한 벤치마크 구축.

성능 개선 입증: 62,476개 합성 데이터로 LMM을 미세 조정한 결과, 차트 해석과 지도 내비게이션 성능 향상 확인.",,,