{
  "metadata": {
    "timestamp": "2025-02-18T11:05:46.313869",
    "filename": "/Users/oldman/Library/CloudStorage/OneDrive-개인/001_Documents/001_TelePIX/000_workspace/02_PseudoLab/01_summarization/핸드북 인프라 데이터 정리.xlsx",
    "total_rows": 15,
    "model": "gpt-4o-mini"
  },
  "results": [
    {
      "No.": 1,
      "엔드포인트": "AlphaSignal - Tranding Signal",
      "날짜": "2025.02.06",
      "링크": "https://www.youtube.com/watch?v=7xTGNNLPyMI",
      "제목": "Deep Dive into LLMs like ChatGPT",
      "본문": "This is a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full training stack of how the models are developed, along with mental models of how to think about their \"psychology\", and how to get the best use them in practical applications. I have one \"Intro to LLMs\" video already from ~year ago, but that is just a re-recording of a random talk, so I wanted to loop around and do a lot more comprehensive version.\n\nInstructor\nAndrej was a founding member at OpenAI (2015) and then Sr. Director of AI at Tesla (2017-2022), and is now a founder at Eureka Labs, which is building an AI-native school. His goal in this video is to raise knowledge and understanding of the state of the art in AI, and empower people to effectively use the latest and greatest in their work.\nFind more at https://karpathy.ai/ and https://x.com/karpathy",
      "요약문": "### Deep Dive into LLMs like ChatGPT\n\n**Summary:** This content provides an in-depth exploration of Large Language Models (LLMs) such as ChatGPT, aimed at a general audience. It details the comprehensive training process behind these AI technologies and discusses their underlying \"psychology\" to enhance practical usage. The instructor, Andrej Karpathy, a founding member of OpenAI and former Senior Director of AI at Tesla, seeks to improve understanding of advanced AI and empower users in their applications. This video serves as an updated and more thorough version of a previous introductory talk on LLMs. For additional resources, visit https://karpathy.ai/ and https://x.com/karpathy.",
      "분류": "### Deep Dive into LLMs like ChatGPT\n\n**Category:** 업데이트&트렌드",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Deep Dive into LLMs like ChatGPT\n\n**Keywords:** Large Language Models, AI Technology, Model Training, Practical Applications, AI Education"
    },
    {
      "No.": 2,
      "엔드포인트": "AlphaSignal - Tranding Signal",
      "날짜": "2025.02.05",
      "링크": "https://www.anthropic.com/research/constitutional-classifiers",
      "제목": "Constitutional Classifiers: Defending against universal jailbreaks",
      "본문": "A new paper from the Anthropic Safeguards Research Team describes a method that defends AI models against universal jailbreaks. A prototype version of the method was robust to thousands of hours of human red teaming for universal jailbreaks, albeit with high overrefusal rates and compute overhead. An updated version achieved similar robustness on synthetic evaluations, and did so with a 0.38% increase in refusal rates and moderate additional compute costs.\n\nWe are currently hosting a temporary live demo version of a Constitutional Classifiers system, and we encourage readers who have experience jailbreaking AI systems to help “red team” it. Find out more below and at the demo website.\n\n*Update 5 February 2025: We are now offering $10,000 to the first person to pass all eight levels of our jailbreaking demo, and $20,000 to the first person to do so with a universal jailbreaking strategy. Full details of the reward and the associated conditions can be found at HackerOne.\n\nLarge language models have extensive safety training to prevent harmful outputs. For example, we train Claude to refuse to respond to user queries involving the production of biological or chemical weapons.\n\nNevertheless, models are still vulnerable to jailbreaks: inputs designed to bypass their safety guardrails and force them to produce harmful responses. Some jailbreaks flood the model with very long prompts; others modify the style of the input, such as uSiNg uNuSuAl cApItALiZaTiOn. Historically, jailbreaks have proved difficult to detect and block: these kinds of attacks were described over 10 years ago, yet to our knowledge there are still no fully robust deep-learning models in production.\n\nWe’re developing better jailbreak defenses so that we can safely deploy increasingly capable models in the future. Under our Responsible Scaling Policy, we may deploy such models as long as we’re able to mitigate risks to acceptable levels through appropriate safeguards—but jailbreaking lets users bypass these safeguards. In particular, we’re hopeful that a system defended by Constitutional Classifiers could allow us to mitigate jailbreaking risks for models which have passed the CBRN capability threshold outlined in our Responsible Scaling Policy1.\n\nIn our new paper, we describe a system based on Constitutional Classifiers that guards models against jailbreaks. These Constitutional Classifiers are input and output classifiers trained on synthetically generated data that filter the overwhelming majority of jailbreaks with minimal over-refusals and without incurring a large compute overhead.\n\nResults from human red teaming\nWe ran two main categories of tests to assess the effectiveness of Constitutional Classifiers.\n\nFirst, we developed a prototype version of the system to identify and block specific scientific knowledge related to chemical, biological, radiological, and nuclear harms. We then invited independent jailbreakers to a bug-bounty program in which they were challenged to “red team” the system (i.e., to attempt to break it under experimental conditions to test its robustness).\n\nSpecifically, they were given a list of ten “forbidden” queries, and their task was to use whichever jailbreaking techniques they wanted in order to get one of our current models (in this case, Claude 3.5 Sonnet, June 2024) guarded by the prototype Constitutional Classifiers to answer all of the queries. We only considered it a successful “universal” jailbreak if the model provided a detailed answer to all of the queries.\n\n183 active2 participants spent an estimated >3,000 hours over a two-month experimental period attempting to jailbreak the model. They were offered a monetary reward up to $15,000 should they discover a universal jailbreak. Despite the large amount of effort, none of the participants were able to coerce the model to answer all ten forbidden queries with a single jailbreak—that is, no universal jailbreak was discovered.\n\nDespite its robustness to jailbreaks, this prototype system had some problems: it refused too many harmless queries and cost a lot of computational resources to run. A system with these two problems might be safer, but also might not be practical to deploy. Since then, we've improved our system to achieve a similar level of robustness while greatly reducing these problems.\n\nResults from automated evaluations\nWe ran a set of automated evaluations where we synthetically generated 10,000 jailbreaking prompts including many of the most-effective attacks on current LLMs, as well as attacks designed to circumvent classifier safeguards. We tried these prompts on a version of Claude 3.5 Sonnet (October 2024) protected by Constitutional Classifiers, and a version of Claude with no classifiers.\n\nUnder baseline conditions, with no defensive classifiers, the jailbreak success rate was 86%—that is, Claude itself blocked only 14% of these advanced jailbreak attempts. Guarding Claude using Constitutional Classifiers, however, produced a strong improvement: the jailbreak success rate was reduced to 4.4%, meaning that over 95% of jailbreak attempts were refused.\n\nA graph showing the results for vulnerability to jailbreaks, overrefusals, and compute overhead for the Constitutional Classifiers system versus the base model\nResults from automated evaluations. For all plots, lower is better. (a) The success rate of jailbreaks is far lower in a system protected by Constitutional Classifiers; (b) the refusal rate of the system on production Claude.ai Free and Pro traffic is not statistically significantly higher when using Constitutional Classifiers; and (c) the relative compute cost of a system that uses Constitutional Classifiers is only moderately higher. Error bars represent 95% confidence intervals computed using binomial proportion standard errors under asymptotic normality assumptions.\nAt the same time, our Constitutional Classifiers did not substantially increase the over-refusal rate on harmless user queries: with the classifiers, Claude had a slightly increased refusal rate (by 0.38%), but this was not statistically significant in a random sample of 5,000 conversations. Finally, the compute cost was moderately higher (by 23.7%) than that of the unguarded model. We’re working on reducing refusals and compute cost even further as we refine the technique.\n\nOverall, our automated analyses found that this updated version of the Constitutional Classifiers system dramatically improved the robustness of the AI model against jailbreaking—and did so with only minimal additional cost.\n\nHow it works\nConstitutional Classifiers is based on a similar process to Constitutional AI, another technique we have used to align Claude. Both techniques use a constitution: a list of principles to which the model should adhere. In the case of Constitutional Classifiers, the principles define the classes of content that are allowed and disallowed (for example, recipes for mustard are allowed, but recipes for mustard gas are not).\n\nWith the help of Claude, we use this constitution to generate a large number of synthetic prompts and synthetic model completions across all the content classes. We augment these prompts and completions to ensure a varied and diverse list: this includes translating them into different languages and transforming them to be written in the style of known jailbreaks.\n\nA schematic diagram of the how the Constitutional Classifiers system works, from the creation of the constitution through to generating a test set to using the system to guard an LLM\nTraining and implementing Constitutional Classifiers. (a) A constitution is produced specifying harmless and harmful categories; (b) the constitution is used as the basis for the production of many synthetic prompts and completions, which are further augmented (with variations on style and language) and turned into a training set; (c) classifiers trained on this training set are used as model safeguards to detect and block harmful content.\nWe then use these synthetic data to train our input and output classifiers to flag (and block) potentially harmful content according to the given constitution. To help minimize over-refusals (i.e., harmless content incorrectly flagged as harmful), we also train the classifiers on a fixed set of benign queries generated by a contractor.\n\nLimitations\nConstitutional Classifiers may not prevent every universal jailbreak, though we believe that even the small proportion of jailbreaks that make it past our classifiers require far more effort to discover when the safeguards are in use. It’s also possible that new jailbreaking techniques might be developed in the future that are effective against the system; we therefore recommend using complementary defenses. Nevertheless, the constitution used to train the classifiers can rapidly be adapted to cover novel attacks as they’re discovered.\n\nThe full paper contains all the details about the Constitutional Classifiers method, and about the classifiers themselves.\n\nConstitutional Classifiers live demo\nWant to try red teaming Claude yourself? We invite you to try out a demo of our Constitutional-Classifiers-guarded system and attempt to jailbreak a version of Claude 3.5 Sonnet that is guarded using our new technique.\n\nAlthough the Constitutional Classifiers technique is flexible and can be adapted to any topic, we chose to focus on queries related to chemical weapons for the demo.\n\nChallenging users to attempt to jailbreak our product serves an important safety purpose: we want to stress-test our system under real-world conditions, beyond the testing we did for our paper. This allows us to gather additional data and improve the robustness of the method prior to deploying this method on our production systems in the future.\n\nThe demo will be live from Feb 3, 2025 to Feb 10, 2025. It includes a feedback form where you can contact us to report any successful jailbreaks as well as information on our Responsible Disclosure Policy, which we ask that participants follow. We’ll announce any successes and the general results of the demo in an update to this post.\n\n*Update 5 February 2025: As noted above, we are now offering a monetary reward for successful jailbreaking of our system. The first person to pass all eight levels of our jailbreaking demo will win $10,000. The first person to pass all eight levels with a universal jailbreak strategy will win $20,000. Full details of the reward and the associated conditions can be found at HackerOne.\n\nAcknowledgements\nWe’d like to thank HackerOne for supporting our bug-bounty program for red teaming our prototype system. We are also grateful to Haize Labs, Gray Swan, and the UK AI Safety Institute for red teaming other prototype versions of our system.\n\nJoin our team\nIf you’re interested in working on problems such as jailbreak robustness or on other questions related to model safeguards, we’re currently recruiting for Research Engineers / Scientists, and we’d love to see your application.",
      "요약문": "### Constitutional Classifiers: Defending against universal jailbreaks\n\n**Summary:** A paper from the Anthropic Safeguards Research Team introduces a method called Constitutional Classifiers designed to protect AI models from universal jailbreaks, which are attempts to bypass safety measures that prevent harmful outputs. The prototype version of this system demonstrated resilience during extensive human red teaming, although it faced challenges such as high overrefusal rates and computational costs. An improved version maintained robustness while only slightly increasing refusal rates (by 0.38%) and incurring moderate additional compute costs (23.7%).\n\nThe team conducted extensive testing involving a bug-bounty program where participants attempted to create universal jailbreaks against their model, Claude 3.5 Sonnet. Over 3,000 hours of effort by 183 participants resulted in no successful universal jailbreaks. Automated evaluations also showed a significant reduction in jailbreak success rates from 86% to 4.4% when using Constitutional Classifiers, while keeping overrefusal rates low.\n\nThe Constitutional Classifiers system operates by employing a constitution that outlines allowed and disallowed content, training classifiers to filter potentially harmful inputs and outputs. Despite its effectiveness, the system may not block all jailbreaks, and new techniques could emerge that challenge its defenses. The team encourages participation in a live demo to further test the system and offers monetary rewards for successful jailbreaks during this period.",
      "분류": "### Constitutional Classifiers: Defending against universal jailbreaks\n\n**Category:** 논문",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Constitutional Classifiers: Defending against universal jailbreaks\n\n**Keywords:** Constitutional Classifiers, Jailbreak Defense, Red Teaming, AI Safety, Synthetic Evaluations"
    },
    {
      "No.": 3,
      "엔드포인트": "AlphaSignal - Tranding Signal",
      "날짜": "2024.10.31",
      "링크": "https://openai.com/index/introducing-chatgpt-search/",
      "제목": "Introducing ChatGPT search",
      "본문": "ChatGPT can now search the web in a much better way than before. You can get fast, timely answers with links to relevant web sources, which you would have previously needed to go to a search engine for. This blends the benefits of a natural language interface with the value of up-to-date sports scores, news, stock quotes, and more.\nChatGPT will choose to search the web based on what you ask, or you can manually choose to search by clicking the web search icon.\nSearch will be available at chatgpt.com, as well as on our desktop and mobile apps. All ChatGPT Plus and Team users, as well as SearchGPT waitlist users, will have access today. Enterprise and Edu users will get access in the next few weeks. We’ll roll out to all Free users over the coming months. \nDesigned to get you to a better answer\nGetting useful answers on the web can take a lot of effort. It often requires multiple searches and digging through links to find quality sources and the right information for you.\nNow, chat can get you to a better answer: Ask a question in a more natural, conversational way, and ChatGPT can choose to respond with information from the web. Go deeper with follow-up questions, and ChatGPT will consider the full context of your chat to get a better answer for you.\nWe also partnered with news and data providers to add up-to-date information and new visual designs for categories like weather, stocks, sports, news, and maps.\n“ChatGPT search promises to better highlight and attribute information from trustworthy news sources, benefiting audiences while expanding the reach of publishers like ourselves who produce premium journalism.” Pam Wasserstein, President, Vox Media\nGo straight to the source\nChats now include links to sources, such as news articles and blog posts, giving you a way to learn more. Click the Sources button below the response to open a sidebar with the references.\n“We are convinced that AI search will be, in a near future and for the next generations, a primary way to access information, and partnering with OpenAI positions Le Monde at the forefront of this shift. It allows us to test innovations at an early stage while safeguarding journalism’s core values and integrity.” Louis Dreyfus, CEO & Publisher of Le Monde\nChatGPT search connects people with original, high-quality content from the web and makes it part of their conversation. By integrating search with a chat interface, users can engage with information in a new way, while content owners gain new opportunities to reach a broader audience. We hope to help users discover publishers and websites, while bringing more choice to search.\n“As AI reshapes the media landscape, Axel Springer’s partnership with OpenAI opens up tremendous opportunities for innovative advancements. Together, we're driving new business models that ensure journalism remains both trustworthy and profitable.” Mathias Sanchez, SVP Global Strategic Partnerships Axel Springer SE\nWe collaborated extensively with the news industry and carefully listened to feedback from our global publisher partners, including Associated Press, Axel Springer, Condé Nast, Dotdash Meredith, Financial Times, GEDI, Hearst, Le Monde, News Corp, Prisa (El País), Reuters, The Atlantic, Time, and Vox Media. Any website or publisher can choose to appear in ChatGPT search. If you’d like to share feedback, please email us at publishers-feedback@openai.com\nHow it works and what comes next\nThe search model is a fine-tuned version of GPT-4o, post-trained using novel synthetic data generation techniques, including distilling outputs from OpenAI o1-preview. ChatGPT search leverages third-party search providers, as well as content provided directly by our partners, to provide the information users are looking for. Learn more here.\nThanks to feedback from the SearchGPT prototype, we brought the best of the SearchGPT experience into ChatGPT. We plan to keep improving search, particularly in areas like shopping and travel, and leverage the reasoning capabilities of the OpenAI o1 series to do deeper research. We also plan to bring our new search experience to Advanced Voice and canvas, as well as to Free and logged out users in the future.\nChatGPT Plus and Team users can try it out today at chatgpt.com. They can also download the Chrome extension to search directly via the browser URL bar.",
      "요약문": "### Introducing ChatGPT search\n\n**Summary:** ChatGPT has enhanced its web search capabilities, allowing users to receive quick, relevant answers with links to sources, directly through the chat interface. This feature will be accessible on chatgpt.com and the mobile and desktop apps, starting with ChatGPT Plus, Team, and SearchGPT waitlist users, followed by Enterprise and Edu users in the coming weeks, and Free users in the months ahead. The search function enables users to ask questions conversationally, with ChatGPT utilizing web information to provide comprehensive answers while considering the entire chat context. Partnerships with various news and data providers ensure the inclusion of real-time information across categories like sports, news, and weather. Chats now feature source links for users to explore further, promoting original content and enhancing user engagement with reputable publishers. The search model is based on a refined version of GPT-4o and is designed for continuous improvement, with plans to expand features in shopping and travel. Users can also utilize a Chrome extension for direct searches.",
      "분류": "### Introducing ChatGPT search\n\n**Category:** 업데이트&트렌드",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Introducing ChatGPT search\n\n**Keywords:** web search integration, natural language interface, real-time information, publisher partnerships, AI-driven content discovery"
    },
    {
      "No.": 4,
      "엔드포인트": "AlphaSignal - Tranding Signal",
      "날짜": "2025.02.04",
      "링크": "https://huggingface.co/blog/open-deep-research",
      "제목": "Open-source DeepResearch – Freeing our search agents",
      "본문": "TLDR\nYesterday, OpenAI released Deep Research, a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time.\n\nOne of the main results in the blog post is a strong improvement of performances on the General AI Assistants benchmark (GAIA), a benchmark we’ve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on especially challenging “level 3” questions that involve multiple steps of reasoning and tool usage (see below for a presentation of GAIA).\n\nDeepResearch is composed of an LLM (which can be selected from the current list of LLMs provided by OpenAI, 4o, o1, o3, etc) and an internal “agentic framework” which guide the LLM to use tools like web search and organize its actions in steps.\n\nWhile powerful LLMs are now freely available in open-source (see e.g. the recent DeepSeek R1 model), OpenAI didn’t disclose much about the agentic framework underlying Deep Research…\n\nSo we decided to embark on a 24-hour mission to reproduce their results and open-source the needed framework along the way!\n\nThe clock is ticking, let’s go! ⏱️\n\nTable of Contents\nWhat are Agent frameworks and why they matter?\nThe GAIA benchmark\nBuilding an open Deep Research\nUsing a CodeAgent\nMaking the right tools 🛠️\nResults 🏅\nCommunity reproductions\nMost important next steps\nWhat are Agent frameworks and why they matter?\nAn Agent framework is a layer on top of an LLM to make said LLM execute actions (like browse the web or read PDF documents), and organize its operations in a series of steps. For a quick intro to agents, check this great interview by Andrew Ng and our introduction blog post to the smolagents library. For a more detailed dive in agents you can subscribe to our agents course that starts in just a few days: link here.\n\nAlmost everyone has already experienced how powerful LLMs can be simply by playing with chatbots.. However, what not everyone is aware of yet is that integrating these LLMs into agentic systems can give them real superpowers!\n\nHere is a recent example comparing the performance of a few frontier LLMs with and without an agentic framework (in this case the simple smolagents library) - using an agentic framework bumps performance by up to 60 points!\n\nBenchmarks\n\nIn fact, OpenAI also highlighted in its release blogpost how Deep Research performed dramatically better than standalone LLMs on the knowledge-intensive \"Humanity’s Last Exam\" benchmark.\n\nSo, what happens when we integrate our current top LLM in an agentic framework, to work toward an open-DeepResearch ?\n\nA quick note: We’ll benchmark our results on the same GAIA challenge but keep in mind that this is a work in progress. DeepResearch is a massive achievement and its open reproduction will take time. In particular, full parity will require improved browser use and interaction like OpenAI Operator is providing, i.e. beyond the current text-only web interaction we explore in this first step.\n\nLet’s first understand the scope of the challenge: GAIA.\n\nThe GAIA benchmark\nGAIA is arguably the most comprehensive benchmark for agents. Its questions are very difficult and hit on many challenges of LLM-based systems. Here is an example of a hard question:\n\nWhich of the fruits shown in the 2008 painting \"Embroidery from Uzbekistan\" were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film \"The Last Voyage\"? Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o'clock position. Use the plural form of each fruit.\n\nYou can see this question involves several challenges:\n\nAnswering in a constrained format,\nUsing multimodal capabilities (to extract the fruits from the image),\nGathering several pieces of information, some depending on others:\nIdentifying the fruits on the picture\nFinding which ocean liner was used as a floating prop for “The Last Voyage”\nFinding the October 1949 breakfast menu for the above ocean liner\nChaining together a problem-solving trajectory in the correct order.\nSolving this requires both high-level planning abilities and rigorous execution, which are two areas where LLMs struggle when used alone.\n\nSo it’s an excellent test set for agent systems!\n\nOn GAIA’s public leaderboard, GPT-4 does not even reach 7% on the validation set when used without any agentic setup. On the other side of the spectrum, with Deep Research, OpenAI reached 67.36% score on the validation set, so an order of magnitude better! (Though we don’t know how they would actually fare on the private test set.)\n\nLet’s see if we can do better with open source tools!\n\nBuilding an open Deep Research\nUsing a CodeAgent\nThe first improvement over traditional AI agent systems we’ll tackle is to use a so-called “code agent”. As shown by Wang et al. (2024), letting the agent express its actions in code has several advantages, but most notably that code is specifically designed to express complex sequences of actions.\n\nConsider this example given by Wang et al.:\n\nCode Agent\n\nThis highlights several advantages of using code:\n\nCode actions are much more concise than JSON.\nNeed to run 4 parallel streams of 5 consecutive actions ? In JSON, you would need to generate 20 JSON blobs, each in their separate step; in Code it’s only 1 step.\nOn average, the paper shows that Code actions require 30% fewer steps than JSON, which amounts to an equivalent reduction in the tokens generated. Since LLM calls are often the dimensioning cost of agent systems, it means your agent system runs are ~30% cheaper.\nCode enables to re-use tools from common libraries\nBetter performance in benchmarks, due to two reasons:\nMore intuitive way to express actions\nExtensive exposure of LLMs to code in training\nThe advantages above were confirmed by our experiments on the agent_reasoning_benchmark.\n\nFrom building smolagents we can also cite a notable additional advantage, which is a better handling of state: this is very useful for multimodal tasks in particular. Need to store this image/audio/other for later use? No problem, just assign it as a variable in your state and you can re-use it 4 steps later if needed. In JSON you would have to let the LLM name it in a dictionary key and trust the LLM will later understand that it can still use it.\n\nMaking the right tools 🛠️\nNow we need to provide the agent with the right set of tools.\n\n1. A web browser. While a fully fledged web browser interaction like Operator will be needed to reach full performance, we started with an extremely simple text-based web browser for now for our first proof-of-concept. You can find the code here\n\n2. A simple text inspector, to be able to read a bunch of text file format, find it here.\n\nThese tools were taken from the excellent Magentic-One agent by Microsoft Research, kudos to them! We didn’t change them much, as our goal was to get as high a performance as we can with the lowest complexity possible.\n\nHere is a short roadmap of improvements which we feel would really improve these tools’ performance (feel free to open a PR and contribute!):\n\nextending the number of file formats which can be read.\nproposing a more fine-grained handling of files.\nreplacing the web browser with a vision-based one, which we’ve started doing here.\nResults 🏅\nIn our 24h+ reproduction sprint, we’ve already seen steady improvements in the performance of our agent on GAIA!\n\nWe’ve quickly gone up from the previous SoTA with an open framework, around 46% for Magentic-One, to our current performance of 55.15% on the validation set.\n\nThis bump in performance is due mostly to letting our agents write their actions in code! Indeed, when switching to a standard agent that writes actions in JSON instead of code, performance of the same setup is instantly degraded to 33% average on the validation set.\n\nHere is the final agentic system.\n\nWe’ve set up a live demo here for you to try it out!\n\n\nHowever, this is only the beginning, and there are a lot of things to improve! Our open tools can be made better, the smolagents framework can also be tuned, and we’d love to explore the performance of better open models to support the agent.\n\nWe welcome the community to come join us in this endeavour, so we can leverage the power of open research together to build a great open-source agentic framework! It would allow anyone to run a DeepResearch-like agent at home, with their favorite models, using a completely local and customized approach!\n\nCommunity Reproductions\nWhile we were working on this and focusing on GAIA, other great open implementations of Deep Research emerged from the community, specifically from\n\ndzhng,\nassafelovic,\nnickscamara,\njina-ai and\nmshumer.\nEach of these implementations use different libraries for indexing data, browsing the web and querying LLMs. In this project, we would like to reproduce the benchmarks presented by OpenAI (pass@1 average score), benchmark and document our findings with switching to open LLMs (like DeepSeek R1), using vision LMs, benchmark traditional tool calling against code-native agents.\n\nMost important next steps\nOpenAI’s Deep Research is probably boosted by the excellent web browser that they introduced with Operator.\n\nSo we’re tackling that next! In a more general problem: we’re going to build GUI agents, i.e. “agents that view your screen and can act directly with mouse & keyboard”. If you’re excited about this project, and want to help everyone get access to such cool capabilities through open source, we’d love to get your contribution!\n\nWe’re also hiring a full time engineer to help us work on this and more, apply if you’re interested 🙂\n\nTo get started with Open Deep Research, try the examples here.\nCheck the smolagents repo.\nRead more about smolagents docs, introduction blog post.",
      "요약문": "### Open-source DeepResearch – Freeing our search agents\n\n**Summary:** OpenAI has launched Deep Research, an advanced system that utilizes a web-browsing capability to summarize content and answer queries. This system demonstrated significant performance improvements on the General AI Assistants benchmark (GAIA), achieving nearly 67% correct answers on 1-shot questions and 47.6% on complex \"level 3\" questions. DeepResearch integrates a large language model (LLM) with an \"agentic framework\" that enables the LLM to perform tasks like web searching and organizing actions methodically.\n\nThe authors aim to replicate OpenAI's results by developing an open-source version of Deep Research, focusing on agent frameworks that enhance LLM capabilities. The GAIA benchmark poses challenging questions that test the reasoning and problem-solving skills of LLMs, highlighting the need for an agentic approach. Current LLMs struggle with such tasks without additional frameworks, but Deep Research has shown a dramatic improvement in performance.\n\nThe project includes developing a \"CodeAgent,\" which expresses actions in code instead of JSON, offering advantages like conciseness and improved performance in benchmarks. Initial efforts have led to a performance increase from 46% to 55.15% on the GAIA validation set, primarily due to the use of code. The authors invite community collaboration to enhance the project and explore various open-source implementations that have emerged.\n\nFuture steps involve creating GUI agents for more interactive tasks and improving browser capabilities. The team is also seeking a full-time engineer to advance this initiative. The project aims to democratize access to powerful agentic frameworks, allowing users to run DeepResearch-like systems locally with customizable models.",
      "분류": "### Open-source DeepResearch – Freeing our search agents\n\n**Category:** 도구",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Open-source DeepResearch – Freeing our search agents\n\n**Keywords:** DeepResearch, Agent Frameworks, GAIA Benchmark, CodeAgent, Open-source AI"
    },
    {
      "No.": 5,
      "엔드포인트": "Decoding ML",
      "날짜": "2025.02.06",
      "링크": "https://decodingml.substack.com/p/build-your-second-brain-ai-assistant?utm_source=substack&utm_medium=email",
      "제목": "Build your Second Brain AI assistant",
      "본문": "The first lesson of the open-source course “Building Your Second Brain AI Assistant Using LLMs and RAG” — a free course that will teach you how to design and build a Notion-like AI assistant that talks to digital notes and resources.\n\nLessons:\nLesson 1: Build your Second Brain AI assistant\n\nLesson 2: Data pipelines for building AI assistants (WIP)\n\nLesson 3: Generate high-quality fine-tuning datasets (WIP)\n\nLesson 4: Fine-tune and deploy open-source LLMs (WIP)\n\nLesson 5: RAG feature pipelines for building AI assistants (WIP)\n\nLesson 6: Agents and LLMOps (WIP)\n\n🔗 Learn more about the course and its outline.\n\nBuild your Second Brain AI assistant\nWelcome to Decoding ML’s “Building Your Second Brain AI Assistant Using LLMs and RAG” open-source course, where you will learn to architect and build a production-ready Notion-like AI assistant:\n\nIntuitively building a Second Brain AI assistant is like having access to the collective wisdom of your own mind. Sounds fantastic, right? More on this later.\n\nTo build the Second Brain AI assistant, we must implement an LLM application using agents, advanced Retrieval-augmented generation (RAG), fine-tuning, LLMOps and AI systems techniques.\n\nThe best way to learn something new is by doing. Thus, within this course, you will learn these concepts and how to apply them while building your Second Brain AI assistant, which you can later customize.\n\nThis lesson will present what you will build and learn throughout the course.\n\nNext, we will explore the system architecture of the Second Brain AI assistant illustrated in Figure 1. We will explain each component's role, what it is, what algorithms and tools we used, and, most importantly, why we used them.\n\nBy the end of this lesson, you will have a strong intuition of what it takes to architect an AI assistant for your Second Brain, such as a Notion-like AI assistant that allows you to chat with your notes and resources.\n\n\nFigure 1: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.\nWe will zoom in on each component in future lessons and present the theory and implementation behind each ML pipeline. Thus, by the end of this course, you will learn how to build your own AI assistant.\n\nLLM systems have the same fundamental blocks. Hence, after going through our lessons, you will have the necessary knowledge to build your production LLM apps on your favorite use cases.\n\nMeanwhile, here is a quick and fun podcast-like audio that will walk you through what you will build and learn through the course (at 1.5x speed, it’s super helpful):\n\nLet’s get started. Enjoy!\n\nTable of Contents:\nWhat are we going to build?\n\nWhat are we going to learn?\n\nIntroducing the custom Notion data source\n\nExploring the flow of the data\n\nPresenting the feature/training/inference (FTI) architecture\n\nArchitecting our Second Brain AI assistant\n\nOffline vs. online ML pipelines\n\nRunning the code\n\n1. What are we going to build?\nAs the first lesson of the open-source six-lesson course “Building Your Second Brain AI Assistant Using LLMs and RAG,” we must clearly define our ultimate goal, including what we will build and learn throughout the process.\n\nDefining our end goal or target is critical whether you're doing a course or implementing a real-world application.\n\nIn this course, we will show you how to build a custom AI assistant on top of your notes, lists or other resources that you usually store in apps such as Notion, Apple Notes, Google Keep, Evernote, Obsidian or similar applications.\n\nThe productivity geeks (such as myself) like to call the system that captures all your thoughts, tasks, meetings, events and notes “your Second Brain.” Usually, a Second Brain is more than just a note-taking app. It includes tools such as a calendar for meetings and cloud storage for, well…, storage.\n\nFor the sake of simplicity, we will narrow down our problem to building an AI assistant on top of our Notion custom data sources, imitating Notion’s AI features, as seen in Figure 2. Another similar example is NotebookLM, where you provide a set of sources, and the AI generates answers only based on them.\n\n\nFigure 2: Screenshot of asking the Notion AI assistant, “How can I optimize LLMs during inference?”\nSo… What will we build?\n\nAn AI assistant that generates answers solely based on our digital knowledge stored in our Second Brain, which, in our case, will be the data we store in Notion.\n\nAs we said in the beginning, it’s like having access to the collective wisdom of your own mind.\n\nThis is a relevant use case for avoiding hallucinations, as you limit the domain to your resources. You can easily control the generation and evaluation steps by conditioning the LLM to your resources.\n\nAs a fun (and relevant) example, we will use our list of filtered resources (which we keep in Notion) on AI and ML, such as GenAI, LLMs, RAG, MLOps, LLMOps and information retrieval.\n\nEveryone has one of those, right?\n\nThe thing is that it gets hard to access exactly what you need when you need it.\n\nThus, we will show you how to hook a GenAI system on top of your research and resources to ask questions, retrieve relevant resources and synthesize information solely based on your research, which you already know is valuable and useful.\n\n2. What are we going to learn?\nFollowing Decoding ML’s mission, we will show you how to build an end-to-end AI system using the Second Brain AI Assistant as an example.\n\nThus, we will walk you through how to design such a system with production in mind.\n\nThen, we will show you how to implement it, starting with collecting data from Notion, preprocessing and storing it, until using it to fine-tune LLMs and build an agentic RAG application.\n\nAs this is an educational project, we tried to avoid using frameworks such as LangChain and build everything from scratch. Doing so will help you develop your intuition, making using LLM frameworks a breeze.\n\nStill, extensibility is a real pain when using LLM frameworks such as LangChain. Thus, real-world skills include extending these frameworks using object-oriented programming (OOP).\n\nThat’s why we used LangChain to load and retrieve data from a MongoDB vector database while showing you how to extend its components and add your app’s custom implementation.\n\nThus, in this course, we will cover the following concepts, algorithms and tools:\n\nArchitecting an AI system using the FTI architecture.\n\nUsing MLOps best practices such as data registries, model registries, and experiment trackers.\n\nCrawling over 700 links and normalizing everything into Markdown using Crawl4AI.\n\nComputing quality scores using LLMs.\n\nGenerating summarization datasets using distillation.\n\nFine-tuning a Llama model using Unsloth and Comet.\n\nDeploying the Llama model as an inference endpoint to Hugging Face serverless Dedicated Endpoints.\n\nImplement advanced RAG algorithms using contextual retrieval, hybrid search and MongoDB vector search.\n\nBuild an agent that uses multiple tools using Hugging Face’s smolagents framework.\n\nUsing LLMOps best practices such as prompt monitoring and RAG evaluation using Opik.\n\nIntegrate pipeline orchestration, artifact and metadata tracking using ZenML.\n\nManage the Python project using uv and ruff.\n\nApply software engineering best practices.\n\nExcited?\n\nLet’s start by exploring our custom Notion data source in more depth.\n\n3. Introducing the custom Notion data source\nYou know what everyone says: “Every successful AI/ML project starts with understanding your data.”\n\nOur use case is not different. It all starts with understanding our custom Notion data source.\n\nIf you are unfamiliar with Notion, you must know that it’s a fancier note-taking app that allows you to create notes, tasks, wikis and databases.\n\nFigure 3 shows our eight Notion databases, which contain various resources on topics such as GenAI, information retrieval, MLOps and system design.\n\nWe use these notes, resources, and research to build AI and software products: “Yes, it’s the actual database we reference while building.”\n\n\nFigure 3: Our Notion databases\nLet’s dig into a specific database.\n\nFigure 4 shows the “Generative AI” Notion database, which contains ~25 data entries on different topics in the space. The “Node” type contains high-level links, such as blogs, benchmarks, or awesome lists, while a “Leaf” contains super-specific resources or tools.\n\n\nFigure 4: The “Generative AI” Notion database.\nLet’s open a data entry.\n\nAs seen in Figure 5, we can see that a data entry contains:\n\nMultiple “Notes” pages containing my thoughts on various topics.\n\nMultiple “Toggle” elements contain links to various blogs, articles or tools.\n\n\nFigure 5: The “LLM Inference Optimization & Other Techniques” Notion database entry\nWe integrated with Notion’s API and automatically downloaded and parsed all these documents in Markdown format.\n\nThe fun and interesting thing about this problem is that the data contains relevant links that must be crawled and further processed into Markdown. But, the catch is that the links are mixed with insightful notes we want to keep and feed into our system. Thus, we must find a way to differentiate between documents that contain only links for crawling and valuable documents by themselves.\n\nA database entry will not always look like the one from Figure 5. The data is noisy and can have any form. The only rule is that it contains links, text, images, and attached documents (similar to a real-world use case).\n\nWe will stick to links and text for this course, but it can be extended to processing images and documents (a fun exercise for you).\n\nFor ease of use, we stored a snapshot of the Notion data from above in a public S3 bucket, which you can download without AWS credentials.\n\nThus, you don’t need to use Notion or hook your Notion to complete this course. But if you want to, you can, as we expose in the GitHub repository, a flexible pipeline that can load any Notion database.\n\nThe next step is to explore the data flow required to build your Second Brain AI assistant, from Notion to fine-tuning and RAG.\n\n4. Exploring the flow of the data\nThe first step to understanding how our AI system looks is to understand its data flow, abstracting away other details such as tooling, infrastructure or algorithms.\n\nOur goal is to collect data for Retrieval-Augmented Generation (RAG). Thus, we can feed our custom data as context to an LLM. We also need to collect data to fine-tune an open-source LLM (such as Llama 3.1 8B) to specialize in summarization (you will soon understand “why summarization”).\n\nIf you are not familiar with how a naive Retrieval Augmented Generation (RAG) system works, read more about it in Decoding ML:\n\nRetrieval-Augmented Generation (RAG) Fundamentals First\nRetrieval-Augmented Generation (RAG) Fundamentals First\nPaul Iusztin\n·\n2024년 8월 31일\nRead full story\nAs illustrated in Figure 6, let’s walk you through the flow and lifecycle of your data:\n\nWe collect raw Notion documents in Markdown format.\n\nWe crawl each link in the Notion documents and normalize them in Markdown.\n\nWe store a snapshot of the data in a NoSQL database.\n\nFor fine-tuning, we filter the documents more strictly to narrow the data to only high-quality samples.\n\nWe use the high-quality samples to distillate a summarization instruction dataset, which we store in a data registry.\n\nUsing the generated dataset, we fine-tune an open-source LLM, which we save in a model registry.\n\nIn parallel, we use a different filter threshold for RAG to narrow down the documents to medium to high-quality samples (for RAG, we can work with more noise).\n\nWe chunk, embed, plus other advanced RAG preprocessing steps to optimize the retrieval of the documents.\n\nWe load the embedded chunks and their metadata in a vector database.\n\nLeveraging the vector database, we use semantic search to retrieve the top K most relevant chunks relative to a user query.\n\n\nFigure 6: The data flow of building a RAG system and fine-tuning LLMs\nIf something doesn’t make sense, don’t worry. Throughout the course, we will zoom in on each component and explain why and how we did everything.\n\nNow that we understand how the data flows, let's quickly examine the feature/training/inference (FTI) design we will use to build our Notion AI assistant.\n\n5. Presenting the feature/training/inference (FTI) architecture\nThe pattern suggests that any AI/ML system can be boiled down to these three pipelines: feature, training, and inference.\n\nJim Dowling, CEO and Co-Founder of Hopsworks, introduced the pattern to simplify building production ML systems.\n\nThe feature pipelines take raw data as input and output features and labels to train our model(s).\n\nThe training pipeline takes the features and labels from the feature stored as input and outputs our trained model(s).\n\nThe inference pipeline inputs the features and labels from the feature store and the trained model(s) from the model registry. Using these two components, we can make predictions in batch or real-time mode and serve them to the client.\n\n\nFigure 7: The feature/training/inference (FTI) architecture\nNote that larger ML systems will have more than three pipelines. Thus, by convention, we name each pipeline based on its output artifact. That’s how we decide whether it’s a feature, training or inference pipeline.\n\nTo conclude, the most important thing you must remember about the FTI pipelines is their interface:\n\nThe feature pipeline takes in data and outputs features & labels saved to the feature store.\n\nThe training pipelines query the features store for features & labels and output a model to the model registry.\n\nThe inference pipeline uses the features from the feature store and the model from the model registry to make predictions.\n\nIt doesn’t matter how complex your ML system gets. These interfaces will remain the same.\n\nThere is a lot more to the FTI architecture. To learn more, consider reading the following article from Decoding ML:\n\nBuilding ML systems the right way using the FTI architecture\nBuilding ML systems the right way using the FTI architecture\nPaul Iusztin\n·\n2024년 8월 10일\nRead full story\n6. Architecting our Second Brain AI assistant\nNow that we have laid out all the foundations, such as understanding the data flow involved in building an AI assistant and the FTI architecture, let’s design our AI system.\n\nAs seen in Figure 8, we have 5 significant components that we have to understand:\n\nThe data pipelines\n\nThe feature pipelines\n\nThe training pipeline\n\nThe inference pipelines\n\nThe observability pipeline\n\nYou might wonder why we have five pipelines instead of three, as the FTI architecture suggests.\n\nThe data engineering team often owns the data pipeline, which prepares the data required to build AI systems.\n\nMeanwhile, the observability pipeline is implemented on top of the FTI architecture to monitor and evaluate the system. Having eyes and ears all over your system is critical for success, especially in LLM systems, which are highly non-deterministic.\n\nAn ideal strategy is to implement an end-to-end workflow of your app quickly, plus the observability pipeline. Then, you use the metrics from the evaluation pipeline and logs from the prompt monitoring pipeline as clear signals on what works and what doesn’t.\n\n\nFigure 8: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.\nLet’s zoom in on each.\n\nThe data pipelines\nAs the name suggests, the data pipelines collect data from Notion, clean it, standardize it, and load it to a NoSQL database as our clean and normalized data snapshot before feeding it into the feature pipelines.\n\nBy storing the standardized and cleaned data in a NoSQL database, such as MongoDB, we collect the data once (as a backup), which we can use to experiment and build our AI system.\n\nIn larger AI systems, this is usually done with a data warehouse or lake, but a NoSQL database is lighter and does the job for us.\n\nThe data pipeline is split into two significant components.\n\nThe data collection pipeline: It uses Notion’s API to retrieve the data programmatically, where each Notion data entry is standardized to Markdown format and saved as JSON along with necessary metadata. To decouple the data collection step from the rest of the system, we save everything to a public S3 bucket to avoid giving public access to our Notion workspace.\n\nThe ETL pipeline: It extracts the raw Notion documents from S3, finds all the embedded links within the documents, crawls them, and standardizes them into Markdown format. It also computes a quality score/document using LLMs. Ultimately, it saves all the documents and metadata into a MongoDB NoSQL database.\n\nThe feature pipelines\nThe feature pipelines leverage the standardized and clean data from the MongoDB NoSQL database for two things:\n\nTo populate a MongoDB vector database for doing RAG.\n\nTo generate a summarization instruct dataset for fine-tuning an LLM.\n\nThese are two standard feature pipelines you will see in the GenAI world.\n\nWithin the RAG feature pipeline, we will implement advanced pre-retrieval RAG techniques, such as Contextual Retrieval, proposed by Antrophic. To implement it, we will require a summarization LLM and hybrid search.\n\nUltimately, we chunk the documents, embed them, and loaded them into a MongoDB vector database.\n\nFor more theory on advanced RAG and how a naive RAG system can be optimized, you can read our article from Decoding ML:\n\nYour RAG is wrong: Here's how to fix it\nYour RAG is wrong: Here's how to fix it\nPaul Iusztin\n·\n1월 2일\nRead full story\nUsing APIs such as OpenAI for summarization can get costly (which we need for Contextual Retrieval), so we fine-tune a summarization open-source LLM. To do this, we require a custom summarization dataset.\n\nThus, we will leverage distillation techniques for the dataset generation pipeline to create a high-quality summarization instruction dataset based on our documents.\n\nWe will save the generated dataset to Hugging Face’s data registry. As an example, you can check our generated dataset: pauliusztin/second_brain_course_summarization_task\n\nThe training pipeline\nThe training pipeline reads the instruct dataset from the data registry and uses Unsloth to fine-tune a Llama 3.1 8B LLM. We use Comet to log the metrics and hyperparameters between multiple experiments, compare them, and pick the best one.\n\nAfter deciding on the best model, we load it into Hugging Face’s model registry. As an example, you can check our fine-tuned LLM: pauliusztin/Meta-Llama-3.1-8B-Instruct-Second-Brain-Summarization\n\nZenML orchestrates and manages the data, feature, and training pipelines, helping us run the pipelines with a clearly defined structure and configuration. As illustrated in Figure 9, we can track the progress, status and history of each pipeline in a beautiful UI.\n\n\nFigure 9: Visualizing our ML pipelines from ZenML’s dashboard.\nThe inference pipelines\nWe have two inference pipelines.\n\nThe summarization inference pipeline, which contains only the fine-tuned LLM, is deployed as a real-time inference endpoint on Hugging Face’s Dedicated Endpoints serverless service.\n\nThe agentic inference pipeline is our AI assistant, which takes as input requests from a user and provides answers leveraging the data from the MongoDB vector database.\n\nWe implemented it using Hugging Face’s smolagents Python framework, which allows us to build agents without hiding too much of what is going on behind the scenes. We attached a retriever tool that interacts with the vector database and a summarization tool to help us synthesize answers.\n\nWe will also attach the agentic inference pipeline to a Gradio UI to completely simulate the experience of an AI assistant, as shown in Figure 10.\n\n\nFigure 10: Screenshot of our AI assistant Gradio UI asking it to write a paragraph on optimizing LLMs during inference. We can visualize how it used the MongoDB vector search tool to get more context from our custom data until it reached the maximum allowed number of steps.\nThe observability pipeline\nthe last piece of the puzzle is the observability pipeline, which consists of two main components:\n\nPrompt monitoring\n\nLLM evaluation\n\nFor both, we will use Opik, which provides a beautiful dashboard for monitoring complex prompt traces, as seen in Figure 11.\n\nIt also provides a Python SDK to help us evaluate agentic and RAG applications, track the results and compare them (similar to experiment tracking, but for evaluating LLM applications).\n\n\nFigure 11: Screenshot from Opik on monitoring the trace of an agent. We can visualize the user’s prompt and all the hidden steps until it reaches its final answer.\n7. Offline vs. online ML pipelines\nOne last architectural decision we have to highlight is the difference between the offline and online ML pipelines.\n\nOffline pipelines are batch pipelines that run on a schedule or trigger. They usually take input data, process it, and save the output artifact in another type of storage. From there, other pipelines or clients can consume the artifact as they see fit.\n\nThus, in our AI system, the offline ML pipelines are the\n\nData collection pipeline\n\nETL data pipeline\n\nRAG feature pipeline\n\nDataset generation feature pipeline\n\nTraining pipeline\n\nThese are all independent processes that can run one after the other or on different schedules. They don’t have to run in sequence, as they are entirely decoupled through various storages: a NoSQL database, a vector database, a data registry or a model registry.\n\nBecause of their nature, we will orchestrate all the offline pipelines using ZenML, a popular ML orchestrator that allows us to schedule, trigger, configure, or deploy each pipeline.\n\n\nFigure 12: Offline vs. online ML pipelines\nOn the other hand, we have online pipelines that directly interact with a client. In this setup, a client (e.g., a user or other software) requests a prediction in real or near real-time. Thus, the system has to be online 24/7, process the request, and return the answer.\n\nIn our use case, the online pipelines are the following:\n\nAgentic inference pipeline\n\nSummarization inference pipeline\n\nObservability pipeline\n\nBecause of their request-answer nature, online pipelines do not need orchestration. Instead, they adopt a strategy similar to deploying RESTful APIs from the software engineering world.\n\nIt is critical to highlight that the offline and online pipelines are entirely different processes and often entirely different applications.\n\nSeeing these LangChain PoCs, where the RAG ingestion, retrieval and generation are in the same Notebook, can be deceiving. You never (or almost never) want to ingest the data at query time; you want to do it offline. Thus, when the user asks a question, the vector database is already populated and ready for retrieval.\n\nTo clearly reflect this aspect, our codebase decoupled the offline and online pipelines into two different Python applications, as shown in Figure 13.\n\n\nFigure 13: The folder structure of the offline and online Python applications.\nThe last step is to say a few words about how you can run the code.\n\n8. Running the code\nThis lesson was only an overview of what you will learn in the following five lessons. Thus, there is no specific code attached to this lesson.\n\nHowever, if you want to test our code without going through the following lessons, we have provided end-to-end instructions on how to do so in our GitHub repository.\n\nThus, you can choose your learning journey: go through our lessons or directly try out the code.\n\nEnjoy!\n\nConclusion\nThis lesson taught you what you will build and learn throughout the “Building Your Second Brain AI Assistant Using LLMs and RAG” open-source course.\n\nIn this lesson, we’ve laid out the foundations by presenting the data flow of the AI assistant and the FTI architecture.\n\nNext, we’ve shown how to apply the FTI architecture on top of our data flow to architect a production-ready AI assistant.\n\nLesson 2 (WIP) will focus on implementing the ETL data pipeline. While building it, we will learn how to use ZenML to orchestrate offline ML pipelines, crawl custom URLs, parse them to Markdown, and compute a quality score/document using LLMs.\n\n💻 Explore all the lessons and the code in our freely available GitHub repository.\n\nIf you have questions or need clarification, feel free to ask. See you in the next session!\n\nWhenever you’re ready, there are 3 ways we can help you:\nPerks: Exclusive discounts on our recommended learning resources\n\n(live courses, self-paced courses, learning platforms and books).\n\nThe LLM Engineer’s Handbook: Our bestseller book on mastering the art of engineering Large Language Models (LLMs) systems from concept to production.\n\nFree open-source courses: Master production AI with our end-to-end open-source courses, which reflect real-world AI projects, covering everything from system architecture to data collection and deployment.\n\nReferences\nDecodingml. (n.d.). GitHub - decodingml/second-brain-ai-assistant-course. GitHub. https://github.com/decodingml/second-brain-ai-assistant-course\n\nIusztin, P. (2024a, August 10). Building ML system using the FTI architecture. Decoding ML. https://decodingml.substack.com/p/building-ml-systems-the-right-way\n\nIusztin, P. (2024b, August 31). RAG Fundamentals first. Decoding ML. https://decodingml.substack.com/p/rag-fundamentals-first\n\nIusztin, P. (2025a, January 2). Advanced RAG Blueprint: Optimize LLM retrieval Systems. Decoding ML. https://decodingml.substack.com/p/your-rag-is-wrong-heres-how-to-fix",
      "요약문": "### Build your Second Brain AI assistant\n\n**Summary:** The open-source course “Building Your Second Brain AI Assistant Using LLMs and RAG” aims to teach participants how to create a Notion-like AI assistant that interacts with digital notes and resources. The course consists of six lessons, starting with an overview of building the AI assistant and continuing with topics such as data pipelines, dataset generation, fine-tuning LLMs, and implementing RAG features. \n\nThe course emphasizes a practical, hands-on approach to learning AI system architecture, focusing on a custom AI assistant that can generate answers based solely on users' stored knowledge. Core concepts include data collection from Notion, preprocessing data, and employing machine learning techniques to enhance the assistant's capabilities. The architecture involves various pipelines, including data, feature, training, and inference pipelines, as well as an observability pipeline for monitoring system performance. \n\nBy the end of the course, participants will have the skills needed to build their own AI assistant, leveraging advanced techniques such as retrieval-augmented generation and LLMOps best practices. The course encourages a structured learning journey, with practical examples and a supportive GitHub repository for hands-on coding experiences.",
      "분류": "### Build your Second Brain AI assistant\n\n**Category:** 도구",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Build your Second Brain AI assistant\n\n**Keywords:** Second Brain, AI Assistant, Retrieval-Augmented Generation (RAG), Fine-Tuning, Data Pipelines"
    },
    {
      "No.": 6,
      "엔드포인트": "Decoding ML",
      "날짜": "2025.02.13",
      "링크": "https://decodingml.substack.com/p/data-pipelines-for-ai-assistants?utm_source=substack&utm_medium=email",
      "제목": "Data pipelines for AI assistants",
      "본문": "Data pipelines for AI assistants\nThe backbone of successful AI systems\nPaul Iusztin\nFeb 13, 2025\n\nThe second lesson of the open-source course Building Your Second Brain AI Assistant Using Agents, LLMs and RAG — a free course that will teach you how to architect and build a personal AI research assistant that talks to your digital resources.\n\nA journey where you will have the chance to learn to implement an LLM application using agents, advanced Retrieval-augmented generation (RAG), fine-tuning, LLMOps, and AI systems techniques.\n\nLessons:\nLesson 1: Build your Second Brain AI assistant\n\nLesson 2: Data pipelines for AI assistants\n\nLesson 3: Generate high-quality fine-tuning datasets (WIP)\n\nLesson 4: Fine-tune and deploy open-source LLMs (WIP)\n\nLesson 5: RAG feature pipelines for building AI assistants (WIP)\n\nLesson 6: Agents and LLMOps (WIP)\n\n🔗 Learn more about the course and its outline.\n\nData pipelines for AI assistants\nWelcome to Lesson 2 of Decoding ML’s Building Your Second Brain AI Assistant Using Agents, LLMs and RAG open-source course, where you will learn to architect and build a production-ready Notion-like AI research assistant.\n\nEvery data and AI system starts with data. If you don’t have data, you don’t have the raw material to work with. You can have the most fancy algorithms, but without data, they are still like a car without fuel.\n\nHence, this lesson will teach us to architect and build the data pipelines that fuel our Second Brain AI assistant, such as the Notion data collection and ETL data pipelines.\n\nWhile implementing the data pipelines, we will learn the following:\n\nUse an MLOps framework such as ZenML to manage the ML pipelines.\n\nRead documents from Notion.\n\nStructure and validate our documents using Pydantic.\n\nCrawl ~400 links found in the Notion documents using Crawl4AI.\n\nCompute quality scores for each document using a two-stage design based on heuristics and LLMs.\n\nNormalize all the documents to Markdown.\n\nStore all the standardized documents in a NoSQL MongoDB.\n\n\nFigure 1: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.\nLet’s get started. Enjoy!\n\nPodcast version of the lesson\nTable of contents:\nArchitecting data pipelines for AI systems\n\nUnderstanding MLOps frameworks for managing ML pipelines\n\nExploring the collected Notion data\n\nLooking into requirements for crawling\n\nImplementing crawling\n\nComputing the quality score\n\nLoading the standardized data to a NoSQL database\n\nRunning the code\n\n1. Architecting data pipelines for AI systems\nIn most use cases, a data pipeline starts with raw data collection, undergoes some transformation steps, and ultimately loads the transformed data into storage.\n\nTo build our Second Brain AI assistant, we must collect data from Notion, crawl all the links found in the Notion documents, and standardize everything in Markdown so downstream processes can easily process the documents.\n\nTo implement that, we split the data pipelines into two main components, making the architecture flexible and scalable.\n\nThe data collection pipeline\nWhich uses Notion’s API to access our personal data programmatically. Then, it extracts all the links from the crawled documents and adds them to their metadata. Ultimately, it standardizes the document into Markdown, saves it as JSON, and loads the results in S3.\n\nForcing people to use their Notion collections wasn’t a scalable solution or a good user experience, and neither was making our Notion collection public.\n\nThus, we decided to save our processed Notion collection into a public S3 bucket, which everyone can access effortlessly.\n\nTo draw a parallel to the industry, the S3 bucket could be seen as a data lake, where multiple teams from the organization push raw data that can be used within the company.\n\nThe course will start with reading our custom Notion collection from S3 and the next steps in processing. However, if you want a personalized experience, we provide the code and instructions for collecting your Notion collections.\n\nThus, let’s dig into the ETL data pipeline, where most of our focus will be.\n\nThe ETL data pipeline\nETL stands for “Extract, Transform, Load,” a popular data engineering pattern applied to most data pipelines.\n\nThe pattern is simple. You have to extract data from a source, apply some transformations, and ultimately load it into storage that makes the processed data accessible.\n\nHere is how it looks to us:\n\nExtract storage: S3\n\nTransformations: crawling, standardization to Markdown, computing a quality score\n\nLoad storage: NoSQL MongoDB\n\nNow, let’s dig into the details of each step.\n\nOnce the data has been downloaded from S3, we want to enrich it by crawling all the links within the Notion documents. This makes sense for two core reasons:\n\nWhen you chat with your AI assistant, you are primarily curious about what’s inside the link, not the link itself.\n\nWe have more data for fine-tuning our summarization LLM.\n\nAfter the documents are crawled, they are standardized to Markdown format (as well) and added to our existing collection.\n\nWe track the source of each document within the metadata, and because the documents are in Markdown format regardless of their source, we can treat the Notion and crawled documents equally and store them in the same collection. This will make our lives 100 times easier during future processing steps.\n\nFormatting everything in Markdown is critical because it ensures the data is standardized and more straightforward to process downstream in the pipeline. This standardization is very important because the data will later be used for RAG (Retrieval Augmented Generation) and to fine-tune a summarization large language model (LLM).\n\n\nFigure 2: The architecture of the ETL data pipeline\nNow that the data has been augmented, ensuring its quality is essential. We compute a quality score for each document. This quality score is a number between 0 and 1, which you can use to filter the documents based on the target quality you are looking for.\n\nYou know the famous saying regarding AI systems: “Trash in, trash out.”\n\nHence, this is a critical step for fine-tuning high-quality LLMs and doing RAG correctly.\n\nWe want to filter the data differently depending on whether we use it for RAG or fine-tuning. Thus, we store all documents with the quality score in the metadata. This allows downstream pipelines to decide what to filter.\n\nUltimately, the standardized data is loaded into a NoSQL database and ready for consumption by the feature pipelines, which will further process it for RAG and fine-tuning.\n\nIt is a good practice to create a snapshot of the data between the data and AI layers. This decouples the two, allowing you to run the data pipeline once and experiment further with your AI architecture. You can extend this by versioning your data sources and making the data available across your company (instead of just your project).\n\nAs before, to draw a parallel to the industry, this is similar to how a data warehouse, such as Big Query, connects the dots between the data and AI systems.\n\nThe last piece of the puzzle is our MLOps framework. For our use case, we picked ZenML. Through ZenML, we will manage all our offline pipelines, such as the data collection, ETL pipeline and feature engineering pipelines.\n\nBut why do we need an MLOps framework in the first place? Let’s dig into this in the next section.\n\n2. Understanding MLOps frameworks for managing ML pipelines\nMLOps frameworks are powerful tools that allow you to easily manage, schedule, track, and deploy your ML pipelines.\n\nMetaflow and ZenML are popular MLOps frameworks. They are optimized for long-running ML jobs, tracking metadata and output artifacts for reproducibility and setting up complex environments required for training and inference.\n\nOne core component of these MLOps frameworks is orchestrating offline ML pipelines. There is a fine line between data and ML orchestrators. Popular data orchestrators include Airflow and Prefect, which are optimized for running multiple small units in parallel.\n\nHowever, these data engineering tools are not built with ML as their first citizen. For example, they don’t include robust features for tracking and versioning output artifacts. In reality, they started rolling out ML-related features, but you will quickly realize they are forced and don’t fit naturally into their SDKs.\n\nFor our course, we chose ZenML because it can quickly run locally in dev mode, has a beautiful UI, and has an intuitive Python SDK.\n\nIt also supports all the requirements for most ML projects, such as model management and tracking configuration, metadata, and output artifacts per pipeline.\n\nIt also supports infrastructure management for all the popular cloud services such as AWS, GCP, Azure, and more. They recently introduced new features that allow you to quickly deploy all your pipelines using Terraform (Infrastructure as Code) or directly deploying them from their dashboard with zero code involved in the process (the heaven for Data Scientists).\n\nTo conclude, we need an MLOps framework to easily track, reproduce, schedule, and deploy all our offline ML pipelines without reinventing the wheel.\n\nEnough with the theory. Let’s quickly take a look at how ZenML works.\n\nFor example, Figure 3 shows ZenML’s dashboard with all the pipelines we’ve run in their latest state.\n\n\nFigure 3: ZenML’s pipelines dashboard.\nIf we zoom in, for example, in the ETL pipeline, we can see all the previous runs with essential details, such as the infrastructure (“Stack”) in which they ran, as seen in Figure 4.\n\n\nFigure 4: ZenML’s ETL pipeline dashboard.\nUltimately, if we click on a specific pipeline run, we can see the whole Directed Acyclic Graph (DAG) with all its steps. If it fails, we can see the steps it failed at. Also, as seen in Figure 5, we can easily visualize and track the output of each step.\n\n\nFigure 5: Visualising the ETL pipeline run in ZenML.\nThere is much more to ZenML. However, to avoid creating a section that sounds like documentation, we will highlight its other features while building our AI assistant.\n\nAs we said in the previous section, this course will start with the ETL data pipeline, which will use our precomputed Notion dataset. Hence, let’s implement it in ZenML.\n\nWhat does our ETL data pipeline look like when implemented in ZenML?\nThe heart of our pipeline is the etl() function, decorated with ZenML's @pipeline decorator (found at pipelines/etl.py). This function orchestrates the entire data flow, accepting configuration parameters that control its behavior - from specifying data directories to controlling parallel processing and quality scoring settings:\n\n@pipeline\ndef etl(\n    data_dir: Path,\n    load_collection_name: str,\n    to_s3: bool = False,\n    max_workers: int = 10,\n    quality_agent_model_id: str = \"gpt-4o-mini\",\n    quality_agent_mock: bool = True,\n) -> None:\nThe pipeline's workflow begins by setting up the data paths. We establish two key directories: one for reading the raw Notion data and another for storing the processed results:\n\n    notion_data_dir = data_dir / \"notion\"\n    crawled_data_dir = data_dir / \"crawled\"\nThe primary data processing flow consists of three major steps. First, we read the documents from the disk. Then, we crawl every link found in each document. Next, we enhance the documents with quality scores:\n\n    documents = read_documents_from_disk(\n        data_directory=notion_data_dir, nesting_level=1\n    )\n    crawled_documents = crawl(documents=documents, max_workers=max_workers)\n    enhanced_documents = add_quality_score(\n        documents=crawled_documents,\n        model_id=quality_agent_model_id,\n        mock=quality_agent_mock,\n        max_workers=max_workers,\n    )\nThe final stage of our pipeline handles data persistence. We save the enhanced documents to disk, optionally upload them to S3, and finally load them into MongoDB for downstream processing:\n\n    save_documents_to_disk(documents=enhanced_documents, output_dir=crawled_data_dir)\n    if to_s3:\n        upload_to_s3(\n            folder_path=crawled_data_dir,\n            s3_prefix=\"second_brain_course/crawled\",\n            after=\"save_documents_to_disk\",\n        )\n    ingest_to_mongodb(\n        models=enhanced_documents,\n        collection_name=load_collection_name,\n        clear_collection=True,\n    )\nIn Figure 6, we can see what the ETL pipeline looks like in ZenML:\n\n\nFigure 6: An ETL pipeline run from ZenML.\nIn future sections of the article, we will zoom in on each step and understand how it works.\n\nOne last key feature of ZenML is that it can be configured through YAML configuration files (one per pipeline). This allows you to easily configure each pipeline run without touching the code. Most importantly, you can track and version the configuration of each pipeline run, which is critical for reproducibility and debugging.\n\nLet’s look at it in more detail (found under configs/etl.yaml):\n\nparameters:\n  data_dir: data/\n  load_collection_name: raw\n  to_s3: false\n  max_workers: 4\n  quality_agent_model_id: gpt-4o-mini\n  quality_agent_mock: false\nAs you can see, it’s a YAML file that is one-on-one with the pipeline Python function parameters. As the pipeline function acts as the entry point to your application, it makes sense to be able to configure it from a clean YAML file that can be easily tracked by git instead of tweaking the values from the CLI.\n\nIf you are curious to learn more about ZenML, they have some fantastic guides while also learning production MLOps and LLMOps:\n\nMore on ZenML\n\n3. Exploring the collected Notion data\nIn Lesson 1, we explored how our data looks directly in Notion. However, as we start working with it only after our data collection pipeline collects it, we have to visualize how the data we ingest from S3 looks like, as that is what we will work with.\n\nThe data is stored in JSON, containing the content of the Notion document in Markdown, along with its metadata and embedded URLs. Here is one sample:\n\n{\n    \"id\": \"8eb8a0ed6afffaa581ef6dff9b3eec17\",\n    \"metadata\": {\n        \"id\": \"8eb8a0ed6afffaa581ef6dff9b3eec17\",\n        \"url\": \"https://www.notion.so/Training-Fine-tuning-LLMs-8eb8a0ed6afffaa581ef6dff9b3eec17\",\n        \"title\": \"Training & Fine-tuning LLMs\",\n        \"properties\": {\n            \"Type\": \"Leaf\"\n        }\n    },\n    \"parent_metadata\": {\n        \"id\": \"6cfa25bcea00377355cfe21f7dfaadff\",\n        \"url\": \"\",\n        \"title\": \"\",\n        \"properties\": {}\n    },\n    \"content\": \"# Resources [Community]\\n\\n\\t<child_page>\\n\\t# Number of samples for fine-tuning based on general, domain, task-specific \n... # The rest of the document in Markdown format\n\"\n    \"content_quality_score\": null,\n    \"summary\": null,\n    \"child_urls\": [\n        \"https://github.com/huggingface/trl/\",\n        \"https://www.linkedin.com/company/liquid-ai-inc/\",\n        \"https://github.com/unslothai/unsloth/\",\n        \"https://arxiv.org/abs/2106.09685/\",\n        \"https://paperswithcode.com/sota/code-generation-on-humaneval/\",\n        \"https://github.com/axolotl-ai-cloud/axolotl/\",\n        \"https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/\",\n        \"https://arxiv.org/abs/2305.14314/\"\n}\nEvery document is stored in its own JSON following the same structure:\n\nmetadata: containing the source URL plus other details about the source\n\nparent_metadata: containing the parent’s URL, plus other details about the parent (if empty, it has no parent)\n\ncontent: the actual content in Markdown format\n\nchild_urls: all the URLs that were found in the document’s content (notice how diverse the links are)\n\nThe next step is to load this data into Python while ensuring that each JSON file is valid (has the expected structure and data types). The preferred method for this is using Pydantic.\n\nHow do we model this data into a Pydantic class?\nLet’s see how we can model our data using Pydantic, the go-to Python package for defining modern data structures in Python. But first, let’s make a quick analogy to LangChain.\n\nWhen working with LangChain, one of the fundamental building blocks is the Document class. Let's explore how we can implement our own version that builds upon LangChain's concept while respecting our custom functionality and needs.\n\nOur Document class maintains LangChain's core principle of combining content with metadata while extending it with additional features. Each document has its unique identifier, content, and structured metadata, plus we've added support for document hierarchies, quality assessment, and summarization.\n\nclass Document(BaseModel):\n    id: str = Field(default_factory=lambda: utils.generate_random_hex(length=32))\n    metadata: DocumentMetadata\n    parent_metadata: DocumentMetadata | None = None\n    content: str\n    content_quality_score: float | None = None\n    summary: str | None = None\n    child_urls: list[str] = Field(default_factory=list)\n\n    @classmethod\n    def from_file(cls, file_path: Path) -> \"Document\":\n        json_data = file_path.read_text(encoding=\"utf-8\")\n        return cls.model_validate_json(json_data)\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Document):\n            return False\n        return self.id == other.id\n\n    def __hash__(self) -> int:\n        return hash(self.id)\nWhile LangChain uses a simple dictionary for metadata, we've created a dedicated DocumentMetadata class. This structured approach ensures consistent metadata across our pipeline and provides better type safety:\n\nclass DocumentMetadata(BaseModel):\n    id: str\n    url: str\n    title: str\n    properties: dict\nBy storing the source URL and the parent’s source URL within the metadata while doing RAG, we can show the user the source used as context and where the source originates from. For example, we can show the user from which Notion database the link was accessed and what link was crawled.\n\nOne last thing to highlight is that storing the parent_metadata and child_urls fields can extend this to multi-hoping algorithms simulating a GraphDB structure. We won’t do that in our course, but it’s good to know that you don’t necessarily need a GraphDB to do GraphRAG. In cases where you need parents or children from only 2-3 levels relative to your data point, modeling your data using relationships is good enough to get started.\n\nBefore we discuss the implementation, we need to review some basic requirements for crawling.\n\n4. Looking into requirements for crawling\nSo, how does crawling work? Essentially, it's about automatically visiting web pages, extracting the content, and following links to discover more pages. This process can be very complex because every website has a different structure, and there are many ways to present content. You'll need to ensure you're able to handle this complexity.\n\nBefore crawling any website, you must check the site’s crawling limitations. You can do that by adding /robots.txt to the end of the website's URL. This file tells you which parts of the website are off-limits for web crawlers. Respecting these rules is essential since they protect the website from overloading and ensure you’re not crawling sensitive information.\n\n\nFigure 7: Result after accessing “https://www.youtube.com/robots.txt”\nNow, if you want to crawl a website, you’ll also need to know all the pages you need to visit. You might think you can start from a home page and follow all the links. However, you'll soon discover this approach can be inefficient and may not capture all website pages.\n\nThat’s why many websites provide a sitemap that lists all their pages. The sitemap is usually added to better index the site for search engines (which also crawl it), but we can also leverage it to get a list of recurrent links we can crawl easily.\n\nYou can usually find this sitemap by adding /sitemap.xml to the end of the website's URL. This file gives you a structured list of all the website’s sub-URLs, which makes it a lot easier to do a recursive crawl, which means you can follow all the links on the website.\n\n\nFigure 8: Result after accessing “https://neptune.ai/sitemap.xml”\nNow, you need a tool to do all this.\n\nWe will use Crawl4AI for crawling, an open-source web crawling framework specifically designed to scrape websites and format the output for LLMs to understand.\n\nThe tool has built-in HTML to Markdown conversion, which is perfect for our needs. Crawl4AI is designed to be efficient, fast, and easy to set up. It can handle things like proxies, session management, and removing irrelevant content, which are not easy to handle.\n\n5. Implementing crawling\nHow can we apply these principles to our crawling algorithm?\n\nOur code is educative and harmless. Hence, to keep things simple, we will skip checking the robots.txt file. But it’s super important to remember this when working with real-world products.\n\nAlso, to avoid working with too many links, we will skip checking the sitemap.xml file and stick to the links found directly on our Notion pages. However, we could easily augment our dataset by accessing the sitemap.xml file of each link we use from Notion, expanding our dataset exponentially.\n\nWith that in mind, let’s dig into the implementation.\n\nAt the heart of our crawling step, we have a ZenML step that orchestrates the crawling process (which is called from the ZenML pipeline):\n\n@step\ndef crawl(\n    documents: list[Document], max_workers: int = 10\n) -> Annotated[list[Document], \"crawled_documents\"]:\n    crawler = Crawl4AICrawler(max_concurrent_requests=max_workers)\n    child_pages = crawler(documents)\n\n    augmented_pages = documents.copy()\n    augmented_pages.extend(child_pages)\n    augmented_pages = list(set(augmented_pages))\nTo track our crawling progress and provide insights into the pipeline's performance, we add metadata about the number of documents processed:\n\n    step_context = get_step_context()\n    step_context.add_output_metadata(\n        output_name=\"crawled_documents\",\n        metadata={\n            \"len_documents_before_crawling\": len(documents),\n            \"len_documents_after_crawling\": len(augmented_pages),\n            \"len_documents_new\": len(augmented_pages) - len(documents),\n        },\n    )\n\n    return augmented_pages\nThis metadata is attached to the crawled_documents output artifact and can be visualized from ZenML’s dashboard. As shown in Figure 9, it helps us monitor and debug each pipeline run.\n\n\nFigure 9: Metadata of the `crawled_documents` artifact.\nNow, let's look at our Crawl4AICrawler class implementation, which leverages the powerful features of Crawl4AI. This crawler is designed to handle concurrent web crawling efficiently while providing clean, LLM-ready output in Markdown format.\n\nThe crawler class under the hood uses Crawl4AI's AsyncWebCrawler with its sophisticated browser and crawler configurations. As the class is called from the ZenML step, which runs in a Python synchronous process, and the crawler uses an async method, we must define and manage an async loop internally. Using async, it’s a powerful mechanism in Python to manage concurrently I/O dependent processes such as crawling or API requests:\n\nclass Crawl4AICrawler:\n    def __init__(self, max_concurrent_requests: int = 10) -> None:\n        self.max_concurrent_requests = max_concurrent_requests\n\n    def __call__(self, pages: list[Document]) -> list[Document]:\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            return asyncio.run(self.__crawl_batch(pages))\n        else:\n            return loop.run_until_complete(self.__crawl_batch(pages))\nThe core crawling logic happens in the __crawl_batch method. We use Crawl4AI's CacheMode.BYPASS to ensure fresh content:\n\n    async def __crawl_batch(self, pages: list[Document]) -> list[Document]:\n        process = psutil.Process(os.getpid())\n        start_mem = process.memory_info().rss\n        \n        semaphore = asyncio.Semaphore(self.max_concurrent_requests)\n        all_results = []\n\n        async with AsyncWebCrawler(cache_mode=CacheMode.BYPASS) as crawler:\n            for page in pages:\n                tasks = [\n                    self.__crawl_url(crawler, page, url, semaphore)\n                    for url in page.child_urls\n                ]\n                results = await asyncio.gather(*tasks)\n                all_results.extend(results)\n\n        successful_results = [result for result in all_results if result is not None]\nFinally, each URL is processed by the __crawl_url method, which utilizes Crawl4AI's powerful content extraction and Markdown generation capabilities. As we kick off all the jobs simultaneously, we control how many concurrent requests we can run simultaneously using the semaphore Python object. This is useful for managing your computer’s resources or API limits:\n\n    async def __crawl_url(\n        self,\n        crawler: AsyncWebCrawler,\n        page: Document,\n        url: str,\n        semaphore: asyncio.Semaphore,\n    ) -> Document | None:\n        async with semaphore:\n            result = await crawler.arun(url=url)\n            await asyncio.sleep(0.5)  # Rate limiting\n\n            if not result or not result.success or result.markdown is None:\n                return None\n\n            child_links = [\n                link[\"href\"]\n                for link in result.links[\"internal\"] + result.links[\"external\"]\n            ]\n            title = result.metadata.pop(\"title\", \"\") if result.metadata else \"\"\n\n            return Document(\n                id=utils.generate_random_hex(length=32),\n                metadata=DocumentMetadata(\n                    id=document_id,\n                    url=url,\n                    title=title,\n                    properties=result.metadata or {},\n                ),\n                parent_metadata=page.metadata,\n                content=str(result.markdown),\n                child_urls=child_links,\n            )\nCrawl4AI is particularly well-suited for our needs as it's designed to be LLM-friendly. It generates clean Markdown output directly and handles all the formatting complexity behind the scenes.\n\nThe next step from our ETL pipeline is to compute the quality score.\n\n6. Computing the quality score\nAssessing content quality is crucial when processing documents in data pipelines. RAG systems can be incredibly powerful as long they use high-quality context. Also, when it comes to fine-tuning LLMs, using high-quality samples is the most critical aspect in which you should invest.\n\nThus, let's explore a sophisticated two-stage system that combines quick heuristic rules with an advanced LLM-based evaluation approach.\n\nAs parsing the document using an LLM can quickly increase the latency and costs of your system, we try first to do our best and compute the quality score using a set of heuristics. The key is to use heuristics where we are 100% sure we can score the documents. Next, for more complex and nuanced scenarios, we use the LLM.\n\nOur ZenML pipeline step orchestrates the quality scoring process, starting with fast heuristics and falling back to the LLM-based method when needed:\n\n@step\ndef add_quality_score(\n    documents: list[Document],\n    model_id: str = \"gpt-4o-mini\",\n    mock: bool = False,\n    max_workers: int = 10,\n) -> Annotated[list[Document], \"scored_documents\"]:\n    heuristic_quality_agent = HeuristicQualityAgent()\n    scored_documents: list[Document] = heuristic_quality_agent(documents)\n\n    scored_documents_with_heuristics = [\n        d for d in scored_documents if d.content_quality_score is not None\n    ]\n    documents_without_scores = [\n        d for d in scored_documents if d.content_quality_score is None\n    ]\n\n    quality_agent = QualityScoreAgent(\n        model_id=model_id, mock=mock, max_concurrent_requests=max_workers\n    )\n    scored_documents_with_agents: list[Document] = quality_agent(\n        documents_without_scores\n    )\nThe heuristic agent provides a quick first pass by analyzing URL density - a simple yet effective way to filter out low-quality documents that are mostly just collections of links:\n\nclass HeuristicQualityAgent:\n    def __call__(self, documents: list[Document]) -> list[Document]:\n        ...\n\n    def __score_document(self, document: Document) -> Document:\n        if len(document.content) == 0:\n            return document.add_quality_score(score=0.0)\n\n        url_based_content = sum(len(url) for url in document.child_urls)\n        url_content_ratio = url_based_content / len(document.content)\n\n        if url_content_ratio >= 0.7:\n            return document.add_quality_score(score=0.0)\n        elif url_content_ratio >= 0.5:\n            return document.add_quality_score(score=0.2)\n        return document\nFor more nuanced evaluation, we define a structured response format to ensure consistent scoring from our LLM:\n\nclass QualityScoreAgent:\n    SYSTEM_PROMPT_TEMPLATE = \"\"\"You are an expert judge tasked with evaluating the quality of a given DOCUMENT.\n\nGuidelines:\n1. Evaluate the DOCUMENT based on generally accepted facts and reliable information.\n2. Evaluate that the DOCUMENT contains relevant information and not only links or error messages.\n3. Check that the DOCUMENT doesn't oversimplify or generalize information in a way that changes its meaning or accuracy.\n\nAnalyze the text thoroughly and assign a quality score between 0 and 1, where:\n- **0.0**: The DOCUMENT is completely irrelevant containing only noise such as links or error messages\n- **0.1 - 0.7**: The DOCUMENT is partially relevant containing some relevant information checking partially guidelines\n- **0.8 - 1.0**: The DOCUMENT is entirely relevant containing all relevant information following the guidelines\n\nIt is crucial that you return only the score in the following JSON format:\n{{\n    \"score\": <your score between 0.0 and 1.0>\n}}\n\nDOCUMENT:\n{document}\n\"\"\"\nThe QualityScoreAgent implements sophisticated batch processing with concurrency control and rate limiting (similar to what you have seen in the crawler class). Calling OpenAI’s API in batch mode can quickly hit its request limits. To find a balance between process time and successfully scoring each document, we first called the API for each document using a shorter wait time between API calls. Next, for each API request that failed, we retry it with a longer wait period:\n\n    async def __get_quality_score_batch(\n        self, documents: list[Document]\n    ) -> list[Document]:\n        scored_documents = await self.__process_batch(documents, await_time_seconds=7)\n        documents_with_scores = [\n            doc for doc in scored_documents if doc.content_quality_score is not None\n        ]\n        documents_without_scores = [\n            doc for doc in scored_documents if doc.content_quality_score is None\n        ]\n\n        # Retry failed documents with increased await time\n        if documents_without_scores:\n            retry_results = await self.__process_batch(\n                documents_without_scores, await_time_seconds=20\n            )\n            documents_with_scores += retry_results\n\n        return scored_documents\nFor each document, we format the input prompt, make the API request, and wait for a given period to avoid API request limits:\n\n    async def __get_quality_score(\n        self,\n        document: Document,\n        semaphore: asyncio.Semaphore | None = None,\n        await_time_seconds: int = 2,\n    ) -> Document | None:\n        async def process_document() -> Document:\n            input_user_prompt = self.SYSTEM_PROMPT_TEMPLATE.format(\n                document=document.content\n            )\n\n            response = await acompletion(\n                model=self.model_id,\n                messages=[\n                    {\"role\": \"user\", \"content\": input_user_prompt},\n                ],\n                stream=False,\n            )\n            await asyncio.sleep(await_time_seconds)  # Rate limiting\n\n            raw_answer = response.choices[0].message.content\n            quality_score = self._parse_model_output(raw_answer)\n            return document.add_quality_score(score=quality_score.score)\nThe last step is to check that the LLM’s output follows our desired format by leveraging a Pydantic class:\n\nclass QualityScoreResponseFormat(BaseModel):\n    \"\"\"Format for quality score responses from the language model.\n\n    Attributes:\n        score: A float between 0.0 and 1.0 representing the quality score.\n    \"\"\"\n\n    score: float\n\n\ndef _parse_model_output(\n        self, answer: str | None\n    ) -> QualityScoreResponseFormat | None:\n        if not answer:\n            return None\n\n        try:\n            dict_content = json.loads(answer)\n            return QualityScoreResponseFormat(\n                score=dict_content[\"score\"],\n            )\n        except Exception:\n            return None\nHere are two important observations we still have to point out:\n\nInstead of directly using OpenAI’s API, we used litellm, a wrapper over multiple popular LLM APIs, such as OpenAI, Antrophic, Cohere, and more. We recommend using them, as they allow you to experiment easily with various providers without touching the code.\n\nTo further optimize the system by reducing costs and the chance of request-limit errors, you can use OpenAI’s batch API. In this way, you can send multiple documents per request.\n\nThe last step is to see how we can load the processed documents to our NoSQL MongoDB.\n\n7. Loading the standardized data to a NoSQL database\nWhen building data pipelines, you often need a reliable way to store and retrieve your processed documents. This type of storage is often known as the data warehouse.\n\nIn our use case, out of simplicity, we used a NoSQL MongoDB database, which is not a data warehouse by the book, but for text data, it gets the job done well.\n\nWe implemented the MongoDBService class to interact with MongoDB. This class provides a generic interface for handling any Python structure that follows a Pydantic model structure.\n\nThe class is designed to be flexible, using Python's generic typing to work with any Pydantic model. This means you can use it to store different types of Python data structures while maintaining type safety and validation:\n\nT = TypeVar(\"T\", bound=BaseModel)\n\nclass MongoDBService(Generic[T]):\n    def __init__(\n        self,\n        model: Type[T],\n        collection_name: str,\n        database_name: str = settings.MONGODB_DATABASE_NAME,\n        mongodb_uri: str = settings.MONGODB_URI,\n    ) -> None:\nThe ingest_documents() method is where the magic happens. It takes your Pydantic models and safely stores them in MongoDB. The method includes validation and proper error handling to ensure your data is stored correctly:\n\n    def ingest_documents(self, documents: list[T]) -> None:\n        try:\n            if not documents or not all(\n                isinstance(doc, BaseModel) for doc in documents\n            ):\n                raise ValueError(\"Documents must be a list of Pycantic models.\")\n\n            dict_documents = [doc.model_dump() for doc in documents]\n\n            # Remove '_id' fields to avoid duplicate key errors\n            for doc in dict_documents:\n                doc.pop(\"_id\", None)\n\n            self.collection.insert_many(dict_documents)\n            logger.debug(f\"Inserted {len(documents)} documents into MongoDB.\")\n        except errors.PyMongoError as e:\n            logger.error(f\"Error inserting documents: {e}\")\n            raise\nThat’s it. We are finally done implementing our data pipeline.\n\nThe last step is to look at how we can run the code.\n\n8. Running the code\nThe best way to set up and run the code is through our GitHub repository, where we have documented everything you need. We will keep these instructions only in our GitHub to avoid having the documentation scattered throughout too many places (which is a pain to maintain and use).\n\nBut to give a sense of the “complexity” of running the code, you have to run ONLY the following commands using Make:\n\nmake local-infrastructure-up    # 1. Spin up the infrastructure\nmake download-notion-dataset    # 2. Download the Notion dataset\nmake etl-pipeline               # 3. Run the ETL pipeline through ZenML\nThat’s all it takes to crawl and compute the quality score for all the documents.\n\nWhile the ETL pipeline is running, you can visualize it on ZenML’s dashboard by typing in your browser: http://127.0.0.1:8237\n\nConclusion\nThis lesson taught you what it takes to build the data pipelines to implement a Second Brain AI assistant.\n\nWe walked you through the architecture of the data pipelines, what it takes to manage it with an MLOps framework such as ZenML, what the Notion data looks like, and what you should consider when crawling.\n\nWe learned how to crawl custom links at scale using Crawl4AI, compute a quality score for all your documents using heuristics and LLMs, and create a snapshot of the data in a NoSQL MongoDB database.\n\nLesson 3 (WIP) will teach you how to generate a high-quality summarization instruction dataset using distillation and how to load it to Hugging Face.\n\n💻 Explore all the lessons and the code in our freely available GitHub repository.\n\nIf you have questions or need clarification, feel free to ask. See you in the next session!\n\nWhenever you’re ready, there are 3 ways we can help you:\nPerks: Exclusive discounts on our recommended learning resources\n\n(live courses, self-paced courses, learning platforms and books).\n\nThe LLM Engineer’s Handbook: Our bestseller book on mastering the art of engineering Large Language Models (LLMs) systems from concept to production.\n\nFree open-source courses: Master production AI with our end-to-end open-source courses, which reflect real-world AI projects, covering everything from system architecture to data collection and deployment.\n\nReferences\nDecodingml. (n.d.). GitHub - decodingml/second-brain-ai-assistant-course. GitHub. https://github.com/decodingml/second-brain-ai-assistant-course\n\nMongoDB: the Developer Data platform. (n.d.). MongoDB. https://www.mongodb.com\n\nPydantic. (n.d.). GitHub - pydantic/pydantic: Data validation using Python type hints. GitHub. https://github.com/pydantic/pydantic\n\nUnclecode. (n.d.). GitHub - unclecode/crawl4ai: 🚀🤖 Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. GitHub. https://github.com/unclecode/crawl4ai\n\nZenML - MLOps framework for infrastructure agnostic ML pipelines. (n.d.). https://zenml.io\n\nCole Medin. (2025, January 13). Turn ANY Website into LLM Knowledge in SECONDS [Video]. YouTube. https://www.youtube.com/watch?v=JWfNLF_g_V0",
      "요약문": "### Data pipelines for AI assistants\n\n**Summary:** This lesson from the open-source course \"Building Your Second Brain AI Assistant Using Agents, LLMs and RAG\" focuses on the importance of data pipelines in developing AI systems, particularly for a personal AI research assistant. It emphasizes that data is the essential foundation of AI and outlines the steps to create data pipelines using tools like MLOps frameworks (ZenML) and data storage solutions (NoSQL MongoDB).\n\nThe course includes several lessons, with Lesson 2 specifically dealing with architecting data pipelines. Key components of the data pipeline include:\n\n1. **Data Collection Pipeline**: Utilizes Notion’s API to programmatically access personal data, extract links, and standardize documents into Markdown format. Processed data is stored in a public S3 bucket for accessibility.\n   \n2. **ETL Pipeline**: Involves Extracting data from S3, Transforming it through crawling and standardization, and Loading it into MongoDB. The crawling process is powered by Crawl4AI, an open-source framework that efficiently scrapes web content.\n\n3. **Quality Scoring**: Implements a two-stage quality assessment using heuristics for quick evaluations and LLM-based assessments for nuanced understanding. Documents are scored between 0 and 1 based on relevance.\n\n4. **Data Storage**: The standardized documents are loaded into a NoSQL MongoDB database, ensuring easy retrieval and processing for subsequent AI tasks.\n\nThe lesson concludes with instructions on running the code and highlights future lessons that will explore generating high-quality datasets and deploying models. The course is designed to provide a comprehensive understanding of building functional data pipelines for AI applications.",
      "분류": "### Data pipelines for AI assistants\n\n**Category:** 도구",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Data pipelines for AI assistants\n\n**Keywords:** Data pipelines, MLOps, ETL, Retrieval-Augmented Generation (RAG), Notion integration"
    },
    {
      "No.": 7,
      "엔드포인트": "TechCrunch",
      "날짜": "2025.02.11",
      "링크": "https://techcrunch.com/2025/02/11/how-musks-97-4b-bid-could-gum-up-openais-for-profit-conversion/",
      "제목": "How Musk’s $97.4B bid could gum up OpenAI’s for-profit conversion",
      "본문": "On Monday, Elon Musk, the world’s richest man, offered to buy the nonprofit that effectively governs OpenAI for $97.4 billion. The unsolicited buyout would be financed by Musk’s AI company, xAI, and a consortium of outside investors, per a letter sent to California and Delaware’s attorneys general.\n\nOpenAI CEO Sam Altman quickly dismissed Musk’s bid, and took it as a chance to publicly dunk on him.\n\n“no thank you, but we will buy Twitter for $9.74 billion if you want,” Altman wrote in a post on X just hours after reports emerged of Musk’s offer for OpenAI. Musk owns X, the social network formerly known as Twitter; he paid roughly $44 billion for it in October 2022. \n\nThe two have a history. Musk is an OpenAI co-founder, and both he and xAI are currently involved in a lawsuit that alleges that OpenAI engaged in anticompetitive behavior, among other things.\n\nBut Altman’s rejection of a $97.4 billion takeover offer is more complicated than just saying “no thanks,” according to corporate governance experts who spoke with TechCrunch.\n\nStalling OpenAI’s nonprofit conversion\nOpenAI CEO Sam Altman\nOpenAI CEO Sam Altman visits “Making Money With Charles Payne” at Fox Business Network Studios on December 04, 2024 in New York City.\nImage Credits:Mike Coppola / Getty Images\nFor background, OpenAI was founded as a nonprofit before transitioning to a “capped-profit” structure in 2019. The nonprofit is the sole controlling shareholder of the capped-profit OpenAI corporation, which retains formal fiduciary responsibility to the nonprofit’s charter. \n\nOpenAI is now in the process of restructuring — this time to a traditional for-profit company, specifically a public benefit corporation — in a bid to raise much more capital. But Musk — who is notorious for drowning his enemies in legal troubles — may have stalled the transition and raised the price of OpenAI’s nonprofit with his bid.\n\nDelaware and California‘s attorneys general have requested more information from the ChatGPT maker about its plans to convert to a for-profit benefit corporation. The situation also forces it to consider outside bids seriously.\n\nOpenAI’s board will almost certainly refuse the bid, but Musk has been setting the stage for future legal and regulatory battles. He’s already attempting to stall OpenAI’s for-profit conversion via an injunction, for instance. The bid appears to be an alternative offer, of sorts. \n\nNow, OpenAI’s board will have to demonstrate that it’s not underselling OpenAI’s nonprofit by handing the nonprofit’s assets, including IP from OpenAI’s proprietary research, to an insider (e.g. Sam Altman) for a steep discount.\n\n“Musk is throwing a spanner into the works,” said Stephen Diamond, a lawyer who represented Musk’s opponents in corporate governance battles at Tesla, in an interview with TechCrunch. “He’s exploiting the fiduciary obligation of the nonprofit board to not undersell the asset. [Musk’s bid] is something OpenAI has to pay attention to.”\n\nOpenAI is said to be gearing up for a funding round that would value its for-profit arm at $260 billion. The Information reports that OpenAI’s nonprofit is slated to get a 25% stake in OpenAI’s for-profit.\n\nWith his bid, Musk has signaled there’s at least one group of investors willing to pay a sizable premium for OpenAI’s nonprofit wing. That puts the board of directors in a tight spot. \n\nGrounds for rejection\nStill, just because Musk threw out an eye-popping offer doesn’t mean that OpenAI’s nonprofit has to accept.\n\nCorporate law gives tremendous authority to incumbent boards to protect against unsolicited takeover bids, according to David Yosifon, a Santa Clara University professor of corporate governance law.\n\nOpenAI could make the case that Musk’s bid is a hostile takeover attempt given that Musk and Altman aren’t the best of friends.\n\nThe company could also argue that Musk’s offer isn’t credible because OpenAI is already in the midst of a corporate restructuring process.\n\nAnother approach OpenAI could take would be challenging Musk on whether he has the funds. As The New York Times notes, Musk’s wealth is largely tied to his Tesla stock, meaning that Musk’s investment partners would have to supply much of the $97.4 billion total.\n\nOpenAI’s board may need to review Musk’s offer to fully asses whether it aligns with the nonprofit’s mission, not just specific financial or strategic goals, according to Scott Curran, the former general counsel to the Clinton Foundation. That means Musk’s offer could be weighed against OpenAI’s mission: “to ensure that artificial general intelligence – AI systems that are generally smarter than humans – benefits all of humanity.”\n\n“When Altman posted that response [on X], that was probably done without legal guidance,” Yosifon said. “It’s not good for a regulator to see that kind of dismissive, knee-jerk tweet.”\n\nRaising the value for OpenAI assets\nThe board is likely to side with Altman. Nearly all the directors joined after Altman was briefly fired, then rehired, by the nonprofit’s board in late 2023. Altman himself is also a board member.\n\nIf nothing else, Musk’s bid may raise the potential market value of the OpenAI nonprofit’s assets. That could force OpenAI to raise more capital than it originally anticipated, and complicate talks with the startup’s existing backers. It could also dilute the value of stakes held by OpenAI investors in the for-profit arm, including major partners such as Microsoft. \n\nThat’s sure to anger Altman, who’s been working with investors for months to determine how to fairly compensate the nonprofit.\n\nThe gist is: OpenAI’s corporate restructuring plans just got more complex.\n\nTechCrunch has an AI-focused newsletter! Sign up here to get it in your inbox every Wednesday.\n\nTopics\n\nAI\nAI\nChatGPT\nElon Musk\nnonprofit\nOpenAI\nsam altman\nStartups",
      "요약문": "### How Musk’s $97.4B bid could gum up OpenAI’s for-profit conversion\n\n**Summary:** Elon Musk has proposed a $97.4 billion buyout of the nonprofit governing OpenAI, backed by his AI company xAI and external investors. OpenAI CEO Sam Altman quickly rejected the offer, humorously suggesting a counter-offer for Twitter. The bid is rooted in Musk's history as an OpenAI co-founder and ongoing legal disputes over anticompetitive behavior allegations against OpenAI. Musk's unsolicited offer complicates OpenAI's transition from a nonprofit to a for-profit public benefit corporation, which is aimed at raising capital. His bid has prompted inquiries from state attorneys general and could incite legal challenges, potentially stalling the conversion process. OpenAI's board is likely to refuse the bid, but they must ensure they are not undervaluing their assets in light of Musk's offer. The board's decision could be further complicated by Musk’s assertion of interest from investors and the need to align the offer with OpenAI's mission to benefit humanity through AI. Ultimately, Musk's bid has introduced complexities into OpenAI's restructuring plans and may increase the valuation of its nonprofit assets, affecting negotiations with existing investors like Microsoft.",
      "분류": "### How Musk’s $97.4B bid could gum up OpenAI’s for-profit conversion\n\n**Category:** 업데이트&트렌드",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### How Musk’s $97.4B bid could gum up OpenAI’s for-profit conversion\n\n**Keywords:** Elon Musk, OpenAI, nonprofit conversion, corporate governance, anticompetitive behavior"
    },
    {
      "No.": 8,
      "엔드포인트": "GPTers",
      "날짜": "2025.01.22",
      "링크": "https://www.gpters.org/nocode/post/issuing-inhouse-certificates-one-bve9DC05JG2LJus",
      "제목": "클릭 한번으로 사내 증명서 발급하기 (1)",
      "본문": "소개\n직원수가 많은 회사에서 아르바이트 하다보니 각종 증명서를 발급하는 담당직원이 매번 각종 정보를 복사하거나 타이핑 하는걸 보면서 조금 편하게 해줄까 하는 생각이 들었습니다. 한번 만들어😁 🥰 두면 다양하게 활용이 가능하겠더군요.\n\n(글쓰는 재주가 없어 정보 위주로 정리할게요)\n\n진행 방법\n사용도구: chatGPT와 구글 시트, 독스, 앱스 스크립트를 사용\n\ngpt에게 구글시트와 독스 문서ID를 포함해주고 스크립트를 요청\n\n\n사본\n앱스 스크립트를 이용하여 아래 작성된 구글 독스와 시트에서 데이터를 입력받아 pdf를 구글 드라이브에 저장하는 자동화를 만들거야\n\n아래 내용을 반영하여 앱스 스크립트를 작성하는 방법을 단계별로 상세하게 설명해줘 \n \n구글 독스 1v-YXJ0R----------------KrH61xKs\n구글 시트 1QFNF-------------------oELQnQ\n\n구글 시트의 a열에서 체크박스에 체크가 입력되면 시트 행에 해당하는 내용을 구글 독스에 반영하여 pdf로 저장되도록 트리거 설정해줘  \n\n구글 독스에는 이름, 생년월일은 표에 입력하고 \n소속란에는 사업부와 부서를 입력\n직급, 입사일자, 제출처를 그대로 입력하고 \n증명서 하단 날짜는 파일 작성날짜를 입력해줘. \n작성시 준비한 시트와 문서는 아래 그림과 같습니다.\n\n한국어 문자가 적힌 스프레드시트\nALT\n\n한국어 비자 신청서 한국어 비자 신청서 한국어 비자 신청서 한국어 비자 신청서 한국어\nALT\n\n스크립트에 있는 구글 시트와 독스 id를 확인하는 방법은 아래와 같습니다.\n\n\n수정단계\n\ngpt 답변을 단계별로 진행하면서 다양한 오류를 경험하고 수정하면서 하나씩 고쳐나갔습니다.\n\n😢 그러나... 계속 되는 에러를 고민하다 알게된 것이 아웃풋인 pdf 파일을 저장할 공간을 지정해주지 않았더군요(혼자 바보라고 외쳤습니다 😮). 구글 드라이브 폴더 정보를 지정해 주어야 해당 폴더에 파일이 저장되더군요. 아래는 폴더 아이디를 확인하는 방법입니다.\n\n한국사이트 스크린샷\nALT\n\n폴더 아이디까지 추가한 후에야 파일이 생성되기 시작하더군요^^;;; 한 시간 넘게 걸쳐 수정을 반복하고, 트리거 조정하고 하면서 조금씩 완성이 되어갔습니다.\n\n아래는 트리거 설정방법입니다\n\n한국어로 된 Google 검색 페이지의 스크린샷\nALT\n\n완료단계\n\n약 두시간에 걸친 작업 끝에 완성된 pdf 파일입니다.\n\n\n홍길동 (1).pdf\n51.18KB\n\n\n마지막으로 완성된 앱스 스크립트 코드 첨부합니다.\n\n\n사본\nconst SHEET_ID = '-----------'; // 구글시트 iD\nconst DOC_TEMPLATE_ID = '--------'; //구글독스 양식 ID\nconst FOLDER_ID = '--------'; // PDF 저장 폴더 ID 주소중 project/.../edit ...부분\n\n\nfunction onEdit(e) {\n  if (!e) {\n    Logger.log('onEdit 함수는 직접 실행되지 않습니다.');\n    return;\n  }\n\n  const sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet();\n  const range = e.range;\n\n  Logger.log(`수정된 셀: ${range.getA1Notation()}`);\n  Logger.log(`수정된 값: ${range.getValue()}`);\n\n  if (range.getColumn() === 1 && range.getValue() === true) {\n    Logger.log('체크박스가 선택되었습니다.');\n    const row = range.getRow();\n    const rowData = sheet.getRange(row, 2, 1, 7).getValues()[0];\n    Logger.log('해당 행 데이터: ' + rowData.join(', '));\n\n    const [name, dob, businessUnit, department, position, joinDate, submitto] = rowData;\n\n    try {\n      const doc = createDocument(name, dob, businessUnit, department, position, joinDate, submitto);\n      const pdf = convertToPDF(doc, name);\n      Logger.log(`PDF 파일 생성 완료: ${pdf.getName()}`);\n    } catch (error) {\n      Logger.log(`오류 발생: ${error.message}`);\n    }\n  }\n}\n\nfunction createDocument(name, dob, businessUnit, department, position, joinDate, submitto) {\n  Logger.log('문서 생성 중...');\n  const template = DriveApp.getFileById(DOC_TEMPLATE_ID);\n  const newDoc = template.makeCopy(`증명서_${name}`, DriveApp.getFolderById(FOLDER_ID)).getId();\n  Logger.log(`복사된 문서 ID: ${newDoc}`);\n  const doc = DocumentApp.openById(newDoc);\n  const body = doc.getBody();\n\n  body.replaceText('{{이름}}', name);\n  body.replaceText('{{생년월일}}', dob);\n  body.replaceText('{{소속}}', `${businessUnit} / ${department}`);\n  body.replaceText('{{직급}}', position);\n  body.replaceText('{{입사일자}}', formatDate(new Date(joinDate)));\n  body.replaceText('{{제출처}}', submitto);\n  body.replaceText('{{작성일자}}', formatDate(new Date()));\n\n  doc.saveAndClose();\n  return doc;\n}\n\nfunction convertToPDF(doc, name) {\n  Logger.log('PDF 변환 중...');\n  const docFile = DriveApp.getFileById(doc.getId());\n  const pdfBlob = docFile.getAs('application/pdf');\n  const folder = DriveApp.getFolderById(FOLDER_ID);\n  const pdfFile = folder.createFile(pdfBlob.setName(`${name}.pdf`));\n  docFile.setTrashed(true);\n  return pdfFile;\n}\n\nfunction formatDate(date) {\n  if (Object.prototype.toString.call(date) === '[object Date]') {\n    return Utilities.formatDate(date, Session.getScriptTimeZone(), 'yyyy-MM-dd');\n  }\n  return date;\n}\n결과와 배운 점\n이전에 클로드를 이용할 때와 비교하면, gpt가 코딩에서는 설명과 에러수정에서 조금 부족한 느낌입니다. 여유되시면 스크립트는 클로드 추천합니다 😄\n\n다음주는 증명서 신청을 받는 구글폼을 추가하고 이후에는 발행되는 증명서를 이메일로 발송하는 부분까지 추가해 보려합니다.",
      "요약문": "### 클릭 한번으로 사내 증명서 발급하기 (1)\n\n**Summary:** 이 글에서는 직원들이 쉽게 증명서를 발급받을 수 있도록 구글 시트와 독스, 앱스 스크립트를 활용한 자동화 과정을 소개한다. 작성자는 직원 수가 많은 회사에서 아르바이트를 하면서 증명서 발급의 불편함을 느끼고, 이를 개선하기 위해 시스템을 구축하기로 결정하였다. \n\n진행 방법으로는, 구글 시트와 독스의 문서 ID를 앱스 스크립트에 포함시켜, 체크박스가 선택될 때 해당 행의 데이터를 구글 독스에 반영하고 PDF로 저장하는 트리거를 설정하는 방식이다. \n\n작성자는 여러 오류를 경험하며 수정 작업을 진행하였고, 파일 저장을 위한 폴더 아이디를 지정해야 한다는 점을 깨달았다. 최종적으로 약 두 시간의 작업 끝에 PDF 파일이 성공적으로 생성되었으며, 완성된 앱스 스크립트 코드도 제공된다. \n\n작성자는 gpt의 코딩 설명과 오류 수정에서 부족함을 느끼며, 다음 단계로 구글 폼을 통해 증명서 신청을 받고, 발행된 증명서를 이메일로 발송하는 기능을 추가할 계획임을 밝혔다.",
      "분류": "### 클릭 한번으로 사내 증명서 발급하기 (1)\n\n**Category:** 도구",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### 클릭 한번으로 사내 증명서 발급하기 (1)\n\n**Keywords:** 앱스 스크립트, 구글 시트, 자동화, PDF 생성, 트리거 설정"
    },
    {
      "No.": 9,
      "엔드포인트": "Non-Brand Data",
      "날짜": "2025.02.07",
      "링크": "https://www.nb-data.com/p/enhance-rag-accuracy-with-corrective?utm_source=post-email-title&publication_id=37262&post_id=156582633&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email",
      "제목": "Enhance RAG Accuracy with Corrective-RAG (CRAG)",
      "본문": "Retrieval-augmented generation (RAG) is a system that combines the data retrieval technique and LLM generation to produce an accurate response. Thus, RAG output will depend on the retrieval and generation parts.\n\nRAG output accuracy depends on data quality, the retrieval module, and the generation model. Often, the crucial part that could make or break the RAG system is the retrieved document itself.\n\nIn the previous article, we discussed a few techniques that could enhance retrieval. We have previously discussed many techniques, which you can read below.\n\nAlthough the retrieval process can still result in irrelevant or erroneous data, the Corrective-RAG (CRAG) framework emerged to help address these limitations.\n\nCorrective-RAG introduces a mechanism for error detection and correction within the retrieval-augmented generation pipeline by identifying inaccurate retrieved documents.\n\nThis article will explore building a simple CRAG that evaluates the retrieved documents. The system will follow the diagram below, and the code is in the repository.\n\nEnhance RAG Accuracy with Corrective-RAG (CRAG)\n\nIntroduction\nAs mentioned above, Corrective-RAG, or CRAG, is a framework for improving RAG results using error detection and correction steps. It was first introduced by Yan et al. (2024) paper, which explores where the lightweight retrieval evaluator can be used to assess the overall quality of retrieved documents and improve the robustness of generation.\n\nOften, these steps are only performed within the retrieval steps to detect the error documents and perform correction steps. However, they can be extended to the generation step as well.\n\nThe technique is based upon a feedback loop that continuously evaluates the quality of retrieved documents and provides evaluation. Basically, we pass the document to the evaluator and perform a correction step if it doesn’t pass the evaluation.\n\nEnhance RAG Accuracy with Corrective-RAG (CRAG)\nThe evaluation step can leverage advanced techniques such as confidence scoring, consistency checks, and contextual validation to detect potential errors.\n\nConfidence scoring is a mechanism for evaluating the reliability or trustworthiness of a retrieved document or generated response by assigning a numerical score.\n\nConsistency checks ensure that the retrieved information and the generated response are logically coherent and free from contradictions.\n\nContextual validation ensures that the retrieved information and generated response are accurate and contextually appropriate.\n\nBut how can we employ the evaluation technique above in the RAG system? There are many ways to do that, but one of the most used is the LLM-as-a-Judge evaluator.\n\nThe previous article taught us how the LLM-as-a-Judge works, but you can refresh the concept by reading the following article.\n\nOf course, you can use a more rigid evaluation metric such as ROUGE, BLEU, or any score metrics that can be used. What is important in the CRAG framework is that there is an evaluation and a corrective step that we perform.\n\nSo, what are the benefits of using the CRAG framework? There are a few notable benefits, including but not limited to:\n\nImproved Accuracy: By detecting and correcting errors in real time, CRAG significantly improves the factual accuracy of generated responses.\n\nBetter contextual response: CRAG ensures the generated responses are accurate and contextually appropriate.\n\nEnhanced User Trust: By delivering more accurate, reliable, and contextually appropriate outputs, CRAG builds greater trust with users.\n\nThose are some benefits you can expect by using the CRAG framework. Now, let’s try to build a simple CRAG framework.\n\nBuilding CRAG\nIn the next step, we will see the CRAG pipeline that evaluates the context of the retrieved document and performs the correction step. Note that for simplicity purposes, we don’t use a feedback loop here to improve the retrieval result; instead, we perform the corrections step by searching the web.\n\nWe will use the one we performed in the following article to build the simple RAG system.\n\n\nNon-Brand Data\nSimple RAG Implementation With Contextual Semantic Search\nHi everyone! Cornellius here, back with another Lite series. This time, we’ll explore the advanced techniques and production methods of Retrieval-Augmented Generation (RAG)—tools that will be helpful for your use cases. I will make it into a long series, so stay tuned…\nRead more\n23 days ago · 11 likes · 2 comments · Cornellius Yudha Wijaya\nLet’s start building the CRAG pipeline by adding an evaluation step. In this step, I will only use a simple LLM-as-a-Judge to evaluate whether the document is relevant to the query. The output will be either “yes” or “no”.\n\ndef grade_document(query, document):\n    #Uses the Gemini model to decide if a document is relevant to the query.\n    prompt = f\"\"\"Query: {query}\nDocument: {document}\nIs this document relevant to the query? Answer with \"yes\" or \"no\".\"\"\"\n    response = completion(\n        model=\"gemini/gemini-1.5-flash\",\n        messages=[{\"content\": prompt, \"role\": \"user\"}],\n        api_key=GEMINI_API_KEY\n    )\n    answer = response['choices'][0]['message']['content'].strip().lower()\n    return \"yes\" if \"yes\" in answer else \"no\"\nFor the correction step, we will only use a simple web search to retrieve additional context using Internet data.\n\ndef supplementary_retrieval(query):\n    #Performs a web search using DuckDuckGo and returns the result as a string.\n    search_tool = DuckDuckGoSearchRun()\n    web_result = search_tool.invoke(query)\n    return web_result\nLastly, we build the CRAG pipeline using all the components we have previously constructed. We have built our simple CRAG pipeline by combining the semantic search, evaluator, and correction.\n\ndef corrective_rag(query, top_k=2):\n    # The main CRAG pipeline:\n    #   1. Retrieve documents using semantic search.\n    #   2. Grade each document using the evaluator.\n    #   3. If no relevant document is found, perform a web search.\n\n    # Step 1: Retrieve documents\n    results = semantic_search(query, top_k=top_k)\n    retrieved_docs = results.get(\"documents\", [])\n    print(\"Initial retrieved documents:\")\n    for doc in retrieved_docs:\n        print(doc)\n\n    # Step 2: Grade each document for relevance (Evaluation step)\n    relevant_docs = []\n    for doc in retrieved_docs:\n        grade = grade_document(query, doc)\n        print(f\"Grading document (first 60 chars): {doc[:60]}... => {grade}\")\n        if grade == \"yes\":\n            relevant_docs.append(doc)\n\n    # Step 3: If no relevant document is found, perform supplementary retrieval (Correction step)\n    if not relevant_docs:\n        print(\"No relevant documents found; performing supplementary retrieval via web search.\")\n        supplementary_doc = supplementary_retrieval(query)\n        relevant_docs.append(supplementary_doc)\n    else:\n        print(\"Using relevant documents from the vector store.\")\n\n    # Ensure all elements in relevant_docs are strings\n    context = \"\\n\".join([\" \".join(doc) if isinstance(doc, list) else doc for doc in relevant_docs])\n\n    # Step 4: Generate final answer using the combined context.\n    final_answer = generate_response(query, context)\n    return final_answer\nYou can then try to test the CRAG pipeline using the following code.\n\nquery = \"What is the insurance for car?\"\nfinal_answer = corrective_rag(query)\nprint(\"Final Answer:\")\nprint(final_answer)\nThe result will depend on the retrieved document and the generation model.\n\nAs mentioned, you can build the CRAG pipeline even further by adding a more strenuous feedback loop and applying it in the generation module.\n\nThat’s all for now! I hope you all learn more about the CRAG framework.\n\nIs there anything else you’d like to discuss? Let’s dive into it together!",
      "요약문": "### Enhance RAG Accuracy with Corrective-RAG (CRAG)\n\n**Summary:** \nRetrieval-augmented generation (RAG) combines data retrieval and large language model (LLM) generation to produce accurate responses, with the output's accuracy relying heavily on the quality of both the retrieved documents and the generation model. The Corrective-RAG (CRAG) framework addresses limitations in RAG by incorporating mechanisms for error detection and correction in the retrieval-augmented generation pipeline. This article introduces CRAG, which employs a feedback loop to continuously assess the quality of retrieved documents, utilizing advanced techniques such as confidence scoring, consistency checks, and contextual validation.\n\nThe framework helps improve accuracy by identifying and correcting errors in real-time, ensuring generated responses are both accurate and contextually appropriate, thereby enhancing user trust. The article outlines building a simple CRAG system, which consists of an evaluation step using an LLM-as-a-Judge to determine the relevance of retrieved documents, followed by a correction step utilizing web searches for additional context when necessary. The CRAG pipeline integrates these components to refine the output based on retrieved information, with a sample code provided for practical implementation.\n\nIn conclusion, the CRAG framework significantly enhances the effectiveness of RAG systems, paving the way for better accuracy and user satisfaction.",
      "분류": "### Enhance RAG Accuracy with Corrective-RAG (CRAG)\n\n**Category:** 모델",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Enhance RAG Accuracy with Corrective-RAG (CRAG)\n\n**Keywords:** Retrieval-Augmented Generation, Corrective-RAG, Error Detection, Document Evaluation, Contextual Validation"
    },
    {
      "No.": 10,
      "엔드포인트": "Marvelous MLOps Substack",
      "날짜": "2025.02.05",
      "링크": "https://marvelousmlops.substack.com/p/unicorn-and-rainbows-the-reality?utm_source=post-email-title&publication_id=1746193&post_id=156526517&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email",
      "제목": "Unicorns and Rainbows: The Reality of Implementing AI in a Corporate",
      "본문": "Unicorns and Rainbows. Is it a metaphor? Is it a reality? Maybe both. Think of an unicorn dancing on top of a radiant rainbow. But, in fact, what does it mean?\n\n\nImage generated by AI\nHumanity has always been drawn to utopia — a perfect, idealized future where all problems are solved. Believing that the world is steadily marching toward this vision is tempting. In the AI landscape, the unicorn (you have noticed the 5th leg, right?) represents the elevated promises, wild imagination, and relentless hype that paint a picture of transformative, almost magical technology.\n\nMarvelous MLOps Substack is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\n\nUpgrade to paid\n\nThe rainbow, however, represents the real world: entire potential but riddled with imperfections, inconsistencies, and systemic barriers.\n\nJust like the stock market, AI has its declines and flows. Everything might seem to skyrocket, but a slight shift — technical debt, regulatory burdens, or enterprise realities — can send it crashing back to earth. The question is not whether AI is a transformative force (there is no doubt it is!) but whether we’re being realistic about its trajectory.\n\nThis article will discuss the reality of using AI in the enterprise environment, address technical debt, bridge knowledge gaps, and understand the herd effect that fuels the AI bubble. We aim to offer a realistic roadmap for businesses navigating the complex AI landscape by critically analyzing these factors.\n\n1. The AI bubble\nWe have been in Data & AI for over 10 years. The AI bubble has never been so big. We have AI everywhere on our laptops, phones, and websites. The CEOs of Nvidia, Microsoft, Meta, and OpenAI are spreading a lot of news about revolutionary AI technology, how AI agents will replace humans, how we will reach AGI soon, and how we will have AI everywhere. We live in an AI bubble, and even though the technology is accurate, it is nontrivial to apply it to actual use cases and drive business value than advertised.\n\nThe technological advancements in the AI field are significant, and the value AI generates is real. However, there are still many gaps that people who try to build AI products see clearly. Two factors contribute to the AI bubble: knowledge and the herd effect. Somehow, they are tangential but different.\n\n2. The Role of Knowledge Gaps\nThe gap between AI insiders and the general public is one of the hinds of the AI bubble. The saying “Knowledge is power” remains valid for AI within its development and implementation context.\n\nPeople who are deeply invested in the development of AI are fully aware of the nuances, challenges, and limitations that come with the implementation of AI-based solutions.\n\nOn the other hand, AI outsiders are constantly awe-struck by the marketing terms associated with AI which presents an entirely different world of possibilities. This knowledge gap enables misconceptions to spread at an alarming rate, therefore making the hype of AI take precedence over the reality of what AI systems can offer.\n\n3. Herd Effect: Fear of Missing Out (FOMO)\nAnother significant factor driving the AI hype is the herd effect or the Fear of Missing Out (FOMO).\n\nAs more companies invest in AI and tout their successes, others feel compelled to follow suit, fearing they’ll fall behind if they don’t adopt AI technologies. This rush often leads to deploying AI solutions without a thorough understanding of their applicability or potential ROI, further inflating the AI bubble. The result is a market saturated with AI buzzwords and solutions that may not deliver the promised transformative impact.\n\nAI models (under AI models, we understand foundation models) are used everywhere, where a standard ML model should be used instead. This adds complexity and decreases reliability.\n\n4. Back to basics\nMost companies can not reliably bring standard machine learning models to production and lack monitoring practices.\n\nDespite what many people think, workflows that include AI models are, on average, more complex to bring into production and monitor — even if you take the simplest scenario without RAG or finetuning involved — just call a 3rd party API.\n\nIn too many cases, we seem to have forgotten the basic principles of machine learning and blindly rely on what that API outputs. This is the danger of AI hype: AI has become accessible to everyone, and many software engineers treat it as just another API call.\n\nWhat could go wrong? The data model has not changed, the code has not changed (and neither has the environment where it gets executed), and the version of the API has not changed. But this is the beauty of machine learning: even if everything in your control has not changed, the model can start performing poorly unexpectedly because data distribution has changed.\n\nThis does not just happen with standard machine learning models, it also happens with AI models — we just have less means to impact that behavior, and prompt finetuning becomes an essential part of the process.\n\n5. Real-world, 2025\nExperts say that 2025 will be the year of AI agents. But is it really true?\n\nWhile the AI hype machine continues to boom, real-world adoption tells a different story. The promise of autonomous AI agents seamlessly operating across enterprises remains largely aspirational. The reality? AI in enterprise is still a work in progress — complex, expensive, and often misaligned with actual business needs.\n\nTake BBVA, the Spanish bank that went all in on OpenAI’s technology. They deployed over 2,900 AI models to enhance productivity, yet integrating them into their existing systems turned out to be a logistical nightmare. AI doesn’t operate in a vacuum; it needs to connect with legacy infrastructure, existing workflows, and strict regulatory requirements. And that’s where reality bites — scaling AI across an enterprise is exponentially harder than rolling out a chatbot.\n\nThe UK government’s attempt to integrate AI into its welfare system faced significant limitations. At least six AI prototypes, designed to enhance staff training, improve job center services, and streamline disability benefit processing, were discontinued due to issues in scalability, reliability, and insufficient testing. Officials acknowledged several “frustrations and false starts,” highlighting the complexities involved in deploying AI within public services.\n\nA study highlighted several obstacles in developing and deploying AI agents within enterprises. Security concerns were identified as a top challenge by leadership (53%) and practitioners (62%). Other significant challenges included data governance, performance issues, and integration complexity. These findings underscore the multifaceted difficulties organizations face in implementing AI agents effectively.\n\nReflecting on these examples, it’s evident that the widespread adoption of AI agents in enterprise settings faces significant limitations. While 2025 may usher in extensive research, proofs of concept (POCs), and minimum viable products (MVPs), the path to full-scale integration remains complex.\n\n6. AI in a corporate environment\nBig companies operate under strict rules, structured workflows, and a constant focus on ROI. Unlike agile startups that can adapt on the fly, large organizations have to deal with complex approval processes, compliance checks, and risk management. All this makes adopting AI a slower process, and the idea of rapid transformation often feels more like a distant dream than something achievable.\n\nChip Huyen references the most common LLM applications in her AI engineering book. Enterprises are risk-averse and prefer to deploy internal-facing applications first. From what we have seen so far, even though there is initial support from the leadership to deploy such applications, not enough funding goes to those projects (and unlikely will) as they do not generate direct business value. We are not saying there is no value — there is, but it is challenging to convince stakeholders.\n\n\nImage reinterpreted from Huyen, C. (2025). AI Engineering: Building Applications with Foundation Models. Available on Amazon\nIn enterprises, the most common use cases with direct business generation are related to customer service (forwarding customers to the right agents/ processes) and reviewing contracts. These use cases have been there for a while, and have historically been NLP-heavy, and AI models helped to improve these projects.\n\nSome companies have tried to use LLMs for recommendations and chatbots, and the world has seen enough failures. Here are some examples:\n\nDPD’s customer-facing chatbot, “Ruby,” was designed to assist customers with their inquiries. However, due to insufficient safeguards, a user was able to provoke the bot into swearing and composing a poem criticizing the company itself. This incident underscores the importance of implementing strict content moderation protocols and regularly updating AI systems to prevent such occurrences.\n\nSimilarly, Pak’nSave’s AI meal planner app, intended to provide innovative recipe suggestions, malfunctioned and recommended a combination of ingredients that would produce chlorine gas, labeling it as an “aromatic water mix.” This highlights the critical need for rigorous testing and validation of AI outputs, especially in applications directly impacting consumer health and safety.\n\nIt feels like not everyone has learned from it, and we regularly see companies launching AI applications without clear business value with poor guardrails, mainly for “marketing purposes”. Let’s hope it will not turn out to be bad marketing, as users will try to make the app do things it is not supposed to do “just for fun”.\n\nThere are exceptions. Some companies created nice LLM-powered recommendations, for example, Zalando. It has well-implemented guardrails and is useful for the customers (it helps to find items that are otherwise hard to find via search). In October 2024, Zalando expanded its AI-powered assistant to all 25 markets, supporting local languages. This expansion aims to provide customers with personalized fashion advice and insights into emerging local trends, thereby enhancing the shopping experience.\n\n7. Areas of attention & conclusions\nThere is great potential to leverage AI in a corporate setting. However, to hope for enterprise adoption, we must consider security gaps, controlled environments, transparency and traceability, and a way to monitor and evaluate AI systems.\n\nIn enterprise ecosystems, AI systems need large volumes of data, including personal and proprietary information. Their role is to enhance workflows and boost efficiency, but they need access to critical systems, which can be considered a security risk. Organizations must focus on preventing unapproved access to data, breaches, and compliance issues.\n\nThreat actors can deploy malware that mimics AI behavior to breach networks, skew decisions, or steal secrets. AI agents act autonomously, making them harder to detect and control. This creates a major challenge: real-time oversight of AI systems.\n\nMonitoring is a persistent issue. Few companies have proper systems in place, and AI’s complexity makes it even harder. Owners must fully understand every decision their AI makes\n\nAI’s transformative potential is undeniable, but the path from hype to reality is complex and challenging. Rather than chasing unicorns and rainbows, organizations must take a grounded, strategic approach — one that prioritizes real business needs, robust security frameworks, and a deep understanding of AI’s limitations. The road ahead is uncertain, but one thing is clear: the way we answer these questions will determine whether AI becomes a lasting force for good or just another passing bubble.",
      "요약문": "### Unicorns and Rainbows: The Reality of Implementing AI in a Corporate\n\n**Summary:** The article explores the dichotomy between the idealized vision of AI as a transformative force (symbolized by a unicorn) and the complex, often flawed reality of its implementation in corporate environments (represented by a rainbow). It discusses the current AI bubble, driven by significant advancements but also by knowledge gaps and the herd effect, which leads to misconceptions and inflated expectations about AI's capabilities. \n\nKey points include the challenges posed by knowledge gaps between AI insiders and outsiders, the Fear of Missing Out (FOMO) that compels companies to adopt AI without understanding its implications, and the complexities of integrating AI into legacy systems. Real-world examples, such as BBVA's struggles with AI deployment and the UK government's failed AI prototypes, illustrate the obstacles organizations face.\n\nThe article also highlights that large corporations must navigate stringent compliance and risk management processes, making rapid AI adoption difficult. While there are successful applications of AI, such as Zalando's personalized recommendations, many AI projects fail to deliver clear business value. The author emphasizes the need for a grounded, strategic approach to AI adoption that prioritizes security, transparency, and a thorough understanding of AI's limitations, stressing that the future of AI in business will depend on addressing these challenges effectively.",
      "분류": "### Unicorns and Rainbows: The Reality of Implementing AI in a Corporate\n\n**Category:** 업데이트&트렌드",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Unicorns and Rainbows: The Reality of Implementing AI in a Corporate\n\n**Keywords:** AI Bubble, Knowledge Gaps, Herd Effect, Enterprise Integration, Security Frameworks"
    },
    {
      "No.": 11,
      "엔드포인트": "ModuLabs",
      "날짜": "2025.02.05",
      "링크": "https://modulabs.co.kr/blog/deepseek-security-caution?utm_source=stibee&utm_medium=moduletter&utm_campaign=blog_curation&utm_content=deepseek_security&utm_term=none",
      "제목": "DeepSeek 보안, 개인정보 처리방침의 숨겨진 위험성 분석",
      "본문": "챗GPT 같은 인공지능 챗봇 등이 대세가 되면서 대다수의 사람들이 인공지능 사용하면서 생활을 하고 있습니다.\n요근래 중국 회사 DeepSeek에서 만든 챗봇도 인기로 자리잡았으며 특히 R1 성능이 OpenAI에서 만드는 o1에 맞먹는 성능을 자랑한다는 결과까지 나왔습니다.\n그런데 혹시 DeepSeek, 꼼꼼하게 따져보고 쓰고 계신가요?\n오늘은 DeepSeek의 개인정보 처리방침을 샅샅이 파헤쳐 보면서, DeepSeek 보안에 문제가 없는지 그리고 사용할때 주의해야 할 부분은 없는지 알아보겠습니다.\n\n중국 기업의 DeepSeek 보안, 내 정보는 어디로?\n\n\nDeepSeek의 가장 큰 특징은, 데이터를 관리하는 회사가 중국에 있다는 점입니다.\n항저우와 베이징에 있는 DeepSeek 법인들이 우리 정보를 쥐고 있어 DeepSeek 보안 문제가 우려됩니다.\n\n일단, 우리가 DeepSeek에 입력하는 모든 정보는 중국 서버에 저장되고, 중국 법의 적용을 받게 되는데 중국은 국가 안보나 공공질서라는 이유로 정부가 데이터에 접근할 수 있는 권한이 엄청나게 넓습니다.\n특히 우리를 비롯한 중국에 살지 않는 외국인들의 DeepSeek 보안이 더욱 취약할 수 있습니다.\n\n회사에서 중요한 사업 전략이나 기술 정보를 DeepSeek으로 이야기했다면 그 내용이 중국 정부 기관에 넘어갈 수도 있습니다.\n또, 중국의 사이버보안법이나 데이터보안법에 따르면, 정부가 중요하다고 판단하는 정보는 즉시 제공해야 할 의무가 있어서, 사용자 입장에서는 내 정보에 대한 권리를 제대로 행사하기 어려울 수 있기 때문에 DeepSeek 보안 측면에서 좋은 선택지라 볼 수 없습니다.\n\n너무 많은 정보를 가져가는 DeepSeek 보안의 현실\nDeepSeek은 개인 정보를 너무 많이 수집한다는 점에서 DeepSeek 보안 정책의 문제점이 드러납니다.\n특히 이런 부분들까지 전부 수집하는 것이 더 큰 문제입니다.\n\n타이핑 습관까지…?:\n키보드를 누르는 패턴이나 리듬 같은 생체 정보를 가져가는데, 이걸 왜 수집하는지, 어디에 쓰는지 명확하게 설명해주지 않습니다.\n이렇게 되면 우리가 키보드를 어떻게 두드리는지 분석해서, 우리 행동 패턴을 알아낼 수도 있습니다.\n\n“사용하는 기능 및 취하는 행동”…?:\n이건 또 무슨 의미일까요? 너무 애매하게 정의된 사용 정보를 수집해서, 우리를 감시하려는 건 아닌지 우려가 됩니다.\n서비스를 제공하는 데 꼭 필요한 정보만 수집해야 하는 원칙에 어긋난다고 볼 수 있습니다.\n\n광고 파트너에게까지…?:\n심지어 광고 파트너나 다른 회사로부터 우리가 DeepSeek 밖에서 뭘 하는지까지 정보를 가져옵니다.\n이건 AI 챗봇 서비스에서 기대하는 수준을 넘어서는 거죠. 사용자의 온라인 활동을 샅샅이 사찰한다는 생각이 들 수 밖에 없습니다,\n\n내 대화 내용까지 수집하는 DeepSeek 보안 문제\ninformation-thief\n\nDeepSeek은 정보를 “서비스 제공 및 관리, 서비스 개선 및 개발, 안전, 보안 및 안정성 유지” 같은 애매한 목적으로 사용한다고만 밝히고 있습니다.\n특히 챗봇은 우리가 대화한 내용을 AI가 학습하는 데 쓸 가능성이 큰데, 이 부분에서 DeepSeek 보안이 취약하다는 것이 큰 문제입니다.\n\nAI가 학습하는 과정에서 이런 위험들이 생길 수 있습니다:\n\n내 이름이나 주소 같은 개인 정보가 AI 모델에 저장돼서, 다른 사람이랑 대화할 때 갑자기 튀어나올 수도 있습니다.\n\n회사 기밀이나 중요한 업무 내용이 모델에 학습돼서 경쟁 회사에 넘어갈 수 있습니다.\n\n개인적인 고민 상담이나 건강 정보가 학습 데이터로 쓰일 수 있습니다.\n\nDeepSeek 보안과 개인정보 공유의 위험성\nDeepSeek은 광고나 분석 파트너와 사용자 정보를 공유할 수 있습니다. 이는 DeepSeek 보안에 심각한 허점을 만들 수 있습니다:\n\n우리가 뭘 좋아하는지, 어떤 행동을 하는지 광고에 활용될 수 있습니다.\n\n민감한 대화 내용이 다른 회사에 넘어가서 광고에 쓰일 수도 있습니다.\n\n개인 정보가 여러 회사에 공유되면서 우리의 모든 정보가 하나로 합쳐질 수도 있습니다.\n\n게다가 중국 법 때문에 법 집행 기관이나 공공 기관에 정보를 줘야 할 의무도 있어서 정보의 안정성까지 담보하지 못합니다.\n\n내 권리는 어디에…? DeepSeek 보안의 사각지대\nDeepSeek은 우리가 사는 지역에 따라서 개인 정보 보호 권리와 DeepSeek 보안 정책을 다르게 적용하고 있어요.\n이건 글로벌 서비스로서는 심각한 문제가 있는 거죠.\n\n유럽은 괜찮고, 한국은…?: 유럽에 사는 사람들은 GDPR 덕분에 개인 정보 보호를 비교적 잘 받을 수 있지만, 아시아나 아프리카처럼 관련 법이 약한 곳에 사는 사람들은 제대로 보호받기 어렵습니다. 중국에 사는 사람들은 중국 법 때문에 더 제약이 많을 거고요.\n\n삭제해달라고 하면 서비스 이용 제한…?: 개인 정보를 삭제하거나 사용을 막아달라고 하면 서비스 이용을 제한할 수 있습니다. 이건 우리 권리를 포기하라는 압박처럼 느껴집니다. 특히 DeepSeek에 의존하는 사람들은 자기 권리를 제대로 행사하기 어려울 것 같습니다.\n\n불명확한 정보 보관 기간, DeepSeek 보안의 또 다른 문제점\nDeepSeek의 데이터 보관 정책에도 문제가 있습니다:\n\n“필요한 기간 동안”?: 얼마나 오랫동안 보관하는지 명확하게 알려주지 않습니다. “합법적인 사업적 이익”이라는 주관적인 기준으로 보관 기간을 결정한다고 하니, 언제 삭제되는지 알 수 없습니다.\n\n오래될수록 위험한 정보: 예전에 나눴던 대화나 민감한 정보가 계속 서버에 남아있을 수도 있습니다. 시간이 지날수록 정보가 유출되거나 잘못 사용될 위험도 커지며 잊혀질 권리를 제대로 행사하기 어려울 수 있습니다.\n\n청소년 보호 측면의 DeepSeek 보안 실태\nDeepSeek은 청소년 보호 정책도 보완해야 할 부분이 많아요:\n\n나이 확인은 어떻게…?: 14세 미만은 사용하면 안 된다고 하지만, 실제로 나이를 확인하는 절차가 없으며 거짓으로 입력해도 쓸 수 있다는겁니다. 부모님 동의를 받는 시스템도 없습니다.\n\n청소년 맞춤 보호는…?: 14~18세 청소년들을 위한 특별한 보호 장치도 부족합니다. 청소년에게 광고를 하거나 정보를 수집하는 걸 막는 규정도 없고, 유해한 콘텐츠를 걸러내는 시스템도 부족해 보입니다.\n\n결론: DeepSeek 보안, 개선이 시급하다\n지금까지 DeepSeek의 개인 정보 처리 방침을 꼼꼼하게 살펴봤으며 여러 면에서 개선해야 할 부분이 많다는 것을 알 수 있습니다.\n특히 중국 법의 적용을 받는다는 점, 너무 많은 정보를 수집한다는 점, 불분명한 정보 활용 목적, 제한적인 사용자 권리 같은 문제들은 시급하게 해결해야 합니다.\n\n사용자입장에서 이 부분을 알지 못한 채로 DeepSeek의 챗봇 시스템의 성능만 보고 이용한다면 문제될 수 있으니.. 꼭 이 부분 명심하고 사용하시길 부탁드립니다.",
      "요약문": "### DeepSeek 보안, 개인정보 처리방침의 숨겨진 위험성 분석\n\n**Summary:** 최근 중국 기업 DeepSeek의 챗봇이 인기를 끌고 있지만, 사용자 개인정보 보호에 관한 심각한 우려가 제기되고 있다. DeepSeek은 중국에 본사를 두고 있으며, 입력한 모든 데이터는 중국 서버에 저장되어 중국 법의 적용을 받는다. 이로 인해 정부가 사용자 정보를 쉽게 접근할 수 있는 상황이 발생할 수 있다. \n\nDeepSeek은 개인 정보를 과도하게 수집하며, 특히 타이핑 습관과 같은 생체 정보를 포함해 사용자 행동을 감시할 가능성이 있다. 또한, 대화 내용이 AI 학습에 쓰일 수 있어 개인적인 정보가 노출될 위험이 있다. 더불어, 광고 파트너와의 정보 공유로 인해 민감한 개인정보가 외부로 유출될 위험도 존재한다.\n\n정보 보호 권리는 지역에 따라 다르게 적용되며, 유럽과 아시아 간의 차이가 크다. 사용자들이 개인정보 삭제를 요청할 경우 서비스 이용에 제한을 받을 수 있으며, 정보 보관 기간에 대한 명확한 규정이 없어 사용자 권리에 대한 불안이 커진다. 청소년 보호 정책도 미비하여, 나이 확인 절차가 없고 청소년을 위한 특별한 보호 장치가 부족하다.\n\n결론적으로, DeepSeek의 개인정보 처리방침은 다수의 문제점을 안고 있으며, 사용자들이 이러한 리스크를 인지하고 사용해야 할 필요성이 있다.",
      "분류": "### DeepSeek 보안, 개인정보 처리방침의 숨겨진 위험성 분석\n\n**Category:** 업데이트&트렌드",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### DeepSeek 보안, 개인정보 처리방침의 숨겨진 위험성 분석\n\n**Keywords:** DeepSeek 보안, 개인정보 처리방침, 중국 법, 데이터 수집, 사용자 권리"
    },
    {
      "No.": 12,
      "엔드포인트": "ByteByteGo Newsletter",
      "날짜": "2025.02.14",
      "링크": "https://blog.bytebytego.com/p/non-functional-requirements-the-backbone?utm_source=post-email-title&publication_id=817132&post_id=157015041&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email",
      "제목": "Non-Functional Requirements: The Backbone of Great Software - Part 1",
      "본문": "Non-functional requirements (NFRs) are as critical as functional requirements because they define a system's qualities and operational parameters.\n\nWhile functional requirements specify what a software product should do (for example, “users must be able to log in”), non-functional requirements define how well it must accomplish these tasks under real-world conditions (for example, “the login process should respond within two seconds under peak load” or “all user credentials must be encrypted and stored securely”).\n\nTogether, functional and non-functional requirements create a foundation for building great software systems. \n\nNFRs are essential for the following reasons:\n\nQuality of Service: NFRs like response time, availability, and usability directly affect the user’s perception of quality. A system that fulfills its functional requirements but is slow, constantly crashes, or is difficult to use can undermine user trust and satisfaction.\n\nSystem Stability: Requirements such as reliability, fault tolerance, and recoverability help maintain stable operation even when part of the system fails. Without these, unhandled errors can escalate into large-scale outages.\n\nSecurity and Compliance: Security-related NFRs dictate how data is protected, how access is controlled, and how audits are conducted. Neglecting these can lead to breaches, legal consequences, or reputational damage.\n\nScalability and Performance: Requirements for throughput, capacity, and resource utilization ensure the software can handle growth in users or data. If not addressed from the start, scaling can become prohibitively expensive or technically challenging later on.\n\nMaintenance and Evolution: Maintainability, testability, and modularity requirements determine how easily bugs can be fixed, features added, or adaptations made to changing environments. Overlooking them can lead to ballooning technical debt, slowing down future development.\n\nIn short, non-functional requirements are not mere “nice-to-haves” but essential components that ensure a software system truly meets user expectations and withstands real-world challenges. \n\nIn this article (Part 1), we’ll look at the differences between functional and non-functional requirements. Then, we’ll explore the various trade-offs in NFRs and their architectural impact on building systems.\n\n",
      "요약문": "### Non-Functional Requirements: The Backbone of Great Software - Part 1\n\n**Summary:** Non-functional requirements (NFRs) are crucial alongside functional requirements, as they define a software system's qualities and operational parameters. While functional requirements outline what a product must do (e.g., user login capabilities), NFRs specify how well these tasks should be performed under real-world conditions (e.g., response times and security measures). \n\nNFRs are vital for several reasons: \n- **Quality of Service:** They impact user perception through metrics like response time and usability, influencing trust and satisfaction.\n- **System Stability:** Reliability and fault tolerance ensure the system operates smoothly even during failures, preventing widespread outages.\n- **Security and Compliance:** They define data protection measures and access controls, crucial for avoiding breaches and legal issues.\n- **Scalability and Performance:** NFRs related to throughput and capacity prepare the software for growth, preventing future technical challenges.\n- **Maintenance and Evolution:** They address maintainability and adaptability, helping manage technical debt and facilitating future development.\n\nOverall, NFRs are essential, not optional, components that ensure software meets user expectations and can handle real-world challenges. This article (Part 1) will further examine the differences between functional and non-functional requirements, as well as the trade-offs involved in NFRs and their impact on system architecture.",
      "분류": "### Non-Functional Requirements: The Backbone of Great Software - Part 1\n\n**Category:** 업데이트&트렌드",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Non-Functional Requirements: The Backbone of Great Software - Part 1\n\n**Keywords:** Non-Functional Requirements, Quality of Service, System Stability, Security and Compliance, Scalability and Performance"
    },
    {
      "No.": 13,
      "엔드포인트": "LLM Watch",
      "날짜": "2025.02.08",
      "링크": "https://www.linkedin.com/pulse/massive-progress-reasoning-models-pascal-biese-abguf/",
      "제목": "Massive Progress in Reasoning Models",
      "본문": "In this issue:\nBeating OpenAI with Open-Source\n99% performance with only 1% data\nChain-of-Associated-Thoughts (CoAT)\n\n\nFor those of you that enjoy the Linkedin version of LLM Watch, that's great, it's why I publish it here. However, I've heard numerous times by now that people weren't even aware of my Substack - or what Substack is, at all. \n\nYou don't have to use the website or the app itself if you don't want to, you can simply subscribe to me there and get all of my updates straight to your e-mail. This includes additional content that I only publish on Substack.\n\n Click here for the full experience\n\n1. s1: Simple test-time scaling\nWatching: s1 (paper/code)\n\n\nWhat problem does it solve? Current approaches to test-time scaling—using additional compute during inference to boost performance—often rely on opaque or complex methodologies, as seen in OpenAI’s proprietary \"o1\" model. This lack of transparency and simplicity hinders reproducibility and practical adoption. The article addresses this gap by proposing an accessible, minimalistic framework for test-time compute scaling, focusing on enhancing model reasoning without intricate architectural changes.\n\nHow does it solve the problem? The authors combined two key innovations: a compact, high-quality dataset (s1K) and a novel \"budget forcing\" mechanism. s1K was curated via rigorous criteria (difficulty, diversity, quality) to maximize fine-tuning efficiency with minimal data. Budget forcing dynamically controls test-time compute by either truncating the model’s reasoning to limit resources or appending \"Wait\" tokens to prompt self-correction, effectively simulating iterative refinement. The approach was applied via supervised fine-tuning on the Qwen2.5-32B-Instruct model.\n\nWhat are the key findings? The model (s1-32B) outperformed OpenAI’s o1-preview by up to 27% on competition math benchmarks (MATH, AIME24) and extrapolated beyond its base performance with budget forcing, improving from 50% to 57% on AIME24. The results demonstrate that controlled test-time compute interventions, even with a small dataset, yield significant gains in reasoning tasks. The open-source release of the model, data, and code further underscores reproducibility.\n\nWhy does it matter? This work democratizes test-time scaling by proving its viability through simple, transparent methods—contrasting with proprietary \"black-box\" approaches. Budget forcing introduces a lightweight, adaptive mechanism to optimize compute use during reasoning, applicable across domains like education or coding. By open-sourcing their framework, the authors enable broader community adoption and innovation, advancing equitable access to high-performance LLM capabilities.\n\n2. LIMO: Less is More for Reasoning\nWatching: LIMO (paper)\n\n\nWhat problem does it solve? The article challenges the prevailing belief that eliciting complex reasoning—especially in mathematical domains—requires an enormous amount of training data (often exceeding 100,000 examples). It questions the long-held assumption that large-scale supervised fine-tuning (SFT) is necessary for fostering sophisticated reasoning skills in large language models (LLMs).\n\nHow does it solve the problem? The authors propose a model called LIMO (Less-Is-More Reasoning) that challenges the conventional wisdom. They demonstrate that complex mathematical reasoning abilities can be effectively elicited in LLMs with surprisingly few examples. By using only 817 carefully curated training samples, LIMO achieves remarkable performance on mathematical reasoning tasks, outperforming previous models trained on significantly larger datasets. Key innovations include iterative distillation of examples emphasizing intermediate cognitive steps \"borrowed\" from human solvers, combined with gradient-aware example pruning to maximize template efficiency.\n\nWhat are the key findings? LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, a substantial improvement from previous SFT-based models' 6.5% and 59.2% respectively, while using only 1% of the training data. LIMO also demonstrates exceptional out-of-distribution generalization, achieving a 40.5% absolute improvement across 10 diverse benchmarks, surpassing models trained on 100x more data. These results challenge the notion that SFT leads to memorization rather than generalization.\n\nWhy does it matter? They fundamentally reframe how we approach specialized reasoning in LLMs - not as data-hungry pattern recognition tasks, but as knowledge-unlocking challenges. This reduces computational costs and democratizes development of specialized models. The discovery also offers new perspectives on AI cognition, suggesting foundation models may possess \"latent reasoning muscles\" that require targeted activation rather than brute-force training. For practitioners, it enables mathematical reasoning deployment scenarios where large labeled datasets are unavailable.\n\n3. CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning\nWatching: CoAT (paper)\n\n\nWhat problem does it solve? Current LLM inference predominantly relies on a \"fast thinking\" approach, where models generate outputs in a single pass without iterative refinement. While effective for many tasks, this methodology lacks mechanisms to dynamically integrate new information or revisit earlier reasoning steps—key aspects of human-like \"slow thinking.\" This limitation becomes pronounced in complex scenarios requiring adaptability, multi-step reasoning, or incorporation of evolving context.\n\nHow does it solve the problem? The authors introduced Chain-of-Associated-Thoughts (CoAT), blending Monte Carlo Tree Search (MCTS) with a dynamic \"associative memory\" system. MCTS enables structured exploration of diverse reasoning pathways similar to human brainstorming, while associative memory acts as a real-time knowledge repository. This combination allows LLMs to iteratively update their reasoning by retrieving and cross-referencing stored insights, mimicking the human ability to pause, reflect, and refine earlier conclusions.\n\nWhat are the key findings? CoAT significantly outperformed conventional inference methods across generative and reasoning tasks, scoring higher in accuracy, coherence, and output diversity. The framework’s iterative refinement—enabled by MCTS-guided exploration and associative memory integration—produced outputs better aligned with complex problem-solving requirements. Notably, the system demonstrated strong context retention even as search spaces expanded dynamically.\n\nWhy does it matter? These results address a critical gap in LLM capabilities: the inability to self-correct or incorporate new insights mid-reasoning. By aligning model reasoning closer to human cognitive processes, the CoAT framework paves the way for developing AI systems that are not only more accurate but also better equipped to handle complex, real-world tasks that demand flexibility and contextual adaptability.\n\nPapers of the Week:\nCache Me If You Must: Adaptive Key-Value Quantization for Large Language Models\nThe Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking\nReward-Guided Speculative Decoding for Efficient LLM Reasoning\nJudge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment\nSETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling\nPheromone-based Learning of Optimal Reasoning Paths\nJackpot! Alignment as a Maximal Lottery\nCan We Predict the Effect of Prompts?\nPartially Rewriting a Transformer in Natural Language\nSymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs\nAnalyze Feature Flow to Enhance Interpretation and Steering in Language Models",
      "요약문": "### Massive Progress in Reasoning Models\n\n**Summary:** This issue discusses significant advancements in reasoning models for large language models (LLMs), focusing on three key innovations:\n\n1. **Simple Test-Time Scaling (s1)**: The article critiques existing opaque methodologies in test-time scaling, exemplified by OpenAI’s o1 model. It proposes a minimalist framework utilizing a compact dataset (s1K) and a \"budget forcing\" mechanism to enhance model reasoning without complex changes. The resulting s1-32B model outperformed OpenAI’s model by up to 27% on math benchmarks and demonstrated that significant improvements could be achieved with controlled compute interventions. This work promotes accessible and transparent approaches, fostering broader community adoption.\n\n2. **LIMO (Less is More for Reasoning)**: This section challenges the conventional belief that extensive training data is necessary for complex reasoning, showcasing the LIMO model. By employing only 817 curated samples, LIMO achieved high performance on mathematical reasoning tasks, significantly outperforming larger dataset models. It illustrates that targeted training can unlock latent reasoning capabilities in LLMs, reducing computational costs and enabling effective deployment in scenarios with limited data.\n\n3. **CoAT (Chain-of-Associated-Thoughts)**: CoAT integrates Monte Carlo Tree Search with associative memory to enhance LLM reasoning. This framework allows for iterative refinement and adaptability in reasoning, mimicking human cognitive processes. CoAT outperformed traditional methods in various tasks, demonstrating improved accuracy and coherence, while addressing the limitation of LLMs in self-correction and contextual adaptability.\n\nThe issue underscores a shift towards democratizing AI capabilities through open-source methodologies and innovative frameworks, ultimately enhancing the reasoning performance of LLMs.",
      "분류": "### Massive Progress in Reasoning Models\n\n**Category:** 모델",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### Massive Progress in Reasoning Models\n\n**Keywords:** Open-Source Models, Test-Time Scaling, Less-Is-More Reasoning, Chain-of-Associated-Thoughts, Iterative Refinement"
    },
    {
      "No.": 14,
      "엔드포인트": "the decoder",
      "날짜": "2025.02.16",
      "링크": "https://the-decoder.com/chatgpt-passes-turing-test-for-psychotherapy-study-says/",
      "제목": "ChatGPT passes Turing test for psychotherapy, study says",
      "본문": "A recent study reveals that people struggle to differentiate between therapeutic responses from ChatGPT and human therapists, with the AI's answers often rated as more empathetic than those from professionals.\n\nThe classic Turing test, developed by computer science pioneer Alan Turing, measures whether humans can identify if they're interacting with a machine or another person. Researchers recently applied this concept to psychotherapy, asking 830 participants to differentiate between responses from ChatGPT and human therapists.\n\nAccording to research published in PLOS Mental Health, participants performed only slightly better than random guessing when trying to identify the source of therapeutic responses. They correctly identified human therapist responses 56.1 percent of the time and ChatGPT responses 51.2 percent of the time. The researchers examined 18 couples therapy case studies, comparing responses from 13 experienced therapists against those generated by ChatGPT.\n\nThe human factor still influences perception\nThe study found that ChatGPT's responses actually outperformed human experts in measures of therapeutic quality, scoring higher in therapeutic alliance, empathy, and cultural competence.\n\nSeveral factors contributed to ChatGPT's strong performance. The AI system consistently produced longer responses with a more positive tone, and used more nouns and adjectives in its answers. These characteristics likely made its responses appear more detailed and empathetic to readers.\n\nThe research uncovered an important bias: when participants believed they were reading AI-generated responses, they rated them lower - regardless of whether humans or ChatGPT actually wrote them. This bias worked both ways: AI-generated responses received their highest ratings when participants incorrectly attributed them to human therapists.\n\nThe researchers acknowledge important limitations in their work. Their study relied on brief, hypothetical therapy scenarios rather than real therapy sessions. They also question whether their findings from couples therapy would apply equally well to individual counseling.\n\nStill, as evidence grows for AI's potential benefits in therapeutic settings and its likely future role in mental health care, the researchers emphasize that mental health professionals need to understand these systems. They stress that responsible clinicians must carefully train and monitor AI models to maintain high standards of care.\n\nGrowing evidence supports AI's therapeutic potential\nThis isn't the first study to demonstrate AI's capabilities in advisory roles. Research from the University of Melbourne and the University of Western Australia found that ChatGPT provided more balanced, comprehensive, and empathetic advice on social dilemmas compared to human advice columnists, with preference rates between 70 and 85 percent.",
      "요약문": "### ChatGPT passes Turing test for psychotherapy, study says\n\n**Summary:** A recent study published in PLOS Mental Health indicates that participants found it difficult to distinguish between therapeutic responses from ChatGPT and human therapists, often rating the AI's responses as more empathetic. In a Turing test application involving 830 participants, human responses were identified correctly 56.1% of the time, while ChatGPT responses were recognized 51.2% of the time. The study analyzed 18 couples therapy case studies, revealing that ChatGPT outperformed human therapists in therapeutic quality metrics like alliance, empathy, and cultural competence.\n\nFactors contributing to ChatGPT's success included longer, more positively toned responses with a richer vocabulary, which enhanced the perceived empathy. A notable bias was identified, where participants rated AI responses lower when they believed them to be AI-generated, and higher when they mistakenly attributed them to human therapists. \n\nThe researchers acknowledged limitations, including that the study used hypothetical scenarios rather than actual therapy sessions, and questioned if findings from couples therapy would be applicable to individual counseling. Despite these limitations, the research supports the growing evidence of AI's potential in mental health care, emphasizing the need for clinicians to understand and carefully monitor AI systems to ensure quality care. Previous research has also shown that ChatGPT provides superior advice on social dilemmas compared to human advisors, with preference rates ranging from 70 to 85 percent.",
      "분류": "### ChatGPT passes Turing test for psychotherapy, study says\n\n**Category:** 논문",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### ChatGPT passes Turing test for psychotherapy, study says\n\n**Keywords:** Turing test, therapeutic responses, empathy, AI bias, mental health care"
    },
    {
      "No.": 15,
      "엔드포인트": "techcrunch",
      "날짜": "2025.02.16",
      "링크": "https://techcrunch.com/2025/02/16/openai-tries-to-uncensor-chatgpt/",
      "제목": "OpenAI tries to ‘uncensor’ ChatGPT",
      "본문": "OpenAI is changing how it trains AI models to explicitly embrace “intellectual freedom … no matter how challenging or controversial a topic may be,” the company says in a new policy.\n\nAs a result, ChatGPT will eventually be able to answer more questions, offer more perspectives, and reduce the number of topics the AI chatbot won’t talk about.\n\nThe changes might be part of OpenAI’s effort to land in the good graces of the new Trump administration, but it also seems to be part of a broader shift in Silicon Valley and what’s considered “AI safety.”\n\nOn Wednesday, OpenAI announced an update to its Model Spec, a 187-page document that lays out how the company trains AI models to behave. In it, OpenAI unveiled a new guiding principle: Do not lie, either by making untrue statements or by omitting important context.\n\nIn a new section called “Seek the truth together,” OpenAI says it wants ChatGPT to not take an editorial stance, even if some users find that morally wrong or offensive. That means ChatGPT will offer multiple perspectives on controversial subjects, all in an effort to be neutral.\n\nFor example, the company says ChatGPT should assert that “Black lives matter,” but also that “all lives matter.” Instead of refusing to answer or picking a side on political issues, OpenAI says it wants ChatGPT to affirm its “love for humanity” generally, then offer context about each movement.\n\n“This principle may be controversial, as it means the assistant may remain neutral on topics some consider morally wrong or offensive,” OpenAI says in the spec. “However, the goal of an AI assistant is to assist humanity, not to shape it.”\n\nThe new Model Spec doesn’t mean that ChatGPT is a total free-for-all now. The chatbot will still refuse to answer certain objectionable questions or respond in a way that supports blatant falsehoods.\n\nThese changes could be seen as a response to conservative criticism about ChatGPT’s safeguards, which have always seemed to skew center-left. However, an OpenAI spokesperson rejects the idea that it was making changes to appease the Trump administration.\n\nInstead, the company says its embrace of intellectual freedom reflects OpenAI’s “long-held belief in giving users more control.”\n\nBut not everyone sees it that way.\n\nConservatives claim AI censorship\n\nTrump’s closest Silicon Valley confidants — including David Sacks, Marc Andreessen, and Elon Musk — have all accused OpenAI of engaging in deliberate AI censorship over the last several months. We wrote in December that Trump’s crew was setting the stage for AI censorship to be a next culture war issue within Silicon Valley.\n\nOf course, OpenAI doesn’t say it engaged in “censorship,” as Trump’s advisers claim. Rather, the company’s CEO, Sam Altman, previously claimed in a post on X that ChatGPT’s bias was an unfortunate “shortcoming” that the company was working to fix, though he noted it would take some time.\n\nAltman made that comment just after a viral tweet circulated in which ChatGPT refused to write a poem praising Trump, though it would perform the action for Joe Biden. Many conservatives pointed to this as an example of AI censorship.\n\nWhile it’s impossible to say whether OpenAI was truly suppressing certain points of view, it’s a sheer fact that AI chatbots lean left across the board.\n\nEven Elon Musk admits xAI’s chatbot is often more politically correct than he’d like. It’s not because Grok was “programmed to be woke” but more likely a reality of training AI on the open internet. \n\nNevertheless, OpenAI now says it’s doubling down on free speech. This week, the company even removed warnings from ChatGPT that tell users when they’ve violated its policies. OpenAI told TechCrunch this was purely a cosmetic change, with no change to the model’s outputs.\n\nThe company seems to want ChatGPT to feel less censored for users.\n\nIt wouldn’t be surprising if OpenAI was also trying to impress the new Trump administration with this policy update, notes former OpenAI policy leader Miles Brundage in a post on X.\n\nTrump has previously targeted Silicon Valley companies, such as Twitter and Meta, for having active content moderation teams that tend to shut out conservative voices.\n\nOpenAI may be trying to get out in front of that. But there’s also a larger shift going on in Silicon Valley and the AI world about the role of content moderation.\n\nGenerating answers to please everyone\n\nNewsrooms, social media platforms, and search companies have historically struggled to deliver information to their audiences in a way that feels objective, accurate, and entertaining.\n\nNow, AI chatbot providers are in the same delivery information business, but arguably with the hardest version of this problem yet: How do they automatically generate answers to any question?\n\nDelivering information about controversial, real-time events is a constantly moving target, and it involves taking editorial stances, even if tech companies don’t like to admit it. Those stances are bound to upset someone, miss some group’s perspective, or give too much air to some political party.\n\nFor example, when OpenAI commits to let ChatGPT represent all perspectives on controversial subjects — including conspiracy theories, racist or antisemitic movements, or geopolitical conflicts — that is inherently an editorial stance.\n\nSome, including OpenAI co-founder John Schulman, argue that it’s the right stance for ChatGPT. The alternative — doing a cost-benefit analysis to determine whether an AI chatbot should answer a user’s question — could “give the platform too much moral authority,” Schulman notes in a post on X.\n\nSchulman isn’t alone. “I think OpenAI is right to push in the direction of more speech,” said Dean Ball, a research fellow at George Mason University’s Mercatus Center, in an interview with TechCrunch. “As AI models become smarter and more vital to the way people learn about the world, these decisions just become more important.”\n\nIn previous years, AI model providers have tried to stop their AI chatbots from answering questions that might lead to “unsafe” answers. Almost every AI company stopped their AI chatbot from answering questions about the 2024 election for U.S. president. This was widely considered a safe and responsible decision at the time.\n\nBut OpenAI’s changes to its Model Spec suggest we may be entering a new era for what “AI safety” really means, in which allowing an AI model to answer anything and everything is considered more responsible than making decisions for users.\n\nBall says this is partially because AI models are just better now. OpenAI has made significant progress on AI model alignment; its latest reasoning models think about the company’s AI safety policy before answering. This allows AI models to give better answers for delicate questions.\n\nOf course, Elon Musk was the first to implement “free speech” into xAI’s Grok chatbot, perhaps before the company was really ready to handle sensitive questions. It still might be too soon for leading AI models, but now, others are embracing the same idea.\n\nShifting values for Silicon Valley\n\nMark Zuckerberg made waves last month by reorienting Meta’s businesses around First Amendment principles. He praised Elon Musk in the process, saying the owner of X took the right approach by using Community Notes — a community-driven content moderation program — to safeguard free speech.\n\nIn practice, both X and Meta ended up dismantling their longstanding trust and safety teams, allowing more controversial posts on their platforms and amplifying conservative voices.\n\nChanges at X may have hurt its relationships with advertisers, but that could have more to do with Musk, who has taken the unusual step of suing some of them for boycotting the platform. Early signs indicate that Meta’s advertisers were unfazed by Zuckerberg’s free speech pivot.\n\nMeanwhile, many tech companies beyond X and Meta have walked back from left-leaning policies that dominated Silicon Valley for the last several decades. Google, Amazon, and Intel have eliminated or scaled back diversity initiatives in the last year.\n\nOpenAI may be reversing course, too. The ChatGPT-maker seems to have recently scrubbed a commitment to diversity, equity, and inclusion from its website.\n\nAs OpenAI embarks on one of the largest American infrastructure projects ever with Stargate, a $500 billion AI datacenter, its relationship with the Trump administration is increasingly important. At the same time, the ChatGPT maker is vying to unseat Google Search as the dominant source of information on the internet.\n\nComing up with the right answers may prove key to both.",
      "요약문": "### OpenAI tries to ‘uncensor’ ChatGPT\n\n**Summary:** OpenAI has announced a shift in its approach to training AI models, emphasizing \"intellectual freedom\" and the ability to discuss controversial topics through its updated Model Spec. This change aims to reduce restrictions on ChatGPT's responses, allowing it to present multiple perspectives on issues without taking an editorial stance. OpenAI maintains that it will still avoid endorsing blatant falsehoods or addressing certain objectionable queries. The motivation behind these changes is partly to address criticisms from conservative circles, who accuse OpenAI of bias and censorship, particularly in light of previous examples where the chatbot appeared to favor one political viewpoint over another. \n\nThe company asserts that its goal is to assist humanity rather than shape it, and it distances itself from claims of engaging in censorship. OpenAI’s decision may also reflect a broader trend in Silicon Valley towards free speech, as seen in other tech companies like Meta, which have shifted away from left-leaning policies. As AI becomes more integral to information dissemination, OpenAI aims to navigate the complexities of providing balanced answers in a politically charged environment. The company's evolving stance on content moderation aligns with its ambitions to compete in the AI landscape, particularly against Google, while fostering a more open dialogue through its chatbot.",
      "분류": "### OpenAI tries to ‘uncensor’ ChatGPT\n\n**Category:** 업데이트&트렌드",
      "키워드 (RAG, DeepSeek)": null,
      "키워드": "### OpenAI tries to ‘uncensor’ ChatGPT\n\n**Keywords:** intellectual freedom, AI safety, content moderation, political neutrality, model alignment"
    }
  ]
}