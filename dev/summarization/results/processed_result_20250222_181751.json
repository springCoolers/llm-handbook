{
  "metadata": {
    "timestamp": "2025-02-22T09:19:01.759106",
    "filename": "handbook-infra-data.csv",
    "total_rows": 33,
    "model": "gpt-4o-mini"
  },
  "results": [
    {
      "No.": 1,
      "end_point": "https://alphasignal.ai/",
      "post_date": "2025.02.06",
      "link": "https://www.youtube.com/watch?v=7xTGNNLPyMI",
      "title": "Deep Dive into LLMs like ChatGPT",
      "content": "This is a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full training stack of how the models are developed, along with mental models of how to think about their \"psychology\", and how to get the best use them in practical applications. I have one \"Intro to LLMs\" video already from ~year ago, but that is just a re-recording of a random talk, so I wanted to loop around and do a lot more comprehensive version.\n\nInstructor\nAndrej was a founding member at OpenAI (2015) and then Sr. Director of AI at Tesla (2017-2022), and is now a founder at Eureka Labs, which is building an AI-native school. His goal in this video is to raise knowledge and understanding of the state of the art in AI, and empower people to effectively use the latest and greatest in their work.\nFind more at https://karpathy.ai/ and https://x.com/karpathy",
      "summary": "### Deep Dive into LLMs like ChatGPT\n\n**Summary:** This content provides an in-depth exploration of Large Language Models (LLMs), specifically focusing on the technology behind ChatGPT and similar AI products. It covers the entire training process for developing these models, offers insights into their operational \"psychology,\" and provides guidance on effectively utilizing them in practical scenarios. The video serves as a comprehensive follow-up to a previous introductory video, aiming to enhance understanding of AI advancements. The instructor, Andrej Karpathy, is a founding member of OpenAI and former Sr. Director of AI at Tesla, currently leading Eureka Labs, an AI-focused educational initiative. His objective is to increase awareness of cutting-edge AI technologies and empower users in their applications. Additional resources are available at his website and social media.",
      "classification": "### Deep Dive into LLMs like ChatGPT\n\n**Category:** Updates & Trends",
      "keyword": "### Deep Dive into LLMs like ChatGPT\n\n**Keywords:** Large Language Models, AI Training Stack, Practical Applications, AI Psychology, Andrej Karpathy"
    },
    {
      "No.": 2,
      "end_point": "https://alphasignal.ai/",
      "post_date": "2025.02.05",
      "link": "https://www.anthropic.com/research/constitutional-classifiers",
      "title": "Constitutional Classifiers: Defending against universal jailbreaks",
      "content": "A new paper from the Anthropic Safeguards Research Team describes a method that defends AI models against universal jailbreaks. A prototype version of the method was robust to thousands of hours of human red teaming for universal jailbreaks, albeit with high overrefusal rates and compute overhead. An updated version achieved similar robustness on synthetic evaluations, and did so with a 0.38% increase in refusal rates and moderate additional compute costs.\n\nWe are currently hosting a temporary live demo version of a Constitutional Classifiers system, and we encourage readers who have experience jailbreaking AI systems to help ‚Äúred team‚Äù it. Find out more below and at the demo website.\n\n*Update 5 February 2025: We are now offering $10,000 to the first person to pass all eight levels of our jailbreaking demo, and $20,000 to the first person to do so with a universal jailbreaking strategy. Full details of the reward and the associated conditions can be found at HackerOne.\n\nLarge language models have extensive safety training to prevent harmful outputs. For example, we train Claude to refuse to respond to user queries involving the production of biological or chemical weapons.\n\nNevertheless, models are still vulnerable to jailbreaks: inputs designed to bypass their safety guardrails and force them to produce harmful responses. Some jailbreaks flood the model with very long prompts; others modify the style of the input, such as uSiNg uNuSuAl cApItALiZaTiOn. Historically, jailbreaks have proved difficult to detect and block: these kinds of attacks were described over 10 years ago, yet to our knowledge there are still no fully robust deep-learning models in production.\n\nWe‚Äôre developing better jailbreak defenses so that we can safely deploy increasingly capable models in the future. Under our Responsible Scaling Policy, we may deploy such models as long as we‚Äôre able to mitigate risks to acceptable levels through appropriate safeguards‚Äîbut jailbreaking lets users bypass these safeguards. In particular, we‚Äôre hopeful that a system defended by Constitutional Classifiers could allow us to mitigate jailbreaking risks for models which have passed the CBRN capability threshold outlined in our Responsible Scaling Policy1.\n\nIn our new paper, we describe a system based on Constitutional Classifiers that guards models against jailbreaks. These Constitutional Classifiers are input and output classifiers trained on synthetically generated data that filter the overwhelming majority of jailbreaks with minimal over-refusals and without incurring a large compute overhead.\n\nResults from human red teaming\nWe ran two main categories of tests to assess the effectiveness of Constitutional Classifiers.\n\nFirst, we developed a prototype version of the system to identify and block specific scientific knowledge related to chemical, biological, radiological, and nuclear harms. We then invited independent jailbreakers to a bug-bounty program in which they were challenged to ‚Äúred team‚Äù the system (i.e., to attempt to break it under experimental conditions to test its robustness).\n\nSpecifically, they were given a list of ten ‚Äúforbidden‚Äù queries, and their task was to use whichever jailbreaking techniques they wanted in order to get one of our current models (in this case, Claude 3.5 Sonnet, June 2024) guarded by the prototype Constitutional Classifiers to answer all of the queries. We only considered it a successful ‚Äúuniversal‚Äù jailbreak if the model provided a detailed answer to all of the queries.\n\n183 active2 participants spent an estimated >3,000 hours over a two-month experimental period attempting to jailbreak the model. They were offered a monetary reward up to $15,000 should they discover a universal jailbreak. Despite the large amount of effort, none of the participants were able to coerce the model to answer all ten forbidden queries with a single jailbreak‚Äîthat is, no universal jailbreak was discovered.\n\nDespite its robustness to jailbreaks, this prototype system had some problems: it refused too many harmless queries and cost a lot of computational resources to run. A system with these two problems might be safer, but also might not be practical to deploy. Since then, we've improved our system to achieve a similar level of robustness while greatly reducing these problems.\n\nResults from automated evaluations\nWe ran a set of automated evaluations where we synthetically generated 10,000 jailbreaking prompts including many of the most-effective attacks on current LLMs, as well as attacks designed to circumvent classifier safeguards. We tried these prompts on a version of Claude 3.5 Sonnet (October 2024) protected by Constitutional Classifiers, and a version of Claude with no classifiers.\n\nUnder baseline conditions, with no defensive classifiers, the jailbreak success rate was 86%‚Äîthat is, Claude itself blocked only 14% of these advanced jailbreak attempts. Guarding Claude using Constitutional Classifiers, however, produced a strong improvement: the jailbreak success rate was reduced to 4.4%, meaning that over 95% of jailbreak attempts were refused.\n\nA graph showing the results for vulnerability to jailbreaks, overrefusals, and compute overhead for the Constitutional Classifiers system versus the base model\nResults from automated evaluations. For all plots, lower is better. (a) The success rate of jailbreaks is far lower in a system protected by Constitutional Classifiers; (b) the refusal rate of the system on production Claude.ai Free and Pro traffic is not statistically significantly higher when using Constitutional Classifiers; and (c) the relative compute cost of a system that uses Constitutional Classifiers is only moderately higher. Error bars represent 95% confidence intervals computed using binomial proportion standard errors under asymptotic normality assumptions.\nAt the same time, our Constitutional Classifiers did not substantially increase the over-refusal rate on harmless user queries: with the classifiers, Claude had a slightly increased refusal rate (by 0.38%), but this was not statistically significant in a random sample of 5,000 conversations. Finally, the compute cost was moderately higher (by 23.7%) than that of the unguarded model. We‚Äôre working on reducing refusals and compute cost even further as we refine the technique.\n\nOverall, our automated analyses found that this updated version of the Constitutional Classifiers system dramatically improved the robustness of the AI model against jailbreaking‚Äîand did so with only minimal additional cost.\n\nHow it works\nConstitutional Classifiers is based on a similar process to Constitutional AI, another technique we have used to align Claude. Both techniques use a constitution: a list of principles to which the model should adhere. In the case of Constitutional Classifiers, the principles define the classes of content that are allowed and disallowed (for example, recipes for mustard are allowed, but recipes for mustard gas are not).\n\nWith the help of Claude, we use this constitution to generate a large number of synthetic prompts and synthetic model completions across all the content classes. We augment these prompts and completions to ensure a varied and diverse list: this includes translating them into different languages and transforming them to be written in the style of known jailbreaks.\n\nA schematic diagram of the how the Constitutional Classifiers system works, from the creation of the constitution through to generating a test set to using the system to guard an LLM\nTraining and implementing Constitutional Classifiers. (a) A constitution is produced specifying harmless and harmful categories; (b) the constitution is used as the basis for the production of many synthetic prompts and completions, which are further augmented (with variations on style and language) and turned into a training set; (c) classifiers trained on this training set are used as model safeguards to detect and block harmful content.\nWe then use these synthetic data to train our input and output classifiers to flag (and block) potentially harmful content according to the given constitution. To help minimize over-refusals (i.e., harmless content incorrectly flagged as harmful), we also train the classifiers on a fixed set of benign queries generated by a contractor.\n\nLimitations\nConstitutional Classifiers may not prevent every universal jailbreak, though we believe that even the small proportion of jailbreaks that make it past our classifiers require far more effort to discover when the safeguards are in use. It‚Äôs also possible that new jailbreaking techniques might be developed in the future that are effective against the system; we therefore recommend using complementary defenses. Nevertheless, the constitution used to train the classifiers can rapidly be adapted to cover novel attacks as they‚Äôre discovered.\n\nThe full paper contains all the details about the Constitutional Classifiers method, and about the classifiers themselves.\n\nConstitutional Classifiers live demo\nWant to try red teaming Claude yourself? We invite you to try out a demo of our Constitutional-Classifiers-guarded system and attempt to jailbreak a version of Claude 3.5 Sonnet that is guarded using our new technique.\n\nAlthough the Constitutional Classifiers technique is flexible and can be adapted to any topic, we chose to focus on queries related to chemical weapons for the demo.\n\nChallenging users to attempt to jailbreak our product serves an important safety purpose: we want to stress-test our system under real-world conditions, beyond the testing we did for our paper. This allows us to gather additional data and improve the robustness of the method prior to deploying this method on our production systems in the future.\n\nThe demo will be live from Feb 3, 2025 to Feb 10, 2025. It includes a feedback form where you can contact us to report any successful jailbreaks as well as information on our Responsible Disclosure Policy, which we ask that participants follow. We‚Äôll announce any successes and the general results of the demo in an update to this post.\n\n*Update 5 February 2025: As noted above, we are now offering a monetary reward for successful jailbreaking of our system. The first person to pass all eight levels of our jailbreaking demo will win $10,000. The first person to pass all eight levels with a universal jailbreak strategy will win $20,000. Full details of the reward and the associated conditions can be found at HackerOne.\n\nAcknowledgements\nWe‚Äôd like to thank HackerOne for supporting our bug-bounty program for red teaming our prototype system. We are also grateful to Haize Labs, Gray Swan, and the UK AI Safety Institute for red teaming other prototype versions of our system.\n\nJoin our team\nIf you‚Äôre interested in working on problems such as jailbreak robustness or on other questions related to model safeguards, we‚Äôre currently recruiting for Research Engineers / Scientists, and we‚Äôd love to see your application.",
      "summary": "### Constitutional Classifiers: Defending against universal jailbreaks\n\n**Summary:** A recent paper by the Anthropic Safeguards Research Team introduces a method for defending AI models against universal jailbreaks known as Constitutional Classifiers. Initial prototypes demonstrated resilience against extensive human testing, though they faced issues with high over-refusal rates and computational overhead. An improved version maintained similar robustness while only slightly increasing refusal rates (0.38%) and incurring moderate additional computational costs (23.7%).\n\nThe paper discusses the vulnerabilities of large language models (LLMs) to jailbreaks‚Äîinputs designed to bypass safety measures‚Äîand describes how Constitutional Classifiers can filter out harmful prompts based on a constitution that defines acceptable and forbidden content. The system underwent two primary testing phases: human red teaming, where 183 participants attempted to jailbreak the model without success, and automated evaluations, which showed a reduction in jailbreak success rates from 86% to 4.4% when using Constitutional Classifiers.\n\nDespite these advancements, the method is not foolproof and may face emerging jailbreak techniques in the future. The team emphasizes the importance of ongoing development and complementary defenses. A live demo of the system is available from February 3 to February 10, 2025, where users can test the system and potentially earn monetary rewards for successful jailbreaking efforts. The team seeks to improve the robustness of the method through real-world stress testing and is open to recruiting new talent in the field.",
      "classification": "### Constitutional Classifiers: Defending against universal jailbreaks\n\n**Category:** Research Paper",
      "keyword": "### Constitutional Classifiers: Defending against universal jailbreaks\n\n**Keywords:** Constitutional Classifiers, Jailbreak Defense, AI Safety, Red Teaming, Synthetic Evaluations"
    },
    {
      "No.": 3,
      "end_point": "https://alphasignal.ai/",
      "post_date": "2024.10.31",
      "link": "https://openai.com/index/introducing-chatgpt-search/",
      "title": "Introducing ChatGPT search",
      "content": "ChatGPT can now search the web in a much better way than before. You can get fast, timely answers with links to relevant web sources, which you would have previously needed to go to a search engine for. This blends the benefits of a natural language interface with the value of up-to-date sports scores, news, stock quotes, and more.\nChatGPT will choose to search the web based on what you ask, or you can manually choose to search by clicking the web search icon.\nSearch will be available at chatgpt.com, as well as on our desktop and mobile apps. All ChatGPT Plus and Team users, as well as SearchGPT waitlist users, will have access today. Enterprise and Edu users will get access in the next few weeks. We‚Äôll roll out to all Free users over the coming months. \nDesigned to get you to a better answer\nGetting useful answers on the web can take a lot of effort. It often requires multiple searches and digging through links to find quality sources and the right information for you.\nNow, chat can get you to a better answer: Ask a question in a more natural, conversational way, and ChatGPT can choose to respond with information from the web. Go deeper with follow-up questions, and ChatGPT will consider the full context of your chat to get a better answer for you.\nWe also partnered with news and data providers to add up-to-date information and new visual designs for categories like weather, stocks, sports, news, and maps.\n‚ÄúChatGPT search promises to better highlight and attribute information from trustworthy news sources, benefiting audiences while expanding the reach of publishers like ourselves who produce premium journalism.‚Äù Pam Wasserstein, President, Vox Media\nGo straight to the source\nChats now include links to sources, such as news articles and blog posts, giving you a way to learn more. Click the Sources button below the response to open a sidebar with the references.\n‚ÄúWe are convinced that AI search will be, in a near future and for the next generations, a primary way to access information, and partnering with OpenAI positions Le Monde at the forefront of this shift. It allows us to test innovations at an early stage while safeguarding journalism‚Äôs core values and integrity.‚Äù Louis Dreyfus, CEO & Publisher of Le Monde\nChatGPT search connects people with original, high-quality content from the web and makes it part of their conversation. By integrating search with a chat interface, users can engage with information in a new way, while content owners gain new opportunities to reach a broader audience. We hope to help users discover publishers and websites, while bringing more choice to search.\n‚ÄúAs AI reshapes the media landscape, Axel Springer‚Äôs partnership with OpenAI opens up tremendous opportunities for innovative advancements. Together, we're driving new business models that ensure journalism remains both trustworthy and profitable.‚Äù Mathias Sanchez, SVP Global Strategic Partnerships Axel Springer SE\nWe collaborated extensively with the news industry and carefully listened to feedback from our global publisher partners, including Associated Press, Axel Springer, Cond√© Nast, Dotdash Meredith, Financial Times, GEDI, Hearst, Le Monde, News Corp, Prisa (El Pa√≠s), Reuters, The Atlantic, Time, and Vox Media. Any website or publisher can choose to appear in ChatGPT search. If you‚Äôd like to share feedback, please email us at publishers-feedback@openai.com\nHow it works and what comes next\nThe search model is a fine-tuned version of GPT-4o, post-trained using novel synthetic data generation techniques, including distilling outputs from OpenAI o1-preview. ChatGPT search leverages third-party search providers, as well as content provided directly by our partners, to provide the information users are looking for. Learn more here.\nThanks to feedback from the SearchGPT prototype, we brought the best of the SearchGPT experience into ChatGPT. We plan to keep improving search, particularly in areas like shopping and travel, and leverage the reasoning capabilities of the OpenAI o1 series to do deeper research. We also plan to bring our new search experience to Advanced Voice and canvas, as well as to Free and logged out users in the future.\nChatGPT Plus and Team users can try it out today at chatgpt.com. They can also download the Chrome extension to search directly via the browser URL bar.",
      "summary": "### Introducing ChatGPT search\n\n**Summary:** ChatGPT has enhanced its web search capabilities, allowing users to obtain quick and relevant answers with links to trusted sources, merging natural language processing with real-time information such as sports scores and news. Users can initiate searches based on their queries, or opt for manual searches via a dedicated icon. This feature is currently available for ChatGPT Plus, Team, and SearchGPT waitlist users, with plans to extend access to Enterprise and Edu users soon, and to all Free users in the coming months. \n\nThe improved search function enables more conversational questions, with follow-ups considered in context for better responses. OpenAI has partnered with various news organizations to include updated information and visual designs in categories like weather and finance. Each chat now includes source links, enhancing user engagement with original content. Notable industry leaders, including Vox Media and Le Monde, support this integration, emphasizing the shift towards AI-driven information access. \n\nThe search model is based on a fine-tuned version of GPT-4o, utilizing both third-party and partner content. Future updates will focus on enhancing functionalities in shopping and travel, with plans to extend the search experience to more users and platforms. ChatGPT Plus and Team users can start using the new search feature immediately at chatgpt.com and through a Chrome extension.",
      "classification": "### Introducing ChatGPT search\n\n**Category:** Updates & Trends",
      "keyword": "### Introducing ChatGPT search\n\n**Keywords:** web search integration, natural language interface, real-time information, publisher partnerships, AI-driven journalism"
    },
    {
      "No.": 4,
      "end_point": "https://alphasignal.ai/",
      "post_date": "2025.02.04",
      "link": "https://huggingface.co/blog/open-deep-research",
      "title": "Open-source DeepResearch ‚Äì Freeing our search agents",
      "content": "TLDR\nYesterday, OpenAI released Deep Research, a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time.\n\nOne of the main results in the blog post is a strong improvement of performances on the General AI Assistants benchmark (GAIA), a benchmark we‚Äôve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on especially challenging ‚Äúlevel 3‚Äù questions that involve multiple steps of reasoning and tool usage (see below for a presentation of GAIA).\n\nDeepResearch is composed of an LLM (which can be selected from the current list of LLMs provided by OpenAI, 4o, o1, o3, etc) and an internal ‚Äúagentic framework‚Äù which guide the LLM to use tools like web search and organize its actions in steps.\n\nWhile powerful LLMs are now freely available in open-source (see e.g. the recent DeepSeek R1 model), OpenAI didn‚Äôt disclose much about the agentic framework underlying Deep Research‚Ä¶\n\nSo we decided to embark on a 24-hour mission to reproduce their results and open-source the needed framework along the way!\n\nThe clock is ticking, let‚Äôs go! ‚è±Ô∏è\n\nTable of Contents\nWhat are Agent frameworks and why they matter?\nThe GAIA benchmark\nBuilding an open Deep Research\nUsing a CodeAgent\nMaking the right tools üõ†Ô∏è\nResults üèÖ\nCommunity reproductions\nMost important next steps\nWhat are Agent frameworks and why they matter?\nAn Agent framework is a layer on top of an LLM to make said LLM execute actions (like browse the web or read PDF documents), and organize its operations in a series of steps. For a quick intro to agents, check this great interview by Andrew Ng and our introduction blog post to the smolagents library. For a more detailed dive in agents you can subscribe to our agents course that starts in just a few days: link here.\n\nAlmost everyone has already experienced how powerful LLMs can be simply by playing with chatbots.. However, what not everyone is aware of yet is that integrating these LLMs into agentic systems can give them real superpowers!\n\nHere is a recent example comparing the performance of a few frontier LLMs with and without an agentic framework (in this case the simple smolagents library) - using an agentic framework bumps performance by up to 60 points!\n\nBenchmarks\n\nIn fact, OpenAI also highlighted in its release blogpost how Deep Research performed dramatically better than standalone LLMs on the knowledge-intensive \"Humanity‚Äôs Last Exam\" benchmark.\n\nSo, what happens when we integrate our current top LLM in an agentic framework, to work toward an open-DeepResearch ?\n\nA quick note: We‚Äôll benchmark our results on the same GAIA challenge but keep in mind that this is a work in progress. DeepResearch is a massive achievement and its open reproduction will take time. In particular, full parity will require improved browser use and interaction like OpenAI Operator is providing, i.e. beyond the current text-only web interaction we explore in this first step.\n\nLet‚Äôs first understand the scope of the challenge: GAIA.\n\nThe GAIA benchmark\nGAIA is arguably the most comprehensive benchmark for agents. Its questions are very difficult and hit on many challenges of LLM-based systems. Here is an example of a hard question:\n\nWhich of the fruits shown in the 2008 painting \"Embroidery from Uzbekistan\" were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film \"The Last Voyage\"? Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o'clock position. Use the plural form of each fruit.\n\nYou can see this question involves several challenges:\n\nAnswering in a constrained format,\nUsing multimodal capabilities (to extract the fruits from the image),\nGathering several pieces of information, some depending on others:\nIdentifying the fruits on the picture\nFinding which ocean liner was used as a floating prop for ‚ÄúThe Last Voyage‚Äù\nFinding the October 1949 breakfast menu for the above ocean liner\nChaining together a problem-solving trajectory in the correct order.\nSolving this requires both high-level planning abilities and rigorous execution, which are two areas where LLMs struggle when used alone.\n\nSo it‚Äôs an excellent test set for agent systems!\n\nOn GAIA‚Äôs public leaderboard, GPT-4 does not even reach 7% on the validation set when used without any agentic setup. On the other side of the spectrum, with Deep Research, OpenAI reached 67.36% score on the validation set, so an order of magnitude better! (Though we don‚Äôt know how they would actually fare on the private test set.)\n\nLet‚Äôs see if we can do better with open source tools!\n\nBuilding an open Deep Research\nUsing a CodeAgent\nThe first improvement over traditional AI agent systems we‚Äôll tackle is to use a so-called ‚Äúcode agent‚Äù. As shown by Wang et al. (2024), letting the agent express its actions in code has several advantages, but most notably that code is specifically designed to express complex sequences of actions.\n\nConsider this example given by Wang et al.:\n\nCode Agent\n\nThis highlights several advantages of using code:\n\nCode actions are much more concise than JSON.\nNeed to run 4 parallel streams of 5 consecutive actions ? In JSON, you would need to generate 20 JSON blobs, each in their separate step; in Code it‚Äôs only 1 step.\nOn average, the paper shows that Code actions require 30% fewer steps than JSON, which amounts to an equivalent reduction in the tokens generated. Since LLM calls are often the dimensioning cost of agent systems, it means your agent system runs are ~30% cheaper.\nCode enables to re-use tools from common libraries\nBetter performance in benchmarks, due to two reasons:\nMore intuitive way to express actions\nExtensive exposure of LLMs to code in training\nThe advantages above were confirmed by our experiments on the agent_reasoning_benchmark.\n\nFrom building smolagents we can also cite a notable additional advantage, which is a better handling of state: this is very useful for multimodal tasks in particular. Need to store this image/audio/other for later use? No problem, just assign it as a variable in your state and you can re-use it 4 steps later if needed. In JSON you would have to let the LLM name it in a dictionary key and trust the LLM will later understand that it can still use it.\n\nMaking the right tools üõ†Ô∏è\nNow we need to provide the agent with the right set of tools.\n\n1. A web browser. While a fully fledged web browser interaction like Operator will be needed to reach full performance, we started with an extremely simple text-based web browser for now for our first proof-of-concept. You can find the code here\n\n2. A simple text inspector, to be able to read a bunch of text file format, find it here.\n\nThese tools were taken from the excellent Magentic-One agent by Microsoft Research, kudos to them! We didn‚Äôt change them much, as our goal was to get as high a performance as we can with the lowest complexity possible.\n\nHere is a short roadmap of improvements which we feel would really improve these tools‚Äô performance (feel free to open a PR and contribute!):\n\nextending the number of file formats which can be read.\nproposing a more fine-grained handling of files.\nreplacing the web browser with a vision-based one, which we‚Äôve started doing here.\nResults üèÖ\nIn our 24h+ reproduction sprint, we‚Äôve already seen steady improvements in the performance of our agent on GAIA!\n\nWe‚Äôve quickly gone up from the previous SoTA with an open framework, around 46% for Magentic-One, to our current performance of 55.15% on the validation set.\n\nThis bump in performance is due mostly to letting our agents write their actions in code! Indeed, when switching to a standard agent that writes actions in JSON instead of code, performance of the same setup is instantly degraded to 33% average on the validation set.\n\nHere is the final agentic system.\n\nWe‚Äôve set up a live demo here for you to try it out!\n\n\nHowever, this is only the beginning, and there are a lot of things to improve! Our open tools can be made better, the smolagents framework can also be tuned, and we‚Äôd love to explore the performance of better open models to support the agent.\n\nWe welcome the community to come join us in this endeavour, so we can leverage the power of open research together to build a great open-source agentic framework! It would allow anyone to run a DeepResearch-like agent at home, with their favorite models, using a completely local and customized approach!\n\nCommunity Reproductions\nWhile we were working on this and focusing on GAIA, other great open implementations of Deep Research emerged from the community, specifically from\n\ndzhng,\nassafelovic,\nnickscamara,\njina-ai and\nmshumer.\nEach of these implementations use different libraries for indexing data, browsing the web and querying LLMs. In this project, we would like to reproduce the benchmarks presented by OpenAI (pass@1 average score), benchmark and document our findings with switching to open LLMs (like DeepSeek R1), using vision LMs, benchmark traditional tool calling against code-native agents.\n\nMost important next steps\nOpenAI‚Äôs Deep Research is probably boosted by the excellent web browser that they introduced with Operator.\n\nSo we‚Äôre tackling that next! In a more general problem: we‚Äôre going to build GUI agents, i.e. ‚Äúagents that view your screen and can act directly with mouse & keyboard‚Äù. If you‚Äôre excited about this project, and want to help everyone get access to such cool capabilities through open source, we‚Äôd love to get your contribution!\n\nWe‚Äôre also hiring a full time engineer to help us work on this and more, apply if you‚Äôre interested üôÇ\n\nTo get started with Open Deep Research, try the examples here.\nCheck the smolagents repo.\nRead more about smolagents docs, introduction blog post.",
      "summary": "### Open-source DeepResearch ‚Äì Freeing our search agents\n\n**Summary:** OpenAI recently launched Deep Research, a web-browsing system designed to summarize content and answer questions effectively. It achieved notable success on the General AI Assistants benchmark (GAIA), scoring 67% on average for 1-shot questions and 47.6% on complex \"level 3\" questions. Deep Research combines a large language model (LLM) and an \"agentic framework\" that allows the LLM to utilize tools like web searches in a structured manner.\n\nThe blog post emphasizes the importance of agent frameworks, which enhance LLM performance by allowing them to execute actions and organize operations efficiently. For instance, using an agentic framework can significantly boost performance on benchmarks, as seen with Deep Research outperforming standalone LLMs.\n\nThe GAIA benchmark is highlighted as a rigorous test for agent systems, posing difficult questions that require high-level reasoning and execution. OpenAI's Deep Research achieved a remarkable score on GAIA, demonstrating the potential of integrating LLMs with agent frameworks.\n\nThe authors are aiming to replicate OpenAI's results with an open-source project, focusing on improvements like using a \"code agent\" for better action expression and performance. They have made initial progress, achieving a validation score of 55.15% on GAIA using their framework and are inviting community participation to enhance the project further.\n\nNext steps involve developing GUI agents that can interact directly with user interfaces and improving web browsing capabilities, with a call for contributions from the community and a hiring notice for a full-time engineer to support the initiative.",
      "classification": "### Open-source DeepResearch ‚Äì Freeing our search agents\n\n**Category:** Tool",
      "keyword": "### Open-source DeepResearch ‚Äì Freeing our search agents\n\n**Keywords:** Deep Research, Agent Framework, GAIA Benchmark, Code Agent, Open-source LLMs"
    },
    {
      "No.": 5,
      "end_point": "https://decodingml.substack.com/",
      "post_date": "2025.02.06",
      "link": "https://decodingml.substack.com/p/build-your-second-brain-ai-assistant?utm_source=substack&utm_medium=email",
      "title": "Build your Second Brain AI assistant",
      "content": "The first lesson of the open-source course ‚ÄúBuilding Your Second Brain AI Assistant Using LLMs and RAG‚Äù ‚Äî a free course that will teach you how to design and build a Notion-like AI assistant that talks to digital notes and resources.\n\nLessons:\nLesson 1: Build your Second Brain AI assistant\n\nLesson 2: Data pipelines for building AI assistants (WIP)\n\nLesson 3: Generate high-quality fine-tuning datasets (WIP)\n\nLesson 4: Fine-tune and deploy open-source LLMs (WIP)\n\nLesson 5: RAG feature pipelines for building AI assistants (WIP)\n\nLesson 6: Agents and LLMOps (WIP)\n\nüîó Learn more about the course and its outline.\n\nBuild your Second Brain AI assistant\nWelcome to Decoding ML‚Äôs ‚ÄúBuilding Your Second Brain AI Assistant Using LLMs and RAG‚Äù open-source course, where you will learn to architect and build a production-ready Notion-like AI assistant:\n\nIntuitively building a Second Brain AI assistant is like having access to the collective wisdom of your own mind. Sounds fantastic, right? More on this later.\n\nTo build the Second Brain AI assistant, we must implement an LLM application using agents, advanced Retrieval-augmented generation (RAG), fine-tuning, LLMOps and AI systems techniques.\n\nThe best way to learn something new is by doing. Thus, within this course, you will learn these concepts and how to apply them while building your Second Brain AI assistant, which you can later customize.\n\nThis lesson will present what you will build and learn throughout the course.\n\nNext, we will explore the system architecture of the Second Brain AI assistant illustrated in Figure 1. We will explain each component's role, what it is, what algorithms and tools we used, and, most importantly, why we used them.\n\nBy the end of this lesson, you will have a strong intuition of what it takes to architect an AI assistant for your Second Brain, such as a Notion-like AI assistant that allows you to chat with your notes and resources.\n\n\nFigure 1: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.\nWe will zoom in on each component in future lessons and present the theory and implementation behind each ML pipeline. Thus, by the end of this course, you will learn how to build your own AI assistant.\n\nLLM systems have the same fundamental blocks. Hence, after going through our lessons, you will have the necessary knowledge to build your production LLM apps on your favorite use cases.\n\nMeanwhile, here is a quick and fun podcast-like audio that will walk you through what you will build and learn through the course (at 1.5x speed, it‚Äôs super helpful):\n\nLet‚Äôs get started. Enjoy!\n\nTable of Contents:\nWhat are we going to build?\n\nWhat are we going to learn?\n\nIntroducing the custom Notion data source\n\nExploring the flow of the data\n\nPresenting the feature/training/inference (FTI) architecture\n\nArchitecting our Second Brain AI assistant\n\nOffline vs. online ML pipelines\n\nRunning the code\n\n1. What are we going to build?\nAs the first lesson of the open-source six-lesson course ‚ÄúBuilding Your Second Brain AI Assistant Using LLMs and RAG,‚Äù we must clearly define our ultimate goal, including what we will build and learn throughout the process.\n\nDefining our end goal or target is critical whether you're doing a course or implementing a real-world application.\n\nIn this course, we will show you how to build a custom AI assistant on top of your notes, lists or other resources that you usually store in apps such as Notion, Apple Notes, Google Keep, Evernote, Obsidian or similar applications.\n\nThe productivity geeks (such as myself) like to call the system that captures all your thoughts, tasks, meetings, events and notes ‚Äúyour Second Brain.‚Äù Usually, a Second Brain is more than just a note-taking app. It includes tools such as a calendar for meetings and cloud storage for, well‚Ä¶, storage.\n\nFor the sake of simplicity, we will narrow down our problem to building an AI assistant on top of our Notion custom data sources, imitating Notion‚Äôs AI features, as seen in Figure 2. Another similar example is NotebookLM, where you provide a set of sources, and the AI generates answers only based on them.\n\n\nFigure 2: Screenshot of asking the Notion AI assistant, ‚ÄúHow can I optimize LLMs during inference?‚Äù\nSo‚Ä¶ What will we build?\n\nAn AI assistant that generates answers solely based on our digital knowledge stored in our Second Brain, which, in our case, will be the data we store in Notion.\n\nAs we said in the beginning, it‚Äôs like having access to the collective wisdom of your own mind.\n\nThis is a relevant use case for avoiding hallucinations, as you limit the domain to your resources. You can easily control the generation and evaluation steps by conditioning the LLM to your resources.\n\nAs a fun (and relevant) example, we will use our list of filtered resources (which we keep in Notion) on AI and ML, such as GenAI, LLMs, RAG, MLOps, LLMOps and information retrieval.\n\nEveryone has one of those, right?\n\nThe thing is that it gets hard to access exactly what you need when you need it.\n\nThus, we will show you how to hook a GenAI system on top of your research and resources to ask questions, retrieve relevant resources and synthesize information solely based on your research, which you already know is valuable and useful.\n\n2. What are we going to learn?\nFollowing Decoding ML‚Äôs mission, we will show you how to build an end-to-end AI system using the Second Brain AI Assistant as an example.\n\nThus, we will walk you through how to design such a system with production in mind.\n\nThen, we will show you how to implement it, starting with collecting data from Notion, preprocessing and storing it, until using it to fine-tune LLMs and build an agentic RAG application.\n\nAs this is an educational project, we tried to avoid using frameworks such as LangChain and build everything from scratch. Doing so will help you develop your intuition, making using LLM frameworks a breeze.\n\nStill, extensibility is a real pain when using LLM frameworks such as LangChain. Thus, real-world skills include extending these frameworks using object-oriented programming (OOP).\n\nThat‚Äôs why we used LangChain to load and retrieve data from a MongoDB vector database while showing you how to extend its components and add your app‚Äôs custom implementation.\n\nThus, in this course, we will cover the following concepts, algorithms and tools:\n\nArchitecting an AI system using the FTI architecture.\n\nUsing MLOps best practices such as data registries, model registries, and experiment trackers.\n\nCrawling over 700 links and normalizing everything into Markdown using Crawl4AI.\n\nComputing quality scores using LLMs.\n\nGenerating summarization datasets using distillation.\n\nFine-tuning a Llama model using Unsloth and Comet.\n\nDeploying the Llama model as an inference endpoint to Hugging Face serverless Dedicated Endpoints.\n\nImplement advanced RAG algorithms using contextual retrieval, hybrid search and MongoDB vector search.\n\nBuild an agent that uses multiple tools using Hugging Face‚Äôs smolagents framework.\n\nUsing LLMOps best practices such as prompt monitoring and RAG evaluation using Opik.\n\nIntegrate pipeline orchestration, artifact and metadata tracking using ZenML.\n\nManage the Python project using uv and ruff.\n\nApply software engineering best practices.\n\nExcited?\n\nLet‚Äôs start by exploring our custom Notion data source in more depth.\n\n3. Introducing the custom Notion data source\nYou know what everyone says: ‚ÄúEvery successful AI/ML project starts with understanding your data.‚Äù\n\nOur use case is not different. It all starts with understanding our custom Notion data source.\n\nIf you are unfamiliar with Notion, you must know that it‚Äôs a fancier note-taking app that allows you to create notes, tasks, wikis and databases.\n\nFigure 3 shows our eight Notion databases, which contain various resources on topics such as GenAI, information retrieval, MLOps and system design.\n\nWe use these notes, resources, and research to build AI and software products: ‚ÄúYes, it‚Äôs the actual database we reference while building.‚Äù\n\n\nFigure 3: Our Notion databases\nLet‚Äôs dig into a specific database.\n\nFigure 4 shows the ‚ÄúGenerative AI‚Äù Notion database, which contains ~25 data entries on different topics in the space. The ‚ÄúNode‚Äù type contains high-level links, such as blogs, benchmarks, or awesome lists, while a ‚ÄúLeaf‚Äù contains super-specific resources or tools.\n\n\nFigure 4: The ‚ÄúGenerative AI‚Äù Notion database.\nLet‚Äôs open a data entry.\n\nAs seen in Figure 5, we can see that a data entry contains:\n\nMultiple ‚ÄúNotes‚Äù pages containing my thoughts on various topics.\n\nMultiple ‚ÄúToggle‚Äù elements contain links to various blogs, articles or tools.\n\n\nFigure 5: The ‚ÄúLLM Inference Optimization & Other Techniques‚Äù Notion database entry\nWe integrated with Notion‚Äôs API and automatically downloaded and parsed all these documents in Markdown format.\n\nThe fun and interesting thing about this problem is that the data contains relevant links that must be crawled and further processed into Markdown. But, the catch is that the links are mixed with insightful notes we want to keep and feed into our system. Thus, we must find a way to differentiate between documents that contain only links for crawling and valuable documents by themselves.\n\nA database entry will not always look like the one from Figure 5. The data is noisy and can have any form. The only rule is that it contains links, text, images, and attached documents (similar to a real-world use case).\n\nWe will stick to links and text for this course, but it can be extended to processing images and documents (a fun exercise for you).\n\nFor ease of use, we stored a snapshot of the Notion data from above in a public S3 bucket, which you can download without AWS credentials.\n\nThus, you don‚Äôt need to use Notion or hook your Notion to complete this course. But if you want to, you can, as we expose in the GitHub repository, a flexible pipeline that can load any Notion database.\n\nThe next step is to explore the data flow required to build your Second Brain AI assistant, from Notion to fine-tuning and RAG.\n\n4. Exploring the flow of the data\nThe first step to understanding how our AI system looks is to understand its data flow, abstracting away other details such as tooling, infrastructure or algorithms.\n\nOur goal is to collect data for Retrieval-Augmented Generation (RAG). Thus, we can feed our custom data as context to an LLM. We also need to collect data to fine-tune an open-source LLM (such as Llama 3.1 8B) to specialize in summarization (you will soon understand ‚Äúwhy summarization‚Äù).\n\nIf you are not familiar with how a naive Retrieval Augmented Generation (RAG) system works, read more about it in Decoding ML:\n\nRetrieval-Augmented Generation (RAG) Fundamentals First\nRetrieval-Augmented Generation (RAG) Fundamentals First\nPaul Iusztin\n¬∑\n2024ÎÖÑ 8Ïõî 31Ïùº\nRead full story\nAs illustrated in Figure 6, let‚Äôs walk you through the flow and lifecycle of your data:\n\nWe collect raw Notion documents in Markdown format.\n\nWe crawl each link in the Notion documents and normalize them in Markdown.\n\nWe store a snapshot of the data in a NoSQL database.\n\nFor fine-tuning, we filter the documents more strictly to narrow the data to only high-quality samples.\n\nWe use the high-quality samples to distillate a summarization instruction dataset, which we store in a data registry.\n\nUsing the generated dataset, we fine-tune an open-source LLM, which we save in a model registry.\n\nIn parallel, we use a different filter threshold for RAG to narrow down the documents to medium to high-quality samples (for RAG, we can work with more noise).\n\nWe chunk, embed, plus other advanced RAG preprocessing steps to optimize the retrieval of the documents.\n\nWe load the embedded chunks and their metadata in a vector database.\n\nLeveraging the vector database, we use semantic search to retrieve the top K most relevant chunks relative to a user query.\n\n\nFigure 6: The data flow of building a RAG system and fine-tuning LLMs\nIf something doesn‚Äôt make sense, don‚Äôt worry. Throughout the course, we will zoom in on each component and explain why and how we did everything.\n\nNow that we understand how the data flows, let's quickly examine the feature/training/inference (FTI) design we will use to build our Notion AI assistant.\n\n5. Presenting the feature/training/inference (FTI) architecture\nThe pattern suggests that any AI/ML system can be boiled down to these three pipelines: feature, training, and inference.\n\nJim Dowling, CEO and Co-Founder of Hopsworks, introduced the pattern to simplify building production ML systems.\n\nThe feature pipelines take raw data as input and output features and labels to train our model(s).\n\nThe training pipeline takes the features and labels from the feature stored as input and outputs our trained model(s).\n\nThe inference pipeline inputs the features and labels from the feature store and the trained model(s) from the model registry. Using these two components, we can make predictions in batch or real-time mode and serve them to the client.\n\n\nFigure 7: The feature/training/inference (FTI) architecture\nNote that larger ML systems will have more than three pipelines. Thus, by convention, we name each pipeline based on its output artifact. That‚Äôs how we decide whether it‚Äôs a feature, training or inference pipeline.\n\nTo conclude, the most important thing you must remember about the FTI pipelines is their interface:\n\nThe feature pipeline takes in data and outputs features & labels saved to the feature store.\n\nThe training pipelines query the features store for features & labels and output a model to the model registry.\n\nThe inference pipeline uses the features from the feature store and the model from the model registry to make predictions.\n\nIt doesn‚Äôt matter how complex your ML system gets. These interfaces will remain the same.\n\nThere is a lot more to the FTI architecture. To learn more, consider reading the following article from Decoding ML:\n\nBuilding ML systems the right way using the FTI architecture\nBuilding ML systems the right way using the FTI architecture\nPaul Iusztin\n¬∑\n2024ÎÖÑ 8Ïõî 10Ïùº\nRead full story\n6. Architecting our Second Brain AI assistant\nNow that we have laid out all the foundations, such as understanding the data flow involved in building an AI assistant and the FTI architecture, let‚Äôs design our AI system.\n\nAs seen in Figure 8, we have 5 significant components that we have to understand:\n\nThe data pipelines\n\nThe feature pipelines\n\nThe training pipeline\n\nThe inference pipelines\n\nThe observability pipeline\n\nYou might wonder why we have five pipelines instead of three, as the FTI architecture suggests.\n\nThe data engineering team often owns the data pipeline, which prepares the data required to build AI systems.\n\nMeanwhile, the observability pipeline is implemented on top of the FTI architecture to monitor and evaluate the system. Having eyes and ears all over your system is critical for success, especially in LLM systems, which are highly non-deterministic.\n\nAn ideal strategy is to implement an end-to-end workflow of your app quickly, plus the observability pipeline. Then, you use the metrics from the evaluation pipeline and logs from the prompt monitoring pipeline as clear signals on what works and what doesn‚Äôt.\n\n\nFigure 8: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.\nLet‚Äôs zoom in on each.\n\nThe data pipelines\nAs the name suggests, the data pipelines collect data from Notion, clean it, standardize it, and load it to a NoSQL database as our clean and normalized data snapshot before feeding it into the feature pipelines.\n\nBy storing the standardized and cleaned data in a NoSQL database, such as MongoDB, we collect the data once (as a backup), which we can use to experiment and build our AI system.\n\nIn larger AI systems, this is usually done with a data warehouse or lake, but a NoSQL database is lighter and does the job for us.\n\nThe data pipeline is split into two significant components.\n\nThe data collection pipeline: It uses Notion‚Äôs API to retrieve the data programmatically, where each Notion data entry is standardized to Markdown format and saved as JSON along with necessary metadata. To decouple the data collection step from the rest of the system, we save everything to a public S3 bucket to avoid giving public access to our Notion workspace.\n\nThe ETL pipeline: It extracts the raw Notion documents from S3, finds all the embedded links within the documents, crawls them, and standardizes them into Markdown format. It also computes a quality score/document using LLMs. Ultimately, it saves all the documents and metadata into a MongoDB NoSQL database.\n\nThe feature pipelines\nThe feature pipelines leverage the standardized and clean data from the MongoDB NoSQL database for two things:\n\nTo populate a MongoDB vector database for doing RAG.\n\nTo generate a summarization instruct dataset for fine-tuning an LLM.\n\nThese are two standard feature pipelines you will see in the GenAI world.\n\nWithin the RAG feature pipeline, we will implement advanced pre-retrieval RAG techniques, such as Contextual Retrieval, proposed by Antrophic. To implement it, we will require a summarization LLM and hybrid search.\n\nUltimately, we chunk the documents, embed them, and loaded them into a MongoDB vector database.\n\nFor more theory on advanced RAG and how a naive RAG system can be optimized, you can read our article from Decoding ML:\n\nYour RAG is wrong: Here's how to fix it\nYour RAG is wrong: Here's how to fix it\nPaul Iusztin\n¬∑\n1Ïõî 2Ïùº\nRead full story\nUsing APIs such as OpenAI for summarization can get costly (which we need for Contextual Retrieval), so we fine-tune a summarization open-source LLM. To do this, we require a custom summarization dataset.\n\nThus, we will leverage distillation techniques for the dataset generation pipeline to create a high-quality summarization instruction dataset based on our documents.\n\nWe will save the generated dataset to Hugging Face‚Äôs data registry. As an example, you can check our generated dataset: pauliusztin/second_brain_course_summarization_task\n\nThe training pipeline\nThe training pipeline reads the instruct dataset from the data registry and uses Unsloth to fine-tune a Llama 3.1 8B LLM. We use Comet to log the metrics and hyperparameters between multiple experiments, compare them, and pick the best one.\n\nAfter deciding on the best model, we load it into Hugging Face‚Äôs model registry. As an example, you can check our fine-tuned LLM: pauliusztin/Meta-Llama-3.1-8B-Instruct-Second-Brain-Summarization\n\nZenML orchestrates and manages the data, feature, and training pipelines, helping us run the pipelines with a clearly defined structure and configuration. As illustrated in Figure 9, we can track the progress, status and history of each pipeline in a beautiful UI.\n\n\nFigure 9: Visualizing our ML pipelines from ZenML‚Äôs dashboard.\nThe inference pipelines\nWe have two inference pipelines.\n\nThe summarization inference pipeline, which contains only the fine-tuned LLM, is deployed as a real-time inference endpoint on Hugging Face‚Äôs Dedicated Endpoints serverless service.\n\nThe agentic inference pipeline is our AI assistant, which takes as input requests from a user and provides answers leveraging the data from the MongoDB vector database.\n\nWe implemented it using Hugging Face‚Äôs smolagents Python framework, which allows us to build agents without hiding too much of what is going on behind the scenes. We attached a retriever tool that interacts with the vector database and a summarization tool to help us synthesize answers.\n\nWe will also attach the agentic inference pipeline to a Gradio UI to completely simulate the experience of an AI assistant, as shown in Figure 10.\n\n\nFigure 10: Screenshot of our AI assistant Gradio UI asking it to write a paragraph on optimizing LLMs during inference. We can visualize how it used the MongoDB vector search tool to get more context from our custom data until it reached the maximum allowed number of steps.\nThe observability pipeline\nthe last piece of the puzzle is the observability pipeline, which consists of two main components:\n\nPrompt monitoring\n\nLLM evaluation\n\nFor both, we will use Opik, which provides a beautiful dashboard for monitoring complex prompt traces, as seen in Figure 11.\n\nIt also provides a Python SDK to help us evaluate agentic and RAG applications, track the results and compare them (similar to experiment tracking, but for evaluating LLM applications).\n\n\nFigure 11: Screenshot from Opik on monitoring the trace of an agent. We can visualize the user‚Äôs prompt and all the hidden steps until it reaches its final answer.\n7. Offline vs. online ML pipelines\nOne last architectural decision we have to highlight is the difference between the offline and online ML pipelines.\n\nOffline pipelines are batch pipelines that run on a schedule or trigger. They usually take input data, process it, and save the output artifact in another type of storage. From there, other pipelines or clients can consume the artifact as they see fit.\n\nThus, in our AI system, the offline ML pipelines are the\n\nData collection pipeline\n\nETL data pipeline\n\nRAG feature pipeline\n\nDataset generation feature pipeline\n\nTraining pipeline\n\nThese are all independent processes that can run one after the other or on different schedules. They don‚Äôt have to run in sequence, as they are entirely decoupled through various storages: a NoSQL database, a vector database, a data registry or a model registry.\n\nBecause of their nature, we will orchestrate all the offline pipelines using ZenML, a popular ML orchestrator that allows us to schedule, trigger, configure, or deploy each pipeline.\n\n\nFigure 12: Offline vs. online ML pipelines\nOn the other hand, we have online pipelines that directly interact with a client. In this setup, a client (e.g., a user or other software) requests a prediction in real or near real-time. Thus, the system has to be online 24/7, process the request, and return the answer.\n\nIn our use case, the online pipelines are the following:\n\nAgentic inference pipeline\n\nSummarization inference pipeline\n\nObservability pipeline\n\nBecause of their request-answer nature, online pipelines do not need orchestration. Instead, they adopt a strategy similar to deploying RESTful APIs from the software engineering world.\n\nIt is critical to highlight that the offline and online pipelines are entirely different processes and often entirely different applications.\n\nSeeing these LangChain PoCs, where the RAG ingestion, retrieval and generation are in the same Notebook, can be deceiving. You never (or almost never) want to ingest the data at query time; you want to do it offline. Thus, when the user asks a question, the vector database is already populated and ready for retrieval.\n\nTo clearly reflect this aspect, our codebase decoupled the offline and online pipelines into two different Python applications, as shown in Figure 13.\n\n\nFigure 13: The folder structure of the offline and online Python applications.\nThe last step is to say a few words about how you can run the code.\n\n8. Running the code\nThis lesson was only an overview of what you will learn in the following five lessons. Thus, there is no specific code attached to this lesson.\n\nHowever, if you want to test our code without going through the following lessons, we have provided end-to-end instructions on how to do so in our GitHub repository.\n\nThus, you can choose your learning journey: go through our lessons or directly try out the code.\n\nEnjoy!\n\nConclusion\nThis lesson taught you what you will build and learn throughout the ‚ÄúBuilding Your Second Brain AI Assistant Using LLMs and RAG‚Äù open-source course.\n\nIn this lesson, we‚Äôve laid out the foundations by presenting the data flow of the AI assistant and the FTI architecture.\n\nNext, we‚Äôve shown how to apply the FTI architecture on top of our data flow to architect a production-ready AI assistant.\n\nLesson 2 (WIP) will focus on implementing the ETL data pipeline. While building it, we will learn how to use ZenML to orchestrate offline ML pipelines, crawl custom URLs, parse them to Markdown, and compute a quality score/document using LLMs.\n\nüíª Explore all the lessons and the code in our freely available GitHub repository.\n\nIf you have questions or need clarification, feel free to ask. See you in the next session!\n\nWhenever you‚Äôre ready, there are 3 ways we can help you:\nPerks: Exclusive discounts on our recommended learning resources\n\n(live courses, self-paced courses, learning platforms and books).\n\nThe LLM Engineer‚Äôs Handbook: Our bestseller book on mastering the art of engineering Large Language Models (LLMs) systems from concept to production.\n\nFree open-source courses: Master production AI with our end-to-end open-source courses, which reflect real-world AI projects, covering everything from system architecture to data collection and deployment.\n\nReferences\nDecodingml. (n.d.). GitHub - decodingml/second-brain-ai-assistant-course. GitHub. https://github.com/decodingml/second-brain-ai-assistant-course\n\nIusztin, P. (2024a, August 10). Building ML system using the FTI architecture. Decoding ML. https://decodingml.substack.com/p/building-ml-systems-the-right-way\n\nIusztin, P. (2024b, August 31). RAG Fundamentals first. Decoding ML. https://decodingml.substack.com/p/rag-fundamentals-first\n\nIusztin, P. (2025a, January 2). Advanced RAG Blueprint: Optimize LLM retrieval Systems. Decoding ML. https://decodingml.substack.com/p/your-rag-is-wrong-heres-how-to-fix",
      "summary": "### Build your Second Brain AI assistant\n\n**Summary:** The open-source course ‚ÄúBuilding Your Second Brain AI Assistant Using LLMs and RAG‚Äù aims to teach participants how to design and construct a Notion-like AI assistant that interacts with digital notes and resources. The course consists of six lessons, with the first focusing on the architecture and components of the AI assistant. \n\nKey lessons include:\n1. **Introduction to the AI Assistant**: Participants will learn to create an AI assistant that uses their stored knowledge in applications like Notion, enhancing productivity by limiting the AI's responses to their resources, thus reducing inaccuracies.\n2. **Data Pipelines**: Understanding how to collect and preprocess data from Notion, including crawling links and storing them in a NoSQL database.\n3. **Feature/Training/Inference (FTI) Architecture**: The course outlines the three pipelines essential for AI system construction: feature, training, and inference.\n4. **Advanced Techniques**: Participants will implement advanced Retrieval-Augmented Generation (RAG) techniques and learn about LLM fine-tuning, using tools like MongoDB and Hugging Face.\n5. **Observability**: The course emphasizes monitoring and evaluating the AI assistant's performance, ensuring reliability and effectiveness.\n\nBy the end of the course, participants will have the skills to build and customize their AI assistant, utilizing their digital knowledge effectively while adhering to best practices in machine learning and software engineering.",
      "classification": "### Build your Second Brain AI assistant\n\n**Category:** Tool",
      "keyword": "### Build your Second Brain AI assistant\n\n**Keywords:** Second Brain, AI assistant, Retrieval-Augmented Generation, LLM fine-tuning, data pipelines"
    },
    {
      "No.": 6,
      "end_point": "https://decodingml.substack.com/",
      "post_date": "2025.02.13",
      "link": "https://decodingml.substack.com/p/data-pipelines-for-ai-assistants?utm_source=substack&utm_medium=email",
      "title": "Data pipelines for AI assistants",
      "content": "Data pipelines for AI assistants\nThe backbone of successful AI systems\nPaul Iusztin\nFeb 13, 2025\n\nThe second lesson of the open-source course Building Your Second Brain AI Assistant Using Agents, LLMs and RAG ‚Äî a free course that will teach you how to architect and build a personal AI research assistant that talks to your digital resources.\n\nA journey where you will have the chance to learn to implement an LLM application using agents, advanced Retrieval-augmented generation (RAG), fine-tuning, LLMOps, and AI systems techniques.\n\nLessons:\nLesson 1: Build your Second Brain AI assistant\n\nLesson 2: Data pipelines for AI assistants\n\nLesson 3: Generate high-quality fine-tuning datasets (WIP)\n\nLesson 4: Fine-tune and deploy open-source LLMs (WIP)\n\nLesson 5: RAG feature pipelines for building AI assistants (WIP)\n\nLesson 6: Agents and LLMOps (WIP)\n\nüîó Learn more about the course and its outline.\n\nData pipelines for AI assistants\nWelcome to Lesson 2 of Decoding ML‚Äôs Building Your Second Brain AI Assistant Using Agents, LLMs and RAG open-source course, where you will learn to architect and build a production-ready Notion-like AI research assistant.\n\nEvery data and AI system starts with data. If you don‚Äôt have data, you don‚Äôt have the raw material to work with. You can have the most fancy algorithms, but without data, they are still like a car without fuel.\n\nHence, this lesson will teach us to architect and build the data pipelines that fuel our Second Brain AI assistant, such as the Notion data collection and ETL data pipelines.\n\nWhile implementing the data pipelines, we will learn the following:\n\nUse an MLOps framework such as ZenML to manage the ML pipelines.\n\nRead documents from Notion.\n\nStructure and validate our documents using Pydantic.\n\nCrawl ~400 links found in the Notion documents using Crawl4AI.\n\nCompute quality scores for each document using a two-stage design based on heuristics and LLMs.\n\nNormalize all the documents to Markdown.\n\nStore all the standardized documents in a NoSQL MongoDB.\n\n\nFigure 1: The architecture of the Second Brain AI assistant powered by RAG, LLMs and agents.\nLet‚Äôs get started. Enjoy!\n\nPodcast version of the lesson\nTable of contents:\nArchitecting data pipelines for AI systems\n\nUnderstanding MLOps frameworks for managing ML pipelines\n\nExploring the collected Notion data\n\nLooking into requirements for crawling\n\nImplementing crawling\n\nComputing the quality score\n\nLoading the standardized data to a NoSQL database\n\nRunning the code\n\n1. Architecting data pipelines for AI systems\nIn most use cases, a data pipeline starts with raw data collection, undergoes some transformation steps, and ultimately loads the transformed data into storage.\n\nTo build our Second Brain AI assistant, we must collect data from Notion, crawl all the links found in the Notion documents, and standardize everything in Markdown so downstream processes can easily process the documents.\n\nTo implement that, we split the data pipelines into two main components, making the architecture flexible and scalable.\n\nThe data collection pipeline\nWhich uses Notion‚Äôs API to access our personal data programmatically. Then, it extracts all the links from the crawled documents and adds them to their metadata. Ultimately, it standardizes the document into Markdown, saves it as JSON, and loads the results in S3.\n\nForcing people to use their Notion collections wasn‚Äôt a scalable solution or a good user experience, and neither was making our Notion collection public.\n\nThus, we decided to save our processed Notion collection into a public S3 bucket, which everyone can access effortlessly.\n\nTo draw a parallel to the industry, the S3 bucket could be seen as a data lake, where multiple teams from the organization push raw data that can be used within the company.\n\nThe course will start with reading our custom Notion collection from S3 and the next steps in processing. However, if you want a personalized experience, we provide the code and instructions for collecting your Notion collections.\n\nThus, let‚Äôs dig into the ETL data pipeline, where most of our focus will be.\n\nThe ETL data pipeline\nETL stands for ‚ÄúExtract, Transform, Load,‚Äù a popular data engineering pattern applied to most data pipelines.\n\nThe pattern is simple. You have to extract data from a source, apply some transformations, and ultimately load it into storage that makes the processed data accessible.\n\nHere is how it looks to us:\n\nExtract storage: S3\n\nTransformations: crawling, standardization to Markdown, computing a quality score\n\nLoad storage: NoSQL MongoDB\n\nNow, let‚Äôs dig into the details of each step.\n\nOnce the data has been downloaded from S3, we want to enrich it by crawling all the links within the Notion documents. This makes sense for two core reasons:\n\nWhen you chat with your AI assistant, you are primarily curious about what‚Äôs inside the link, not the link itself.\n\nWe have more data for fine-tuning our summarization LLM.\n\nAfter the documents are crawled, they are standardized to Markdown format (as well) and added to our existing collection.\n\nWe track the source of each document within the metadata, and because the documents are in Markdown format regardless of their source, we can treat the Notion and crawled documents equally and store them in the same collection. This will make our lives 100 times easier during future processing steps.\n\nFormatting everything in Markdown is critical because it ensures the data is standardized and more straightforward to process downstream in the pipeline. This standardization is very important because the data will later be used for RAG (Retrieval Augmented Generation) and to fine-tune a summarization large language model (LLM).\n\n\nFigure 2: The architecture of the ETL data pipeline\nNow that the data has been augmented, ensuring its quality is essential. We compute a quality score for each document. This quality score is a number between 0 and 1, which you can use to filter the documents based on the target quality you are looking for.\n\nYou know the famous saying regarding AI systems: ‚ÄúTrash in, trash out.‚Äù\n\nHence, this is a critical step for fine-tuning high-quality LLMs and doing RAG correctly.\n\nWe want to filter the data differently depending on whether we use it for RAG or fine-tuning. Thus, we store all documents with the quality score in the metadata. This allows downstream pipelines to decide what to filter.\n\nUltimately, the standardized data is loaded into a NoSQL database and ready for consumption by the feature pipelines, which will further process it for RAG and fine-tuning.\n\nIt is a good practice to create a snapshot of the data between the data and AI layers. This decouples the two, allowing you to run the data pipeline once and experiment further with your AI architecture. You can extend this by versioning your data sources and making the data available across your company (instead of just your project).\n\nAs before, to draw a parallel to the industry, this is similar to how a data warehouse, such as Big Query, connects the dots between the data and AI systems.\n\nThe last piece of the puzzle is our MLOps framework. For our use case, we picked ZenML. Through ZenML, we will manage all our offline pipelines, such as the data collection, ETL pipeline and feature engineering pipelines.\n\nBut why do we need an MLOps framework in the first place? Let‚Äôs dig into this in the next section.\n\n2. Understanding MLOps frameworks for managing ML pipelines\nMLOps frameworks are powerful tools that allow you to easily manage, schedule, track, and deploy your ML pipelines.\n\nMetaflow and ZenML are popular MLOps frameworks. They are optimized for long-running ML jobs, tracking metadata and output artifacts for reproducibility and setting up complex environments required for training and inference.\n\nOne core component of these MLOps frameworks is orchestrating offline ML pipelines. There is a fine line between data and ML orchestrators. Popular data orchestrators include Airflow and Prefect, which are optimized for running multiple small units in parallel.\n\nHowever, these data engineering tools are not built with ML as their first citizen. For example, they don‚Äôt include robust features for tracking and versioning output artifacts. In reality, they started rolling out ML-related features, but you will quickly realize they are forced and don‚Äôt fit naturally into their SDKs.\n\nFor our course, we chose ZenML because it can quickly run locally in dev mode, has a beautiful UI, and has an intuitive Python SDK.\n\nIt also supports all the requirements for most ML projects, such as model management and tracking configuration, metadata, and output artifacts per pipeline.\n\nIt also supports infrastructure management for all the popular cloud services such as AWS, GCP, Azure, and more. They recently introduced new features that allow you to quickly deploy all your pipelines using Terraform (Infrastructure as Code) or directly deploying them from their dashboard with zero code involved in the process (the heaven for Data Scientists).\n\nTo conclude, we need an MLOps framework to easily track, reproduce, schedule, and deploy all our offline ML pipelines without reinventing the wheel.\n\nEnough with the theory. Let‚Äôs quickly take a look at how ZenML works.\n\nFor example, Figure 3 shows ZenML‚Äôs dashboard with all the pipelines we‚Äôve run in their latest state.\n\n\nFigure 3: ZenML‚Äôs pipelines dashboard.\nIf we zoom in, for example, in the ETL pipeline, we can see all the previous runs with essential details, such as the infrastructure (‚ÄúStack‚Äù) in which they ran, as seen in Figure 4.\n\n\nFigure 4: ZenML‚Äôs ETL pipeline dashboard.\nUltimately, if we click on a specific pipeline run, we can see the whole Directed Acyclic Graph (DAG) with all its steps. If it fails, we can see the steps it failed at. Also, as seen in Figure 5, we can easily visualize and track the output of each step.\n\n\nFigure 5: Visualising the ETL pipeline run in ZenML.\nThere is much more to ZenML. However, to avoid creating a section that sounds like documentation, we will highlight its other features while building our AI assistant.\n\nAs we said in the previous section, this course will start with the ETL data pipeline, which will use our precomputed Notion dataset. Hence, let‚Äôs implement it in ZenML.\n\nWhat does our ETL data pipeline look like when implemented in ZenML?\nThe heart of our pipeline is the etl() function, decorated with ZenML's @pipeline decorator (found at pipelines/etl.py). This function orchestrates the entire data flow, accepting configuration parameters that control its behavior - from specifying data directories to controlling parallel processing and quality scoring settings:\n\n@pipeline\ndef etl(\n    data_dir: Path,\n    load_collection_name: str,\n    to_s3: bool = False,\n    max_workers: int = 10,\n    quality_agent_model_id: str = \"gpt-4o-mini\",\n    quality_agent_mock: bool = True,\n) -> None:\nThe pipeline's workflow begins by setting up the data paths. We establish two key directories: one for reading the raw Notion data and another for storing the processed results:\n\n    notion_data_dir = data_dir / \"notion\"\n    crawled_data_dir = data_dir / \"crawled\"\nThe primary data processing flow consists of three major steps. First, we read the documents from the disk. Then, we crawl every link found in each document. Next, we enhance the documents with quality scores:\n\n    documents = read_documents_from_disk(\n        data_directory=notion_data_dir, nesting_level=1\n    )\n    crawled_documents = crawl(documents=documents, max_workers=max_workers)\n    enhanced_documents = add_quality_score(\n        documents=crawled_documents,\n        model_id=quality_agent_model_id,\n        mock=quality_agent_mock,\n        max_workers=max_workers,\n    )\nThe final stage of our pipeline handles data persistence. We save the enhanced documents to disk, optionally upload them to S3, and finally load them into MongoDB for downstream processing:\n\n    save_documents_to_disk(documents=enhanced_documents, output_dir=crawled_data_dir)\n    if to_s3:\n        upload_to_s3(\n            folder_path=crawled_data_dir,\n            s3_prefix=\"second_brain_course/crawled\",\n            after=\"save_documents_to_disk\",\n        )\n    ingest_to_mongodb(\n        models=enhanced_documents,\n        collection_name=load_collection_name,\n        clear_collection=True,\n    )\nIn Figure 6, we can see what the ETL pipeline looks like in ZenML:\n\n\nFigure 6: An ETL pipeline run from ZenML.\nIn future sections of the article, we will zoom in on each step and understand how it works.\n\nOne last key feature of ZenML is that it can be configured through YAML configuration files (one per pipeline). This allows you to easily configure each pipeline run without touching the code. Most importantly, you can track and version the configuration of each pipeline run, which is critical for reproducibility and debugging.\n\nLet‚Äôs look at it in more detail (found under configs/etl.yaml):\n\nparameters:\n  data_dir: data/\n  load_collection_name: raw\n  to_s3: false\n  max_workers: 4\n  quality_agent_model_id: gpt-4o-mini\n  quality_agent_mock: false\nAs you can see, it‚Äôs a YAML file that is one-on-one with the pipeline Python function parameters. As the pipeline function acts as the entry point to your application, it makes sense to be able to configure it from a clean YAML file that can be easily tracked by git instead of tweaking the values from the CLI.\n\nIf you are curious to learn more about ZenML, they have some fantastic guides while also learning production MLOps and LLMOps:\n\nMore on ZenML\n\n3. Exploring the collected Notion data\nIn Lesson 1, we explored how our data looks directly in Notion. However, as we start working with it only after our data collection pipeline collects it, we have to visualize how the data we ingest from S3 looks like, as that is what we will work with.\n\nThe data is stored in JSON, containing the content of the Notion document in Markdown, along with its metadata and embedded URLs. Here is one sample:\n\n{\n    \"id\": \"8eb8a0ed6afffaa581ef6dff9b3eec17\",\n    \"metadata\": {\n        \"id\": \"8eb8a0ed6afffaa581ef6dff9b3eec17\",\n        \"url\": \"https://www.notion.so/Training-Fine-tuning-LLMs-8eb8a0ed6afffaa581ef6dff9b3eec17\",\n        \"title\": \"Training & Fine-tuning LLMs\",\n        \"properties\": {\n            \"Type\": \"Leaf\"\n        }\n    },\n    \"parent_metadata\": {\n        \"id\": \"6cfa25bcea00377355cfe21f7dfaadff\",\n        \"url\": \"\",\n        \"title\": \"\",\n        \"properties\": {}\n    },\n    \"content\": \"# Resources [Community]\\n\\n\\t<child_page>\\n\\t# Number of samples for fine-tuning based on general, domain, task-specific \n... # The rest of the document in Markdown format\n\"\n    \"content_quality_score\": null,\n    \"summary\": null,\n    \"child_urls\": [\n        \"https://github.com/huggingface/trl/\",\n        \"https://www.linkedin.com/company/liquid-ai-inc/\",\n        \"https://github.com/unslothai/unsloth/\",\n        \"https://arxiv.org/abs/2106.09685/\",\n        \"https://paperswithcode.com/sota/code-generation-on-humaneval/\",\n        \"https://github.com/axolotl-ai-cloud/axolotl/\",\n        \"https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/\",\n        \"https://arxiv.org/abs/2305.14314/\"\n}\nEvery document is stored in its own JSON following the same structure:\n\nmetadata: containing the source URL plus other details about the source\n\nparent_metadata: containing the parent‚Äôs URL, plus other details about the parent (if empty, it has no parent)\n\ncontent: the actual content in Markdown format\n\nchild_urls: all the URLs that were found in the document‚Äôs content (notice how diverse the links are)\n\nThe next step is to load this data into Python while ensuring that each JSON file is valid (has the expected structure and data types). The preferred method for this is using Pydantic.\n\nHow do we model this data into a Pydantic class?\nLet‚Äôs see how we can model our data using Pydantic, the go-to Python package for defining modern data structures in Python. But first, let‚Äôs make a quick analogy to LangChain.\n\nWhen working with LangChain, one of the fundamental building blocks is the Document class. Let's explore how we can implement our own version that builds upon LangChain's concept while respecting our custom functionality and needs.\n\nOur Document class maintains LangChain's core principle of combining content with metadata while extending it with additional features. Each document has its unique identifier, content, and structured metadata, plus we've added support for document hierarchies, quality assessment, and summarization.\n\nclass Document(BaseModel):\n    id: str = Field(default_factory=lambda: utils.generate_random_hex(length=32))\n    metadata: DocumentMetadata\n    parent_metadata: DocumentMetadata | None = None\n    content: str\n    content_quality_score: float | None = None\n    summary: str | None = None\n    child_urls: list[str] = Field(default_factory=list)\n\n    @classmethod\n    def from_file(cls, file_path: Path) -> \"Document\":\n        json_data = file_path.read_text(encoding=\"utf-8\")\n        return cls.model_validate_json(json_data)\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Document):\n            return False\n        return self.id == other.id\n\n    def __hash__(self) -> int:\n        return hash(self.id)\nWhile LangChain uses a simple dictionary for metadata, we've created a dedicated DocumentMetadata class. This structured approach ensures consistent metadata across our pipeline and provides better type safety:\n\nclass DocumentMetadata(BaseModel):\n    id: str\n    url: str\n    title: str\n    properties: dict\nBy storing the source URL and the parent‚Äôs source URL within the metadata while doing RAG, we can show the user the source used as context and where the source originates from. For example, we can show the user from which Notion database the link was accessed and what link was crawled.\n\nOne last thing to highlight is that storing the parent_metadata and child_urls fields can extend this to multi-hoping algorithms simulating a GraphDB structure. We won‚Äôt do that in our course, but it‚Äôs good to know that you don‚Äôt necessarily need a GraphDB to do GraphRAG. In cases where you need parents or children from only 2-3 levels relative to your data point, modeling your data using relationships is good enough to get started.\n\nBefore we discuss the implementation, we need to review some basic requirements for crawling.\n\n4. Looking into requirements for crawling\nSo, how does crawling work? Essentially, it's about automatically visiting web pages, extracting the content, and following links to discover more pages. This process can be very complex because every website has a different structure, and there are many ways to present content. You'll need to ensure you're able to handle this complexity.\n\nBefore crawling any website, you must check the site‚Äôs crawling limitations. You can do that by adding /robots.txt to the end of the website's URL. This file tells you which parts of the website are off-limits for web crawlers. Respecting these rules is essential since they protect the website from overloading and ensure you‚Äôre not crawling sensitive information.\n\n\nFigure 7: Result after accessing ‚Äúhttps://www.youtube.com/robots.txt‚Äù\nNow, if you want to crawl a website, you‚Äôll also need to know all the pages you need to visit. You might think you can start from a home page and follow all the links. However, you'll soon discover this approach can be inefficient and may not capture all website pages.\n\nThat‚Äôs why many websites provide a sitemap that lists all their pages. The sitemap is usually added to better index the site for search engines (which also crawl it), but we can also leverage it to get a list of recurrent links we can crawl easily.\n\nYou can usually find this sitemap by adding /sitemap.xml to the end of the website's URL. This file gives you a structured list of all the website‚Äôs sub-URLs, which makes it a lot easier to do a recursive crawl, which means you can follow all the links on the website.\n\n\nFigure 8: Result after accessing ‚Äúhttps://neptune.ai/sitemap.xml‚Äù\nNow, you need a tool to do all this.\n\nWe will use Crawl4AI for crawling, an open-source web crawling framework specifically designed to scrape websites and format the output for LLMs to understand.\n\nThe tool has built-in HTML to Markdown conversion, which is perfect for our needs. Crawl4AI is designed to be efficient, fast, and easy to set up. It can handle things like proxies, session management, and removing irrelevant content, which are not easy to handle.\n\n5. Implementing crawling\nHow can we apply these principles to our crawling algorithm?\n\nOur code is educative and harmless. Hence, to keep things simple, we will skip checking the robots.txt file. But it‚Äôs super important to remember this when working with real-world products.\n\nAlso, to avoid working with too many links, we will skip checking the sitemap.xml file and stick to the links found directly on our Notion pages. However, we could easily augment our dataset by accessing the sitemap.xml file of each link we use from Notion, expanding our dataset exponentially.\n\nWith that in mind, let‚Äôs dig into the implementation.\n\nAt the heart of our crawling step, we have a ZenML step that orchestrates the crawling process (which is called from the ZenML pipeline):\n\n@step\ndef crawl(\n    documents: list[Document], max_workers: int = 10\n) -> Annotated[list[Document], \"crawled_documents\"]:\n    crawler = Crawl4AICrawler(max_concurrent_requests=max_workers)\n    child_pages = crawler(documents)\n\n    augmented_pages = documents.copy()\n    augmented_pages.extend(child_pages)\n    augmented_pages = list(set(augmented_pages))\nTo track our crawling progress and provide insights into the pipeline's performance, we add metadata about the number of documents processed:\n\n    step_context = get_step_context()\n    step_context.add_output_metadata(\n        output_name=\"crawled_documents\",\n        metadata={\n            \"len_documents_before_crawling\": len(documents),\n            \"len_documents_after_crawling\": len(augmented_pages),\n            \"len_documents_new\": len(augmented_pages) - len(documents),\n        },\n    )\n\n    return augmented_pages\nThis metadata is attached to the crawled_documents output artifact and can be visualized from ZenML‚Äôs dashboard. As shown in Figure 9, it helps us monitor and debug each pipeline run.\n\n\nFigure 9: Metadata of the `crawled_documents` artifact.\nNow, let's look at our Crawl4AICrawler class implementation, which leverages the powerful features of Crawl4AI. This crawler is designed to handle concurrent web crawling efficiently while providing clean, LLM-ready output in Markdown format.\n\nThe crawler class under the hood uses Crawl4AI's AsyncWebCrawler with its sophisticated browser and crawler configurations. As the class is called from the ZenML step, which runs in a Python synchronous process, and the crawler uses an async method, we must define and manage an async loop internally. Using async, it‚Äôs a powerful mechanism in Python to manage concurrently I/O dependent processes such as crawling or API requests:\n\nclass Crawl4AICrawler:\n    def __init__(self, max_concurrent_requests: int = 10) -> None:\n        self.max_concurrent_requests = max_concurrent_requests\n\n    def __call__(self, pages: list[Document]) -> list[Document]:\n        try:\n            loop = asyncio.get_running_loop()\n        except RuntimeError:\n            return asyncio.run(self.__crawl_batch(pages))\n        else:\n            return loop.run_until_complete(self.__crawl_batch(pages))\nThe core crawling logic happens in the __crawl_batch method. We use Crawl4AI's CacheMode.BYPASS to ensure fresh content:\n\n    async def __crawl_batch(self, pages: list[Document]) -> list[Document]:\n        process = psutil.Process(os.getpid())\n        start_mem = process.memory_info().rss\n        \n        semaphore = asyncio.Semaphore(self.max_concurrent_requests)\n        all_results = []\n\n        async with AsyncWebCrawler(cache_mode=CacheMode.BYPASS) as crawler:\n            for page in pages:\n                tasks = [\n                    self.__crawl_url(crawler, page, url, semaphore)\n                    for url in page.child_urls\n                ]\n                results = await asyncio.gather(*tasks)\n                all_results.extend(results)\n\n        successful_results = [result for result in all_results if result is not None]\nFinally, each URL is processed by the __crawl_url method, which utilizes Crawl4AI's powerful content extraction and Markdown generation capabilities. As we kick off all the jobs simultaneously, we control how many concurrent requests we can run simultaneously using the semaphore Python object. This is useful for managing your computer‚Äôs resources or API limits:\n\n    async def __crawl_url(\n        self,\n        crawler: AsyncWebCrawler,\n        page: Document,\n        url: str,\n        semaphore: asyncio.Semaphore,\n    ) -> Document | None:\n        async with semaphore:\n            result = await crawler.arun(url=url)\n            await asyncio.sleep(0.5)  # Rate limiting\n\n            if not result or not result.success or result.markdown is None:\n                return None\n\n            child_links = [\n                link[\"href\"]\n                for link in result.links[\"internal\"] + result.links[\"external\"]\n            ]\n            title = result.metadata.pop(\"title\", \"\") if result.metadata else \"\"\n\n            return Document(\n                id=utils.generate_random_hex(length=32),\n                metadata=DocumentMetadata(\n                    id=document_id,\n                    url=url,\n                    title=title,\n                    properties=result.metadata or {},\n                ),\n                parent_metadata=page.metadata,\n                content=str(result.markdown),\n                child_urls=child_links,\n            )\nCrawl4AI is particularly well-suited for our needs as it's designed to be LLM-friendly. It generates clean Markdown output directly and handles all the formatting complexity behind the scenes.\n\nThe next step from our ETL pipeline is to compute the quality score.\n\n6. Computing the quality score\nAssessing content quality is crucial when processing documents in data pipelines. RAG systems can be incredibly powerful as long they use high-quality context. Also, when it comes to fine-tuning LLMs, using high-quality samples is the most critical aspect in which you should invest.\n\nThus, let's explore a sophisticated two-stage system that combines quick heuristic rules with an advanced LLM-based evaluation approach.\n\nAs parsing the document using an LLM can quickly increase the latency and costs of your system, we try first to do our best and compute the quality score using a set of heuristics. The key is to use heuristics where we are 100% sure we can score the documents. Next, for more complex and nuanced scenarios, we use the LLM.\n\nOur ZenML pipeline step orchestrates the quality scoring process, starting with fast heuristics and falling back to the LLM-based method when needed:\n\n@step\ndef add_quality_score(\n    documents: list[Document],\n    model_id: str = \"gpt-4o-mini\",\n    mock: bool = False,\n    max_workers: int = 10,\n) -> Annotated[list[Document], \"scored_documents\"]:\n    heuristic_quality_agent = HeuristicQualityAgent()\n    scored_documents: list[Document] = heuristic_quality_agent(documents)\n\n    scored_documents_with_heuristics = [\n        d for d in scored_documents if d.content_quality_score is not None\n    ]\n    documents_without_scores = [\n        d for d in scored_documents if d.content_quality_score is None\n    ]\n\n    quality_agent = QualityScoreAgent(\n        model_id=model_id, mock=mock, max_concurrent_requests=max_workers\n    )\n    scored_documents_with_agents: list[Document] = quality_agent(\n        documents_without_scores\n    )\nThe heuristic agent provides a quick first pass by analyzing URL density - a simple yet effective way to filter out low-quality documents that are mostly just collections of links:\n\nclass HeuristicQualityAgent:\n    def __call__(self, documents: list[Document]) -> list[Document]:\n        ...\n\n    def __score_document(self, document: Document) -> Document:\n        if len(document.content) == 0:\n            return document.add_quality_score(score=0.0)\n\n        url_based_content = sum(len(url) for url in document.child_urls)\n        url_content_ratio = url_based_content / len(document.content)\n\n        if url_content_ratio >= 0.7:\n            return document.add_quality_score(score=0.0)\n        elif url_content_ratio >= 0.5:\n            return document.add_quality_score(score=0.2)\n        return document\nFor more nuanced evaluation, we define a structured response format to ensure consistent scoring from our LLM:\n\nclass QualityScoreAgent:\n    SYSTEM_PROMPT_TEMPLATE = \"\"\"You are an expert judge tasked with evaluating the quality of a given DOCUMENT.\n\nGuidelines:\n1. Evaluate the DOCUMENT based on generally accepted facts and reliable information.\n2. Evaluate that the DOCUMENT contains relevant information and not only links or error messages.\n3. Check that the DOCUMENT doesn't oversimplify or generalize information in a way that changes its meaning or accuracy.\n\nAnalyze the text thoroughly and assign a quality score between 0 and 1, where:\n- **0.0**: The DOCUMENT is completely irrelevant containing only noise such as links or error messages\n- **0.1 - 0.7**: The DOCUMENT is partially relevant containing some relevant information checking partially guidelines\n- **0.8 - 1.0**: The DOCUMENT is entirely relevant containing all relevant information following the guidelines\n\nIt is crucial that you return only the score in the following JSON format:\n{{\n    \"score\": <your score between 0.0 and 1.0>\n}}\n\nDOCUMENT:\n{document}\n\"\"\"\nThe QualityScoreAgent implements sophisticated batch processing with concurrency control and rate limiting (similar to what you have seen in the crawler class). Calling OpenAI‚Äôs API in batch mode can quickly hit its request limits. To find a balance between process time and successfully scoring each document, we first called the API for each document using a shorter wait time between API calls. Next, for each API request that failed, we retry it with a longer wait period:\n\n    async def __get_quality_score_batch(\n        self, documents: list[Document]\n    ) -> list[Document]:\n        scored_documents = await self.__process_batch(documents, await_time_seconds=7)\n        documents_with_scores = [\n            doc for doc in scored_documents if doc.content_quality_score is not None\n        ]\n        documents_without_scores = [\n            doc for doc in scored_documents if doc.content_quality_score is None\n        ]\n\n        # Retry failed documents with increased await time\n        if documents_without_scores:\n            retry_results = await self.__process_batch(\n                documents_without_scores, await_time_seconds=20\n            )\n            documents_with_scores += retry_results\n\n        return scored_documents\nFor each document, we format the input prompt, make the API request, and wait for a given period to avoid API request limits:\n\n    async def __get_quality_score(\n        self,\n        document: Document,\n        semaphore: asyncio.Semaphore | None = None,\n        await_time_seconds: int = 2,\n    ) -> Document | None:\n        async def process_document() -> Document:\n            input_user_prompt = self.SYSTEM_PROMPT_TEMPLATE.format(\n                document=document.content\n            )\n\n            response = await acompletion(\n                model=self.model_id,\n                messages=[\n                    {\"role\": \"user\", \"content\": input_user_prompt},\n                ],\n                stream=False,\n            )\n            await asyncio.sleep(await_time_seconds)  # Rate limiting\n\n            raw_answer = response.choices[0].message.content\n            quality_score = self._parse_model_output(raw_answer)\n            return document.add_quality_score(score=quality_score.score)\nThe last step is to check that the LLM‚Äôs output follows our desired format by leveraging a Pydantic class:\n\nclass QualityScoreResponseFormat(BaseModel):\n    \"\"\"Format for quality score responses from the language model.\n\n    Attributes:\n        score: A float between 0.0 and 1.0 representing the quality score.\n    \"\"\"\n\n    score: float\n\n\ndef _parse_model_output(\n        self, answer: str | None\n    ) -> QualityScoreResponseFormat | None:\n        if not answer:\n            return None\n\n        try:\n            dict_content = json.loads(answer)\n            return QualityScoreResponseFormat(\n                score=dict_content[\"score\"],\n            )\n        except Exception:\n            return None\nHere are two important observations we still have to point out:\n\nInstead of directly using OpenAI‚Äôs API, we used litellm, a wrapper over multiple popular LLM APIs, such as OpenAI, Antrophic, Cohere, and more. We recommend using them, as they allow you to experiment easily with various providers without touching the code.\n\nTo further optimize the system by reducing costs and the chance of request-limit errors, you can use OpenAI‚Äôs batch API. In this way, you can send multiple documents per request.\n\nThe last step is to see how we can load the processed documents to our NoSQL MongoDB.\n\n7. Loading the standardized data to a NoSQL database\nWhen building data pipelines, you often need a reliable way to store and retrieve your processed documents. This type of storage is often known as the data warehouse.\n\nIn our use case, out of simplicity, we used a NoSQL MongoDB database, which is not a data warehouse by the book, but for text data, it gets the job done well.\n\nWe implemented the MongoDBService class to interact with MongoDB. This class provides a generic interface for handling any Python structure that follows a Pydantic model structure.\n\nThe class is designed to be flexible, using Python's generic typing to work with any Pydantic model. This means you can use it to store different types of Python data structures while maintaining type safety and validation:\n\nT = TypeVar(\"T\", bound=BaseModel)\n\nclass MongoDBService(Generic[T]):\n    def __init__(\n        self,\n        model: Type[T],\n        collection_name: str,\n        database_name: str = settings.MONGODB_DATABASE_NAME,\n        mongodb_uri: str = settings.MONGODB_URI,\n    ) -> None:\nThe ingest_documents() method is where the magic happens. It takes your Pydantic models and safely stores them in MongoDB. The method includes validation and proper error handling to ensure your data is stored correctly:\n\n    def ingest_documents(self, documents: list[T]) -> None:\n        try:\n            if not documents or not all(\n                isinstance(doc, BaseModel) for doc in documents\n            ):\n                raise ValueError(\"Documents must be a list of Pycantic models.\")\n\n            dict_documents = [doc.model_dump() for doc in documents]\n\n            # Remove '_id' fields to avoid duplicate key errors\n            for doc in dict_documents:\n                doc.pop(\"_id\", None)\n\n            self.collection.insert_many(dict_documents)\n            logger.debug(f\"Inserted {len(documents)} documents into MongoDB.\")\n        except errors.PyMongoError as e:\n            logger.error(f\"Error inserting documents: {e}\")\n            raise\nThat‚Äôs it. We are finally done implementing our data pipeline.\n\nThe last step is to look at how we can run the code.\n\n8. Running the code\nThe best way to set up and run the code is through our GitHub repository, where we have documented everything you need. We will keep these instructions only in our GitHub to avoid having the documentation scattered throughout too many places (which is a pain to maintain and use).\n\nBut to give a sense of the ‚Äúcomplexity‚Äù of running the code, you have to run ONLY the following commands using Make:\n\nmake local-infrastructure-up    # 1. Spin up the infrastructure\nmake download-notion-dataset    # 2. Download the Notion dataset\nmake etl-pipeline               # 3. Run the ETL pipeline through ZenML\nThat‚Äôs all it takes to crawl and compute the quality score for all the documents.\n\nWhile the ETL pipeline is running, you can visualize it on ZenML‚Äôs dashboard by typing in your browser: http://127.0.0.1:8237\n\nConclusion\nThis lesson taught you what it takes to build the data pipelines to implement a Second Brain AI assistant.\n\nWe walked you through the architecture of the data pipelines, what it takes to manage it with an MLOps framework such as ZenML, what the Notion data looks like, and what you should consider when crawling.\n\nWe learned how to crawl custom links at scale using Crawl4AI, compute a quality score for all your documents using heuristics and LLMs, and create a snapshot of the data in a NoSQL MongoDB database.\n\nLesson 3 (WIP) will teach you how to generate a high-quality summarization instruction dataset using distillation and how to load it to Hugging Face.\n\nüíª Explore all the lessons and the code in our freely available GitHub repository.\n\nIf you have questions or need clarification, feel free to ask. See you in the next session!\n\nWhenever you‚Äôre ready, there are 3 ways we can help you:\nPerks: Exclusive discounts on our recommended learning resources\n\n(live courses, self-paced courses, learning platforms and books).\n\nThe LLM Engineer‚Äôs Handbook: Our bestseller book on mastering the art of engineering Large Language Models (LLMs) systems from concept to production.\n\nFree open-source courses: Master production AI with our end-to-end open-source courses, which reflect real-world AI projects, covering everything from system architecture to data collection and deployment.\n\nReferences\nDecodingml. (n.d.). GitHub - decodingml/second-brain-ai-assistant-course. GitHub. https://github.com/decodingml/second-brain-ai-assistant-course\n\nMongoDB: the Developer Data platform. (n.d.). MongoDB. https://www.mongodb.com\n\nPydantic. (n.d.). GitHub - pydantic/pydantic: Data validation using Python type hints. GitHub. https://github.com/pydantic/pydantic\n\nUnclecode. (n.d.). GitHub - unclecode/crawl4ai: üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. GitHub. https://github.com/unclecode/crawl4ai\n\nZenML - MLOps framework for infrastructure agnostic ML pipelines. (n.d.). https://zenml.io\n\nCole Medin. (2025, January 13). Turn ANY Website into LLM Knowledge in SECONDS [Video]. YouTube. https://www.youtube.com/watch?v=JWfNLF_g_V0",
      "summary": "### Data pipelines for AI assistants\n\n**Summary:** \nThis lesson, part of the open-source course \"Building Your Second Brain AI Assistant Using Agents, LLMs and RAG,\" focuses on creating data pipelines essential for developing AI assistants. The course outlines the importance of data in AI systems, emphasizing that without data, advanced algorithms lack utility. The lesson teaches how to architect and build data pipelines, specifically for the Second Brain AI assistant, leveraging Notion for data collection and implementing ETL (Extract, Transform, Load) processes.\n\nKey components of the data pipeline include:\n1. **Data Collection Pipeline**: Utilizes Notion‚Äôs API to extract personal data and standardize it to Markdown format for easier processing. The processed data is stored in a public S3 bucket, akin to a data lake.\n  \n2. **ETL Pipeline**: The pipeline extracts data from S3, crawls links in the Notion documents, standardizes everything to Markdown, computes quality scores, and loads the data into MongoDB. The quality score is crucial for filtering relevant documents for fine-tuning the AI model.\n\n3. **MLOps Framework**: The course employs ZenML for managing the ML pipelines, allowing users to easily track, schedule, reproduce, and deploy their pipelines.\n\n4. **Crawling Mechanism**: The lesson covers how to crawl data using the Crawl4AI framework, respecting web crawling guidelines and ensuring data quality through heuristic and LLM-based evaluations.\n\n5. **Quality Scoring**: A two-stage quality scoring system evaluates the relevance of documents based on various heuristics followed by an LLM assessment.\n\nOverall, the lesson provides a comprehensive guide on building robust data pipelines for AI assistants, preparing the groundwork for subsequent lessons on fine-tuning and deploying LLMs. Participants are encouraged to explore the course materials and code available on GitHub for practical implementation.",
      "classification": "### Data pipelines for AI assistants\n\n**Category:** Tool",
      "keyword": "### Data pipelines for AI assistants\n\n**Keywords:** Data Pipelines, MLOps, ETL Process, Retrieval-Augmented Generation (RAG), Quality Scoring"
    },
    {
      "No.": 7,
      "end_point": "https://techcrunch.com/",
      "post_date": "2025.02.11",
      "link": "https://techcrunch.com/2025/02/11/how-musks-97-4b-bid-could-gum-up-openais-for-profit-conversion/",
      "title": "How Musk‚Äôs $97.4B bid could gum up OpenAI‚Äôs for-profit conversion",
      "content": "On Monday, Elon Musk, the world‚Äôs richest man, offered to buy the nonprofit that effectively governs OpenAI for $97.4 billion. The unsolicited buyout would be financed by Musk‚Äôs AI company, xAI, and a consortium of outside investors, per a letter sent to California and Delaware‚Äôs attorneys general.\n\nOpenAI CEO Sam Altman quickly dismissed Musk‚Äôs bid, and took it as a chance to publicly dunk on him.\n\n‚Äúno thank you, but we will buy Twitter for $9.74 billion if you want,‚Äù Altman wrote in a post on X just hours after reports emerged of Musk‚Äôs offer for OpenAI. Musk owns X, the social network formerly known as Twitter; he paid roughly $44 billion for it in October 2022. \n\nThe two have a history. Musk is an OpenAI co-founder, and both he and xAI are currently involved in a lawsuit that alleges that OpenAI engaged in anticompetitive behavior, among other things.\n\nBut Altman‚Äôs rejection of a $97.4 billion takeover offer is more complicated than just saying ‚Äúno thanks,‚Äù according to corporate governance experts who spoke with TechCrunch.\n\nStalling OpenAI‚Äôs nonprofit conversion\nOpenAI CEO Sam Altman\nOpenAI CEO Sam Altman visits ‚ÄúMaking Money With Charles Payne‚Äù at Fox Business Network Studios on December 04, 2024 in New York City.\nImage Credits:Mike Coppola / Getty Images\nFor background, OpenAI was founded as a nonprofit before transitioning to a ‚Äúcapped-profit‚Äù structure in 2019. The nonprofit is the sole controlling shareholder of the capped-profit OpenAI corporation, which retains formal fiduciary responsibility to the nonprofit‚Äôs charter. \n\nOpenAI is now in the process of restructuring ‚Äî this time to a traditional for-profit company, specifically a public benefit corporation ‚Äî in a bid to raise much more capital. But Musk ‚Äî who is notorious for drowning his enemies in legal troubles ‚Äî may have stalled the transition and raised the price of OpenAI‚Äôs nonprofit with his bid.\n\nDelaware and California‚Äòs attorneys general have requested more information from the ChatGPT maker about its plans to convert to a for-profit benefit corporation. The situation also forces it to consider outside bids seriously.\n\nOpenAI‚Äôs board will almost certainly refuse the bid, but Musk has been setting the stage for future legal and regulatory battles. He‚Äôs already attempting to stall OpenAI‚Äôs for-profit conversion via an injunction, for instance. The bid appears to be an alternative offer, of sorts. \n\nNow, OpenAI‚Äôs board will have to demonstrate that it‚Äôs not underselling OpenAI‚Äôs nonprofit by handing the nonprofit‚Äôs assets, including IP from OpenAI‚Äôs proprietary research, to an insider (e.g. Sam Altman) for a steep discount.\n\n‚ÄúMusk is throwing a spanner into the works,‚Äù said Stephen Diamond, a lawyer who represented Musk‚Äôs opponents in corporate governance battles at Tesla, in an interview with TechCrunch. ‚ÄúHe‚Äôs exploiting the fiduciary obligation of the nonprofit board to not undersell the asset. [Musk‚Äôs bid] is something OpenAI has to pay attention to.‚Äù\n\nOpenAI is said to be gearing up for a funding round that would value its for-profit arm at $260 billion. The Information reports that OpenAI‚Äôs nonprofit is slated to get a 25% stake in OpenAI‚Äôs for-profit.\n\nWith his bid, Musk has signaled there‚Äôs at least one group of investors willing to pay a sizable premium for OpenAI‚Äôs nonprofit wing. That puts the board of directors in a tight spot. \n\nGrounds for rejection\nStill, just because Musk threw out an eye-popping offer doesn‚Äôt mean that OpenAI‚Äôs nonprofit has to accept.\n\nCorporate law gives tremendous authority to incumbent boards to protect against unsolicited takeover bids, according to David Yosifon, a Santa Clara University professor of corporate governance law.\n\nOpenAI could make the case that Musk‚Äôs bid is a hostile takeover attempt given that Musk and Altman aren‚Äôt the best of friends.\n\nThe company could also argue that Musk‚Äôs offer isn‚Äôt credible because OpenAI is already in the midst of a corporate restructuring process.\n\nAnother approach OpenAI could take would be challenging Musk on whether he has the funds. As The New York Times notes, Musk‚Äôs wealth is largely tied to his Tesla stock, meaning that Musk‚Äôs investment partners would have to supply much of the $97.4 billion total.\n\nOpenAI‚Äôs board may need to review Musk‚Äôs offer to fully asses whether it aligns with the nonprofit‚Äôs mission, not just specific financial or strategic goals, according to Scott Curran, the former general counsel to the Clinton Foundation. That means Musk‚Äôs offer could be weighed against OpenAI‚Äôs mission: ‚Äúto ensure that artificial general intelligence ‚Äì AI systems that are generally smarter than humans ‚Äì benefits all of humanity.‚Äù\n\n‚ÄúWhen Altman posted that response [on X], that was probably done without legal guidance,‚Äù Yosifon said. ‚ÄúIt‚Äôs not good for a regulator to see that kind of dismissive, knee-jerk tweet.‚Äù\n\nRaising the value for OpenAI assets\nThe board is likely to side with Altman. Nearly all the directors joined after Altman was briefly fired, then rehired, by the nonprofit‚Äôs board in late 2023. Altman himself is also a board member.\n\nIf nothing else, Musk‚Äôs bid may raise the potential market value of the OpenAI nonprofit‚Äôs assets. That could force OpenAI to raise more capital than it originally anticipated, and complicate talks with the startup‚Äôs existing backers. It could also dilute the value of stakes held by OpenAI investors in the for-profit arm, including major partners such as Microsoft. \n\nThat‚Äôs sure to anger Altman, who‚Äôs been working with investors for months to determine how to fairly compensate the nonprofit.\n\nThe gist is: OpenAI‚Äôs corporate restructuring plans just got more complex.\n\nTechCrunch has an AI-focused newsletter! Sign up here to get it in your inbox every Wednesday.\n\nTopics\n\nAI\nAI\nChatGPT\nElon Musk\nnonprofit\nOpenAI\nsam altman\nStartups",
      "summary": "### How Musk‚Äôs $97.4B bid could gum up OpenAI‚Äôs for-profit conversion\n\n**Summary:** Elon Musk has made a $97.4 billion unsolicited bid to purchase the nonprofit governing OpenAI, financing it through his AI company xAI and various investors. OpenAI CEO Sam Altman quickly rejected the offer with a sarcastic counterproposal. This bid complicates OpenAI's ongoing transition from a capped-profit structure to a public benefit corporation, which is intended to attract more capital. Corporate governance experts suggest that Musk's offer could be seen as a hostile takeover attempt, and it raises legal complexities regarding the nonprofit's fiduciary responsibilities. The Delaware and California attorneys general have requested further information on OpenAI's restructuring plans, potentially stalling the conversion process. Despite the bid, OpenAI's board is expected to reject it, but it may inadvertently increase the perceived value of OpenAI's assets and complicate negotiations with current investors, such as Microsoft. Ultimately, Musk's move introduces significant uncertainty into OpenAI's strategic direction.",
      "classification": "### How Musk‚Äôs $97.4B bid could gum up OpenAI‚Äôs for-profit conversion\n\n**Category:** Updates & Trends",
      "keyword": "### How Musk‚Äôs $97.4B bid could gum up OpenAI‚Äôs for-profit conversion\n\n**Keywords:** Elon Musk, OpenAI, nonprofit conversion, corporate governance, antitrust allegations"
    },
    {
      "No.": 8,
      "end_point": "https://www.gpters.org/",
      "post_date": "2025.01.22",
      "link": "https://www.gpters.org/nocode/post/issuing-inhouse-certificates-one-bve9DC05JG2LJus",
      "title": "ÌÅ¥Î¶≠ ÌïúÎ≤àÏúºÎ°ú ÏÇ¨ÎÇ¥ Ï¶ùÎ™ÖÏÑú Î∞úÍ∏âÌïòÍ∏∞ (1)",
      "content": "ÏÜåÍ∞ú\nÏßÅÏõêÏàòÍ∞Ä ÎßéÏùÄ ÌöåÏÇ¨ÏóêÏÑú ÏïÑÎ•¥Î∞îÏù¥Ìä∏ ÌïòÎã§Î≥¥Îãà Í∞ÅÏ¢Ö Ï¶ùÎ™ÖÏÑúÎ•º Î∞úÍ∏âÌïòÎäî Îã¥ÎãπÏßÅÏõêÏù¥ Îß§Î≤à Í∞ÅÏ¢Ö Ï†ïÎ≥¥Î•º Î≥µÏÇ¨ÌïòÍ±∞ÎÇò ÌÉÄÏù¥Ìïë ÌïòÎäîÍ±∏ Î≥¥Î©¥ÏÑú Ï°∞Í∏à Ìé∏ÌïòÍ≤å Ìï¥Ï§ÑÍπå ÌïòÎäî ÏÉùÍ∞ÅÏù¥ Îì§ÏóàÏäµÎãàÎã§. ÌïúÎ≤à ÎßåÎì§Ïñ¥üòÅ ü•∞ ÎëêÎ©¥ Îã§ÏñëÌïòÍ≤å ÌôúÏö©Ïù¥ Í∞ÄÎä•ÌïòÍ≤†ÎçîÍµ∞Ïöî.\n\n(Í∏ÄÏì∞Îäî Ïû¨Ï£ºÍ∞Ä ÏóÜÏñ¥ Ï†ïÎ≥¥ ÏúÑÏ£ºÎ°ú Ï†ïÎ¶¨Ìï†Í≤åÏöî)\n\nÏßÑÌñâ Î∞©Î≤ï\nÏÇ¨Ïö©ÎèÑÍµ¨: chatGPTÏôÄ Íµ¨Í∏Ä ÏãúÌä∏, ÎèÖÏä§, Ïï±Ïä§ Ïä§ÌÅ¨Î¶ΩÌä∏Î•º ÏÇ¨Ïö©\n\ngptÏóêÍ≤å Íµ¨Í∏ÄÏãúÌä∏ÏôÄ ÎèÖÏä§ Î¨∏ÏÑúIDÎ•º Ìè¨Ìï®Ìï¥Ï£ºÍ≥† Ïä§ÌÅ¨Î¶ΩÌä∏Î•º ÏöîÏ≤≠\n\n\nÏÇ¨Î≥∏\nÏï±Ïä§ Ïä§ÌÅ¨Î¶ΩÌä∏Î•º Ïù¥Ïö©ÌïòÏó¨ ÏïÑÎûò ÏûëÏÑ±Îêú Íµ¨Í∏Ä ÎèÖÏä§ÏôÄ ÏãúÌä∏ÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º ÏûÖÎ†•Î∞õÏïÑ pdfÎ•º Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏åÏóê Ï†ÄÏû•ÌïòÎäî ÏûêÎèôÌôîÎ•º ÎßåÎì§Í±∞Ïïº\n\nÏïÑÎûò ÎÇ¥Ïö©ÏùÑ Î∞òÏòÅÌïòÏó¨ Ïï±Ïä§ Ïä§ÌÅ¨Î¶ΩÌä∏Î•º ÏûëÏÑ±ÌïòÎäî Î∞©Î≤ïÏùÑ Îã®Í≥ÑÎ≥ÑÎ°ú ÏÉÅÏÑ∏ÌïòÍ≤å ÏÑ§Î™ÖÌï¥Ï§ò \n \nÍµ¨Í∏Ä ÎèÖÏä§ 1v-YXJ0R----------------KrH61xKs\nÍµ¨Í∏Ä ÏãúÌä∏ 1QFNF-------------------oELQnQ\n\nÍµ¨Í∏Ä ÏãúÌä∏Ïùò aÏó¥ÏóêÏÑú Ï≤¥ÌÅ¨Î∞ïÏä§Ïóê Ï≤¥ÌÅ¨Í∞Ä ÏûÖÎ†•ÎêòÎ©¥ ÏãúÌä∏ ÌñâÏóê Ìï¥ÎãπÌïòÎäî ÎÇ¥Ïö©ÏùÑ Íµ¨Í∏Ä ÎèÖÏä§Ïóê Î∞òÏòÅÌïòÏó¨ pdfÎ°ú Ï†ÄÏû•ÎêòÎèÑÎ°ù Ìä∏Î¶¨Í±∞ ÏÑ§Ï†ïÌï¥Ï§ò  \n\nÍµ¨Í∏Ä ÎèÖÏä§ÏóêÎäî Ïù¥Î¶Ñ, ÏÉùÎÖÑÏõîÏùºÏùÄ ÌëúÏóê ÏûÖÎ†•ÌïòÍ≥† \nÏÜåÏÜçÎûÄÏóêÎäî ÏÇ¨ÏóÖÎ∂ÄÏôÄ Î∂ÄÏÑúÎ•º ÏûÖÎ†•\nÏßÅÍ∏â, ÏûÖÏÇ¨ÏùºÏûê, Ï†úÏ∂úÏ≤òÎ•º Í∑∏ÎåÄÎ°ú ÏûÖÎ†•ÌïòÍ≥† \nÏ¶ùÎ™ÖÏÑú ÌïòÎã® ÎÇ†ÏßúÎäî ÌååÏùº ÏûëÏÑ±ÎÇ†ÏßúÎ•º ÏûÖÎ†•Ìï¥Ï§ò. \nÏûëÏÑ±Ïãú Ï§ÄÎπÑÌïú ÏãúÌä∏ÏôÄ Î¨∏ÏÑúÎäî ÏïÑÎûò Í∑∏Î¶ºÍ≥º Í∞ôÏäµÎãàÎã§.\n\nÌïúÍµ≠Ïñ¥ Î¨∏ÏûêÍ∞Ä Ï†ÅÌûå Ïä§ÌîÑÎ†àÎìúÏãúÌä∏\nALT\n\nÌïúÍµ≠Ïñ¥ ÎπÑÏûê Ïã†Ï≤≠ÏÑú ÌïúÍµ≠Ïñ¥ ÎπÑÏûê Ïã†Ï≤≠ÏÑú ÌïúÍµ≠Ïñ¥ ÎπÑÏûê Ïã†Ï≤≠ÏÑú ÌïúÍµ≠Ïñ¥ ÎπÑÏûê Ïã†Ï≤≠ÏÑú ÌïúÍµ≠Ïñ¥\nALT\n\nÏä§ÌÅ¨Î¶ΩÌä∏Ïóê ÏûàÎäî Íµ¨Í∏Ä ÏãúÌä∏ÏôÄ ÎèÖÏä§ idÎ•º ÌôïÏù∏ÌïòÎäî Î∞©Î≤ïÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§.\n\n\nÏàòÏ†ïÎã®Í≥Ñ\n\ngpt ÎãµÎ≥ÄÏùÑ Îã®Í≥ÑÎ≥ÑÎ°ú ÏßÑÌñâÌïòÎ©¥ÏÑú Îã§ÏñëÌïú Ïò§Î•òÎ•º Í≤ΩÌóòÌïòÍ≥† ÏàòÏ†ïÌïòÎ©¥ÏÑú ÌïòÎÇòÏî© Í≥†Ï≥êÎÇòÍ∞îÏäµÎãàÎã§.\n\nüò¢ Í∑∏Îü¨ÎÇò... Í≥ÑÏÜç ÎêòÎäî ÏóêÎü¨Î•º Í≥†ÎØºÌïòÎã§ ÏïåÍ≤åÎêú Í≤ÉÏù¥ ÏïÑÏõÉÌíãÏù∏ pdf ÌååÏùºÏùÑ Ï†ÄÏû•Ìï† Í≥µÍ∞ÑÏùÑ ÏßÄÏ†ïÌï¥Ï£ºÏßÄ ÏïäÏïòÎçîÍµ∞Ïöî(ÌòºÏûê Î∞îÎ≥¥ÎùºÍ≥† Ïô∏Ï≥§ÏäµÎãàÎã§ üòÆ). Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å Ìè¥Îçî Ï†ïÎ≥¥Î•º ÏßÄÏ†ïÌï¥ Ï£ºÏñ¥Ïïº Ìï¥Îãπ Ìè¥ÎçîÏóê ÌååÏùºÏù¥ Ï†ÄÏû•ÎêòÎçîÍµ∞Ïöî. ÏïÑÎûòÎäî Ìè¥Îçî ÏïÑÏù¥ÎîîÎ•º ÌôïÏù∏ÌïòÎäî Î∞©Î≤ïÏûÖÎãàÎã§.\n\nÌïúÍµ≠ÏÇ¨Ïù¥Ìä∏ Ïä§ÌÅ¨Î¶∞ÏÉ∑\nALT\n\nÌè¥Îçî ÏïÑÏù¥ÎîîÍπåÏßÄ Ï∂îÍ∞ÄÌïú ÌõÑÏóêÏïº ÌååÏùºÏù¥ ÏÉùÏÑ±ÎêòÍ∏∞ ÏãúÏûëÌïòÎçîÍµ∞Ïöî^^;;; Ìïú ÏãúÍ∞Ñ ÎÑòÍ≤å Í±∏Ï≥ê ÏàòÏ†ïÏùÑ Î∞òÎ≥µÌïòÍ≥†, Ìä∏Î¶¨Í±∞ Ï°∞Ï†ïÌïòÍ≥† ÌïòÎ©¥ÏÑú Ï°∞Í∏àÏî© ÏôÑÏÑ±Ïù¥ ÎêòÏñ¥Í∞îÏäµÎãàÎã§.\n\nÏïÑÎûòÎäî Ìä∏Î¶¨Í±∞ ÏÑ§Ï†ïÎ∞©Î≤ïÏûÖÎãàÎã§\n\nÌïúÍµ≠Ïñ¥Î°ú Îêú Google Í≤ÄÏÉâ ÌéòÏù¥ÏßÄÏùò Ïä§ÌÅ¨Î¶∞ÏÉ∑\nALT\n\nÏôÑÎ£åÎã®Í≥Ñ\n\nÏïΩ ÎëêÏãúÍ∞ÑÏóê Í±∏Ïπú ÏûëÏóÖ ÎÅùÏóê ÏôÑÏÑ±Îêú pdf ÌååÏùºÏûÖÎãàÎã§.\n\n\nÌôçÍ∏∏Îèô (1).pdf\n51.18KB\n\n\nÎßàÏßÄÎßâÏúºÎ°ú ÏôÑÏÑ±Îêú Ïï±Ïä§ Ïä§ÌÅ¨Î¶ΩÌä∏ ÏΩîÎìú Ï≤®Î∂ÄÌï©ÎãàÎã§.\n\n\nÏÇ¨Î≥∏\nconst SHEET_ID = '-----------'; // Íµ¨Í∏ÄÏãúÌä∏ iD\nconst DOC_TEMPLATE_ID = '--------'; //Íµ¨Í∏ÄÎèÖÏä§ ÏñëÏãù ID\nconst FOLDER_ID = '--------'; // PDF Ï†ÄÏû• Ìè¥Îçî ID Ï£ºÏÜåÏ§ë project/.../edit ...Î∂ÄÎ∂Ñ\n\n\nfunction onEdit(e) {\n  if (!e) {\n    Logger.log('onEdit Ìï®ÏàòÎäî ÏßÅÏ†ë Ïã§ÌñâÎêòÏßÄ ÏïäÏäµÎãàÎã§.');\n    return;\n  }\n\n  const sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet();\n  const range = e.range;\n\n  Logger.log(`ÏàòÏ†ïÎêú ÏÖÄ: ${range.getA1Notation()}`);\n  Logger.log(`ÏàòÏ†ïÎêú Í∞í: ${range.getValue()}`);\n\n  if (range.getColumn() === 1 && range.getValue() === true) {\n    Logger.log('Ï≤¥ÌÅ¨Î∞ïÏä§Í∞Ä ÏÑ†ÌÉùÎêòÏóàÏäµÎãàÎã§.');\n    const row = range.getRow();\n    const rowData = sheet.getRange(row, 2, 1, 7).getValues()[0];\n    Logger.log('Ìï¥Îãπ Ìñâ Îç∞Ïù¥ÌÑ∞: ' + rowData.join(', '));\n\n    const [name, dob, businessUnit, department, position, joinDate, submitto] = rowData;\n\n    try {\n      const doc = createDocument(name, dob, businessUnit, department, position, joinDate, submitto);\n      const pdf = convertToPDF(doc, name);\n      Logger.log(`PDF ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å: ${pdf.getName()}`);\n    } catch (error) {\n      Logger.log(`Ïò§Î•ò Î∞úÏÉù: ${error.message}`);\n    }\n  }\n}\n\nfunction createDocument(name, dob, businessUnit, department, position, joinDate, submitto) {\n  Logger.log('Î¨∏ÏÑú ÏÉùÏÑ± Ï§ë...');\n  const template = DriveApp.getFileById(DOC_TEMPLATE_ID);\n  const newDoc = template.makeCopy(`Ï¶ùÎ™ÖÏÑú_${name}`, DriveApp.getFolderById(FOLDER_ID)).getId();\n  Logger.log(`Î≥µÏÇ¨Îêú Î¨∏ÏÑú ID: ${newDoc}`);\n  const doc = DocumentApp.openById(newDoc);\n  const body = doc.getBody();\n\n  body.replaceText('{{Ïù¥Î¶Ñ}}', name);\n  body.replaceText('{{ÏÉùÎÖÑÏõîÏùº}}', dob);\n  body.replaceText('{{ÏÜåÏÜç}}', `${businessUnit} / ${department}`);\n  body.replaceText('{{ÏßÅÍ∏â}}', position);\n  body.replaceText('{{ÏûÖÏÇ¨ÏùºÏûê}}', formatDate(new Date(joinDate)));\n  body.replaceText('{{Ï†úÏ∂úÏ≤ò}}', submitto);\n  body.replaceText('{{ÏûëÏÑ±ÏùºÏûê}}', formatDate(new Date()));\n\n  doc.saveAndClose();\n  return doc;\n}\n\nfunction convertToPDF(doc, name) {\n  Logger.log('PDF Î≥ÄÌôò Ï§ë...');\n  const docFile = DriveApp.getFileById(doc.getId());\n  const pdfBlob = docFile.getAs('application/pdf');\n  const folder = DriveApp.getFolderById(FOLDER_ID);\n  const pdfFile = folder.createFile(pdfBlob.setName(`${name}.pdf`));\n  docFile.setTrashed(true);\n  return pdfFile;\n}\n\nfunction formatDate(date) {\n  if (Object.prototype.toString.call(date) === '[object Date]') {\n    return Utilities.formatDate(date, Session.getScriptTimeZone(), 'yyyy-MM-dd');\n  }\n  return date;\n}\nÍ≤∞Í≥ºÏôÄ Î∞∞Ïö¥ Ï†ê\nÏù¥Ï†ÑÏóê ÌÅ¥Î°úÎìúÎ•º Ïù¥Ïö©Ìï† ÎïåÏôÄ ÎπÑÍµêÌïòÎ©¥, gptÍ∞Ä ÏΩîÎî©ÏóêÏÑúÎäî ÏÑ§Î™ÖÍ≥º ÏóêÎü¨ÏàòÏ†ïÏóêÏÑú Ï°∞Í∏à Î∂ÄÏ°±Ìïú ÎäêÎÇåÏûÖÎãàÎã§. Ïó¨Ïú†ÎêòÏãúÎ©¥ Ïä§ÌÅ¨Î¶ΩÌä∏Îäî ÌÅ¥Î°úÎìú Ï∂îÏ≤úÌï©ÎãàÎã§ üòÑ\n\nÎã§ÏùåÏ£ºÎäî Ï¶ùÎ™ÖÏÑú Ïã†Ï≤≠ÏùÑ Î∞õÎäî Íµ¨Í∏ÄÌèºÏùÑ Ï∂îÍ∞ÄÌïòÍ≥† Ïù¥ÌõÑÏóêÎäî Î∞úÌñâÎêòÎäî Ï¶ùÎ™ÖÏÑúÎ•º Ïù¥Î©îÏùºÎ°ú Î∞úÏÜ°ÌïòÎäî Î∂ÄÎ∂ÑÍπåÏßÄ Ï∂îÍ∞ÄÌï¥ Î≥¥Î†§Ìï©ÎãàÎã§.",
      "summary": "### ÌÅ¥Î¶≠ ÌïúÎ≤àÏúºÎ°ú ÏÇ¨ÎÇ¥ Ï¶ùÎ™ÖÏÑú Î∞úÍ∏âÌïòÍ∏∞ (1)\n\n**Summary:** Ïù¥ Í∏ÄÏóêÏÑúÎäî ÏßÅÏõêÎì§Ïù¥ Ï¶ùÎ™ÖÏÑúÎ•º ÏâΩÍ≤å Î∞úÍ∏âÎ∞õÏùÑ Ïàò ÏûàÎèÑÎ°ù Íµ¨Í∏Ä ÏãúÌä∏ÏôÄ ÎèÖÏä§, Ïï±Ïä§ Ïä§ÌÅ¨Î¶ΩÌä∏Î•º ÌôúÏö©Ìïú ÏûêÎèôÌôî Î∞©Î≤ïÏùÑ ÏÜåÍ∞úÌïúÎã§. ÏûëÏÑ±ÏûêÎäî Ïó¨Îü¨ Î≤àÏùò ÏàòÏ†ïÍ≥º Ïò§Î•ò Í≤ΩÌóòÏùÑ ÌÜµÌï¥ Íµ¨Í∏Ä ÏãúÌä∏ÏóêÏÑú Ï≤¥ÌÅ¨Î∞ïÏä§Î•º ÏÑ†ÌÉùÌïòÎ©¥ Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Íµ¨Í∏Ä ÎèÖÏä§Ïóê ÏûÖÎ†•ÌïòÍ≥†, PDFÎ°ú Î≥ÄÌôòÌïòÏó¨ Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏åÏóê Ï†ÄÏû•ÌïòÎäî Ïä§ÌÅ¨Î¶ΩÌä∏Î•º ÏôÑÏÑ±ÌñàÎã§. Ïä§ÌÅ¨Î¶ΩÌä∏ÏóêÎäî Íµ¨Í∏Ä ÏãúÌä∏ ID, Íµ¨Í∏Ä ÎèÖÏä§ ÌÖúÌîåÎ¶ø ID, PDF Ï†ÄÏû• Ìè¥Îçî ID Îì±Ïù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏúºÎ©∞, Ï≤¥ÌÅ¨Î∞ïÏä§ ÏÑ†ÌÉù Ïãú Ìï¥Îãπ ÌñâÏùò Îç∞Ïù¥ÌÑ∞Î•º ÏùΩÏñ¥Îì§Ïó¨ Î¨∏ÏÑúÎ•º ÏÉùÏÑ±ÌïòÎäî Î∞©ÏãùÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÎã§. ÏûëÏÑ±ÏûêÎäî Ïä§ÌÅ¨Î¶ΩÌä∏Ïùò Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌïòÎã§Îäî Ï†êÍ≥º Ìñ•ÌõÑ Íµ¨Í∏Ä ÌèºÏùÑ ÌÜµÌïú Ïã†Ï≤≠ Î∞è Ïù¥Î©îÏùº Î∞úÏÜ° Í∏∞Îä• Ï∂îÍ∞Ä Í≥ÑÌöçÎèÑ Ïñ∏Í∏âÌïòÍ≥† ÏûàÎã§.",
      "classification": "### ÌÅ¥Î¶≠ ÌïúÎ≤àÏúºÎ°ú ÏÇ¨ÎÇ¥ Ï¶ùÎ™ÖÏÑú Î∞úÍ∏âÌïòÍ∏∞ (1)\n\n**Category:** Tool",
      "keyword": "### ÌÅ¥Î¶≠ ÌïúÎ≤àÏúºÎ°ú ÏÇ¨ÎÇ¥ Ï¶ùÎ™ÖÏÑú Î∞úÍ∏âÌïòÍ∏∞ (1)\n\n**Keywords:** Ïï±Ïä§ Ïä§ÌÅ¨Î¶ΩÌä∏, Íµ¨Í∏Ä ÎèÖÏä§, Íµ¨Í∏Ä ÏãúÌä∏, ÏûêÎèôÌôî, PDF ÏÉùÏÑ±"
    },
    {
      "No.": 9,
      "end_point": "https://www.nb-data.com/",
      "post_date": "2025.02.07",
      "link": "https://www.nb-data.com/p/enhance-rag-accuracy-with-corrective?utm_source=post-email-title&publication_id=37262&post_id=156582633&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email",
      "title": "Enhance RAG Accuracy with Corrective-RAG (CRAG)",
      "content": "Retrieval-augmented generation (RAG) is a system that combines the data retrieval technique and LLM generation to produce an accurate response. Thus, RAG output will depend on the retrieval and generation parts.\n\nRAG output accuracy depends on data quality, the retrieval module, and the generation model. Often, the crucial part that could make or break the RAG system is the retrieved document itself.\n\nIn the previous article, we discussed a few techniques that could enhance retrieval. We have previously discussed many techniques, which you can read below.\n\nAlthough the retrieval process can still result in irrelevant or erroneous data, the Corrective-RAG (CRAG) framework emerged to help address these limitations.\n\nCorrective-RAG introduces a mechanism for error detection and correction within the retrieval-augmented generation pipeline by identifying inaccurate retrieved documents.\n\nThis article will explore building a simple CRAG that evaluates the retrieved documents. The system will follow the diagram below, and the code is in the repository.\n\nEnhance RAG Accuracy with Corrective-RAG (CRAG)\n\nIntroduction\nAs mentioned above, Corrective-RAG, or CRAG, is a framework for improving RAG results using error detection and correction steps. It was first introduced by Yan et al. (2024) paper, which explores where the lightweight retrieval evaluator can be used to assess the overall quality of retrieved documents and improve the robustness of generation.\n\nOften, these steps are only performed within the retrieval steps to detect the error documents and perform correction steps. However, they can be extended to the generation step as well.\n\nThe technique is based upon a feedback loop that continuously evaluates the quality of retrieved documents and provides evaluation. Basically, we pass the document to the evaluator and perform a correction step if it doesn‚Äôt pass the evaluation.\n\nEnhance RAG Accuracy with Corrective-RAG (CRAG)\nThe evaluation step can leverage advanced techniques such as confidence scoring, consistency checks, and contextual validation to detect potential errors.\n\nConfidence scoring is a mechanism for evaluating the reliability or trustworthiness of a retrieved document or generated response by assigning a numerical score.\n\nConsistency checks ensure that the retrieved information and the generated response are logically coherent and free from contradictions.\n\nContextual validation ensures that the retrieved information and generated response are accurate and contextually appropriate.\n\nBut how can we employ the evaluation technique above in the RAG system? There are many ways to do that, but one of the most used is the LLM-as-a-Judge evaluator.\n\nThe previous article taught us how the LLM-as-a-Judge works, but you can refresh the concept by reading the following article.\n\nOf course, you can use a more rigid evaluation metric such as ROUGE, BLEU, or any score metrics that can be used. What is important in the CRAG framework is that there is an evaluation and a corrective step that we perform.\n\nSo, what are the benefits of using the CRAG framework? There are a few notable benefits, including but not limited to:\n\nImproved Accuracy: By detecting and correcting errors in real time, CRAG significantly improves the factual accuracy of generated responses.\n\nBetter contextual response: CRAG ensures the generated responses are accurate and contextually appropriate.\n\nEnhanced User Trust: By delivering more accurate, reliable, and contextually appropriate outputs, CRAG builds greater trust with users.\n\nThose are some benefits you can expect by using the CRAG framework. Now, let‚Äôs try to build a simple CRAG framework.\n\nBuilding CRAG\nIn the next step, we will see the CRAG pipeline that evaluates the context of the retrieved document and performs the correction step. Note that for simplicity purposes, we don‚Äôt use a feedback loop here to improve the retrieval result; instead, we perform the corrections step by searching the web.\n\nWe will use the one we performed in the following article to build the simple RAG system.\n\n\nNon-Brand Data\nSimple RAG Implementation With Contextual Semantic Search\nHi everyone! Cornellius here, back with another Lite series. This time, we‚Äôll explore the advanced techniques and production methods of Retrieval-Augmented Generation (RAG)‚Äîtools that will be helpful for your use cases. I will make it into a long series, so stay tuned‚Ä¶\nRead more\n23 days ago ¬∑ 11 likes ¬∑ 2 comments ¬∑ Cornellius Yudha Wijaya\nLet‚Äôs start building the CRAG pipeline by adding an evaluation step. In this step, I will only use a simple LLM-as-a-Judge to evaluate whether the document is relevant to the query. The output will be either ‚Äúyes‚Äù or ‚Äúno‚Äù.\n\ndef grade_document(query, document):\n    #Uses the Gemini model to decide if a document is relevant to the query.\n    prompt = f\"\"\"Query: {query}\nDocument: {document}\nIs this document relevant to the query? Answer with \"yes\" or \"no\".\"\"\"\n    response = completion(\n        model=\"gemini/gemini-1.5-flash\",\n        messages=[{\"content\": prompt, \"role\": \"user\"}],\n        api_key=GEMINI_API_KEY\n    )\n    answer = response['choices'][0]['message']['content'].strip().lower()\n    return \"yes\" if \"yes\" in answer else \"no\"\nFor the correction step, we will only use a simple web search to retrieve additional context using Internet data.\n\ndef supplementary_retrieval(query):\n    #Performs a web search using DuckDuckGo and returns the result as a string.\n    search_tool = DuckDuckGoSearchRun()\n    web_result = search_tool.invoke(query)\n    return web_result\nLastly, we build the CRAG pipeline using all the components we have previously constructed. We have built our simple CRAG pipeline by combining the semantic search, evaluator, and correction.\n\ndef corrective_rag(query, top_k=2):\n    # The main CRAG pipeline:\n    #   1. Retrieve documents using semantic search.\n    #   2. Grade each document using the evaluator.\n    #   3. If no relevant document is found, perform a web search.\n\n    # Step 1: Retrieve documents\n    results = semantic_search(query, top_k=top_k)\n    retrieved_docs = results.get(\"documents\", [])\n    print(\"Initial retrieved documents:\")\n    for doc in retrieved_docs:\n        print(doc)\n\n    # Step 2: Grade each document for relevance (Evaluation step)\n    relevant_docs = []\n    for doc in retrieved_docs:\n        grade = grade_document(query, doc)\n        print(f\"Grading document (first 60 chars): {doc[:60]}... => {grade}\")\n        if grade == \"yes\":\n            relevant_docs.append(doc)\n\n    # Step 3: If no relevant document is found, perform supplementary retrieval (Correction step)\n    if not relevant_docs:\n        print(\"No relevant documents found; performing supplementary retrieval via web search.\")\n        supplementary_doc = supplementary_retrieval(query)\n        relevant_docs.append(supplementary_doc)\n    else:\n        print(\"Using relevant documents from the vector store.\")\n\n    # Ensure all elements in relevant_docs are strings\n    context = \"\\n\".join([\" \".join(doc) if isinstance(doc, list) else doc for doc in relevant_docs])\n\n    # Step 4: Generate final answer using the combined context.\n    final_answer = generate_response(query, context)\n    return final_answer\nYou can then try to test the CRAG pipeline using the following code.\n\nquery = \"What is the insurance for car?\"\nfinal_answer = corrective_rag(query)\nprint(\"Final Answer:\")\nprint(final_answer)\nThe result will depend on the retrieved document and the generation model.\n\nAs mentioned, you can build the CRAG pipeline even further by adding a more strenuous feedback loop and applying it in the generation module.\n\nThat‚Äôs all for now! I hope you all learn more about the CRAG framework.\n\nIs there anything else you‚Äôd like to discuss? Let‚Äôs dive into it together!",
      "summary": "### Enhance RAG Accuracy with Corrective-RAG (CRAG)\n\n**Summary:** \nRetrieval-augmented generation (RAG) combines data retrieval and large language model (LLM) generation to produce accurate responses, but its effectiveness hinges on the quality of retrieved documents. The Corrective-RAG (CRAG) framework addresses the challenges of irrelevant or erroneous data by introducing error detection and correction mechanisms within the RAG pipeline. This framework, first introduced by Yan et al. (2024), utilizes a feedback loop to evaluate the quality of retrieved documents.\n\nCRAG employs advanced evaluation techniques, including confidence scoring, consistency checks, and contextual validation, to ensure the reliability of retrieved information. A practical implementation involves using an LLM-as-a-Judge to assess document relevance and a web search for supplementary context when necessary.\n\nThe benefits of CRAG include improved factual accuracy, better contextual responses, and enhanced user trust. The article outlines a simple CRAG pipeline, which consists of retrieving documents, grading their relevance, and performing corrective actions if no relevant documents are found. The implementation includes code snippets for evaluating documents and retrieving additional context, demonstrating how CRAG can enhance the overall accuracy and reliability of RAG systems.",
      "classification": "### Enhance RAG Accuracy with Corrective-RAG (CRAG)\n\n**Category:** Model",
      "keyword": "### Enhance RAG Accuracy with Corrective-RAG (CRAG)\n\n**Keywords:** Retrieval-Augmented Generation, Corrective-RAG, Error Detection, Confidence Scoring, Contextual Validation"
    },
    {
      "No.": 10,
      "end_point": "https://marvelousmlops.substack.com/",
      "post_date": "2025.02.05",
      "link": "https://marvelousmlops.substack.com/p/unicorn-and-rainbows-the-reality?utm_source=post-email-title&publication_id=1746193&post_id=156526517&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email",
      "title": "Unicorns and Rainbows: The Reality of Implementing AI in a Corporate",
      "content": "Unicorns and Rainbows. Is it a metaphor? Is it a reality? Maybe both. Think of an unicorn dancing on top of a radiant rainbow. But, in fact, what does it mean?\n\n\nImage generated by AI\nHumanity has always been drawn to utopia‚Ää‚Äî‚Ääa perfect, idealized future where all problems are solved. Believing that the world is steadily marching toward this vision is tempting. In the AI landscape, the unicorn (you have noticed the 5th leg, right?) represents the elevated promises, wild imagination, and relentless hype that paint a picture of transformative, almost magical technology.\n\nMarvelous MLOps Substack is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\n\nUpgrade to paid\n\nThe rainbow, however, represents the real world: entire potential but riddled with imperfections, inconsistencies, and systemic barriers.\n\nJust like the stock market, AI has its declines and flows. Everything might seem to skyrocket, but a slight shift‚Ää‚Äî‚Äätechnical debt, regulatory burdens, or enterprise realities‚Ää‚Äî‚Ääcan send it crashing back to earth. The question is not whether AI is a transformative force (there is no doubt it is!) but whether we‚Äôre being realistic about its trajectory.\n\nThis article will discuss the reality of using AI in the enterprise environment, address technical debt, bridge knowledge gaps, and understand the herd effect that fuels the AI bubble. We aim to offer a realistic roadmap for businesses navigating the complex AI landscape by critically analyzing these factors.\n\n1. The AI bubble\nWe have been in Data & AI for over 10 years. The AI bubble has never been so big. We have AI everywhere on our laptops, phones, and websites. The CEOs of Nvidia, Microsoft, Meta, and OpenAI are spreading a lot of news about revolutionary AI technology, how AI agents will replace humans, how we will reach AGI soon, and how we will have AI everywhere. We live in an AI bubble, and even though the technology is accurate, it is nontrivial to apply it to actual use cases and drive business value than advertised.\n\nThe technological advancements in the AI field are significant, and the value AI generates is real. However, there are still many gaps that people who try to build AI products see clearly. Two factors contribute to the AI bubble: knowledge and the herd effect. Somehow, they are tangential but different.\n\n2. The Role of Knowledge Gaps\nThe gap between AI insiders and the general public is one of the hinds of the AI bubble. The saying ‚ÄúKnowledge is power‚Äù remains valid for AI within its development and implementation context.\n\nPeople who are deeply invested in the development of AI are fully aware of the nuances, challenges, and limitations that come with the implementation of AI-based solutions.\n\nOn the other hand, AI outsiders are constantly awe-struck by the marketing terms associated with AI which presents an entirely different world of possibilities. This knowledge gap enables misconceptions to spread at an alarming rate, therefore making the hype of AI take precedence over the reality of what AI systems can offer.\n\n3. Herd Effect: Fear of Missing Out (FOMO)\nAnother significant factor driving the AI hype is the herd effect or the Fear of Missing Out (FOMO).\n\nAs more companies invest in AI and tout their successes, others feel compelled to follow suit, fearing they‚Äôll fall behind if they don‚Äôt adopt AI technologies. This rush often leads to deploying AI solutions without a thorough understanding of their applicability or potential ROI, further inflating the AI bubble. The result is a market saturated with AI buzzwords and solutions that may not deliver the promised transformative impact.\n\nAI models (under AI models, we understand foundation models) are used everywhere, where a standard ML model should be used instead. This adds complexity and decreases reliability.\n\n4. Back to basics\nMost companies can not reliably bring standard machine learning models to production and lack monitoring practices.\n\nDespite what many people think, workflows that include AI models are, on average, more complex to bring into production and monitor‚Ää‚Äî‚Ääeven if you take the simplest scenario without RAG or finetuning involved‚Ää‚Äî‚Ääjust call a 3rd party API.\n\nIn too many cases, we seem to have forgotten the basic principles of machine learning and blindly rely on what that API outputs. This is the danger of AI hype: AI has become accessible to everyone, and many software engineers treat it as just another API call.\n\nWhat could go wrong? The data model has not changed, the code has not changed (and neither has the environment where it gets executed), and the version of the API has not changed. But this is the beauty of machine learning: even if everything in your control has not changed, the model can start performing poorly unexpectedly because data distribution has changed.\n\nThis does not just happen with standard machine learning models, it also happens with AI models‚Ää‚Äî‚Ääwe just have less means to impact that behavior, and prompt finetuning becomes an essential part of the process.\n\n5. Real-world, 2025\nExperts say that 2025 will be the year of AI agents. But is it really true?\n\nWhile the AI hype machine continues to boom, real-world adoption tells a different story. The promise of autonomous AI agents seamlessly operating across enterprises remains largely aspirational. The reality? AI in enterprise is still a work in progress‚Ää‚Äî‚Ääcomplex, expensive, and often misaligned with actual business needs.\n\nTake BBVA, the Spanish bank that went all in on OpenAI‚Äôs technology. They deployed over 2,900 AI models to enhance productivity, yet integrating them into their existing systems turned out to be a logistical nightmare. AI doesn‚Äôt operate in a vacuum; it needs to connect with legacy infrastructure, existing workflows, and strict regulatory requirements. And that‚Äôs where reality bites‚Ää‚Äî‚Ääscaling AI across an enterprise is exponentially harder than rolling out a chatbot.\n\nThe UK government‚Äôs attempt to integrate AI into its welfare system faced significant limitations. At least six AI prototypes, designed to enhance staff training, improve job center services, and streamline disability benefit processing, were discontinued due to issues in scalability, reliability, and insufficient testing. Officials acknowledged several ‚Äúfrustrations and false starts,‚Äù highlighting the complexities involved in deploying AI within public services.\n\nA study highlighted several obstacles in developing and deploying AI agents within enterprises. Security concerns were identified as a top challenge by leadership (53%) and practitioners (62%). Other significant challenges included data governance, performance issues, and integration complexity. These findings underscore the multifaceted difficulties organizations face in implementing AI agents effectively.\n\nReflecting on these examples, it‚Äôs evident that the widespread adoption of AI agents in enterprise settings faces significant limitations. While 2025 may usher in extensive research, proofs of concept (POCs), and minimum viable products (MVPs), the path to full-scale integration remains complex.\n\n6. AI in a corporate environment\nBig companies operate under strict rules, structured workflows, and a constant focus on ROI. Unlike agile startups that can adapt on the fly, large organizations have to deal with complex approval processes, compliance checks, and risk management. All this makes adopting AI a slower process, and the idea of rapid transformation often feels more like a distant dream than something achievable.\n\nChip Huyen references the most common LLM applications in her AI engineering book. Enterprises are risk-averse and prefer to deploy internal-facing applications first. From what we have seen so far, even though there is initial support from the leadership to deploy such applications, not enough funding goes to those projects (and unlikely will) as they do not generate direct business value. We are not saying there is no value‚Ää‚Äî‚Ääthere is, but it is challenging to convince stakeholders.\n\n\nImage reinterpreted from Huyen, C. (2025). AI Engineering: Building Applications with Foundation Models. Available on Amazon\nIn enterprises, the most common use cases with direct business generation are related to customer service (forwarding customers to the right agents/ processes) and reviewing contracts. These use cases have been there for a while, and have historically been NLP-heavy, and AI models helped to improve these projects.\n\nSome companies have tried to use LLMs for recommendations and chatbots, and the world has seen enough failures. Here are some examples:\n\nDPD‚Äôs customer-facing chatbot, ‚ÄúRuby,‚Äù was designed to assist customers with their inquiries. However, due to insufficient safeguards, a user was able to provoke the bot into swearing and composing a poem criticizing the company itself. This incident underscores the importance of implementing strict content moderation protocols and regularly updating AI systems to prevent such occurrences.\n\nSimilarly, Pak‚ÄônSave‚Äôs AI meal planner app, intended to provide innovative recipe suggestions, malfunctioned and recommended a combination of ingredients that would produce chlorine gas, labeling it as an ‚Äúaromatic water mix.‚Äù This highlights the critical need for rigorous testing and validation of AI outputs, especially in applications directly impacting consumer health and safety.\n\nIt feels like not everyone has learned from it, and we regularly see companies launching AI applications without clear business value with poor guardrails, mainly for ‚Äúmarketing purposes‚Äù. Let‚Äôs hope it will not turn out to be bad marketing, as users will try to make the app do things it is not supposed to do ‚Äújust for fun‚Äù.\n\nThere are exceptions. Some companies created nice LLM-powered recommendations, for example, Zalando. It has well-implemented guardrails and is useful for the customers (it helps to find items that are otherwise hard to find via search). In October 2024, Zalando expanded its AI-powered assistant to all 25 markets, supporting local languages. This expansion aims to provide customers with personalized fashion advice and insights into emerging local trends, thereby enhancing the shopping experience.\n\n7. Areas of attention & conclusions\nThere is great potential to leverage AI in a corporate setting. However, to hope for enterprise adoption, we must consider security gaps, controlled environments, transparency and traceability, and a way to monitor and evaluate AI systems.\n\nIn enterprise ecosystems, AI systems need large volumes of data, including personal and proprietary information. Their role is to enhance workflows and boost efficiency, but they need access to critical systems, which can be considered a security risk. Organizations must focus on preventing unapproved access to data, breaches, and compliance issues.\n\nThreat actors can deploy malware that mimics AI behavior to breach networks, skew decisions, or steal secrets. AI agents act autonomously, making them harder to detect and control. This creates a major challenge: real-time oversight of AI systems.\n\nMonitoring is a persistent issue. Few companies have proper systems in place, and AI‚Äôs complexity makes it even harder. Owners must fully understand every decision their AI makes\n\nAI‚Äôs transformative potential is undeniable, but the path from hype to reality is complex and challenging. Rather than chasing unicorns and rainbows, organizations must take a grounded, strategic approach‚Ää‚Äî‚Ääone that prioritizes real business needs, robust security frameworks, and a deep understanding of AI‚Äôs limitations. The road ahead is uncertain, but one thing is clear: the way we answer these questions will determine whether AI becomes a lasting force for good or just another passing bubble.",
      "summary": "### Unicorns and Rainbows: The Reality of Implementing AI in a Corporate\n\n**Summary:** The article discusses the contrasting perceptions of AI in corporate environments, symbolized by the \"unicorn\" representing lofty promises and the \"rainbow\" signifying the complex reality of implementation. Despite significant advancements in AI technology, the article argues that organizations must confront knowledge gaps, the herd effect, and technical challenges that inflate AI's perceived potential.\n\n1. **AI Bubble:** The current enthusiasm for AI is likened to a bubble, driven by influential tech leaders and media hype, but real-world applications remain challenging and often fail to deliver on promises.\n\n2. **Knowledge Gaps:** A divide exists between AI experts and the general public, leading to misconceptions about AI‚Äôs capabilities. Insiders understand the nuanced challenges, while outsiders are swayed by marketing.\n\n3. **Herd Effect:** Companies feel pressured to adopt AI technologies due to fear of falling behind, often leading to rushed implementations without proper understanding or strategic planning.\n\n4. **Back to Basics:** Many companies struggle to deploy standard machine learning models, complicating the integration of AI solutions. A lack of monitoring and reliance on third-party APIs can lead to inconsistent performance.\n\n5. **Real-world Challenges by 2025:** While predictions suggest a rise in autonomous AI agents, actual integration into enterprises faces logistical and regulatory hurdles, as evidenced by failed attempts in large organizations and government initiatives.\n\n6. **Corporate Environment:** Large companies face bureaucratic challenges and risk aversion, making rapid AI adoption difficult. Successful AI applications tend to focus on customer service and process automation.\n\n7. **Cautionary Examples:** Several high-profile failures highlight the need for rigorous testing and clear business value in AI deployments. Successful cases, like Zalando‚Äôs AI-powered recommendations, illustrate potential when guardrails are in place.\n\n8. **Conclusion:** While AI holds great promise, organizations must prioritize security, transparency, and a thorough understanding of AI's limitations. A strategic approach, grounded in reality, is essential for realizing AI's true potential in the corporate world.",
      "classification": "### Unicorns and Rainbows: The Reality of Implementing AI in a Corporate\n\n**Category:** Updates & Trends",
      "keyword": "### Unicorns and Rainbows: The Reality of Implementing AI in a Corporate\n\n**Keywords:** AI bubble, knowledge gaps, herd effect, enterprise integration, security frameworks"
    },
    {
      "No.": 11,
      "end_point": "https://modulabs.co.kr/",
      "post_date": "2025.02.05",
      "link": "https://modulabs.co.kr/blog/deepseek-security-caution?utm_source=stibee&utm_medium=moduletter&utm_campaign=blog_curation&utm_content=deepseek_security&utm_term=none",
      "title": "DeepSeek Î≥¥Ïïà, Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®Ïùò Ïà®Í≤®ÏßÑ ÏúÑÌóòÏÑ± Î∂ÑÏÑù",
      "content": "Ï±óGPT Í∞ôÏùÄ Ïù∏Í≥µÏßÄÎä• Ï±óÎ¥á Îì±Ïù¥ ÎåÄÏÑ∏Í∞Ä ÎêòÎ©¥ÏÑú ÎåÄÎã§ÏàòÏùò ÏÇ¨ÎûåÎì§Ïù¥ Ïù∏Í≥µÏßÄÎä• ÏÇ¨Ïö©ÌïòÎ©¥ÏÑú ÏÉùÌôúÏùÑ ÌïòÍ≥† ÏûàÏäµÎãàÎã§.\nÏöîÍ∑ºÎûò Ï§ëÍµ≠ ÌöåÏÇ¨ DeepSeekÏóêÏÑú ÎßåÎì† Ï±óÎ¥áÎèÑ Ïù∏Í∏∞Î°ú ÏûêÎ¶¨Ïû°ÏïòÏúºÎ©∞ ÌäπÌûà R1 ÏÑ±Îä•Ïù¥ OpenAIÏóêÏÑú ÎßåÎìúÎäî o1Ïóê ÎßûÎ®πÎäî ÏÑ±Îä•ÏùÑ ÏûêÎûëÌïúÎã§Îäî Í≤∞Í≥ºÍπåÏßÄ ÎÇòÏôîÏäµÎãàÎã§.\nÍ∑∏Îü∞Îç∞ ÌòπÏãú DeepSeek, ÍººÍººÌïòÍ≤å Îî∞Ï†∏Î≥¥Í≥† Ïì∞Í≥† Í≥ÑÏã†Í∞ÄÏöî?\nÏò§ÎäòÏùÄ DeepSeekÏùò Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®ÏùÑ ÏÉÖÏÉÖÏù¥ ÌååÌó§Ï≥ê Î≥¥Î©¥ÏÑú, DeepSeek Î≥¥ÏïàÏóê Î¨∏Ï†úÍ∞Ä ÏóÜÎäîÏßÄ Í∑∏Î¶¨Í≥† ÏÇ¨Ïö©Ìï†Îïå Ï£ºÏùòÌï¥Ïïº Ìï† Î∂ÄÎ∂ÑÏùÄ ÏóÜÎäîÏßÄ ÏïåÏïÑÎ≥¥Í≤†ÏäµÎãàÎã§.\n\nÏ§ëÍµ≠ Í∏∞ÏóÖÏùò DeepSeek Î≥¥Ïïà, ÎÇ¥ Ï†ïÎ≥¥Îäî Ïñ¥ÎîîÎ°ú?\n\n\nDeepSeekÏùò Í∞ÄÏû• ÌÅ∞ ÌäπÏßïÏùÄ, Îç∞Ïù¥ÌÑ∞Î•º Í¥ÄÎ¶¨ÌïòÎäî ÌöåÏÇ¨Í∞Ä Ï§ëÍµ≠Ïóê ÏûàÎã§Îäî Ï†êÏûÖÎãàÎã§.\nÌï≠Ï†ÄÏö∞ÏôÄ Î≤†Ïù¥ÏßïÏóê ÏûàÎäî DeepSeek Î≤ïÏù∏Îì§Ïù¥ Ïö∞Î¶¨ Ï†ïÎ≥¥Î•º Ï•êÍ≥† ÏûàÏñ¥ DeepSeek Î≥¥Ïïà Î¨∏Ï†úÍ∞Ä Ïö∞Î†§Îê©ÎãàÎã§.\n\nÏùºÎã®, Ïö∞Î¶¨Í∞Ä DeepSeekÏóê ÏûÖÎ†•ÌïòÎäî Î™®Îì† Ï†ïÎ≥¥Îäî Ï§ëÍµ≠ ÏÑúÎ≤ÑÏóê Ï†ÄÏû•ÎêòÍ≥†, Ï§ëÍµ≠ Î≤ïÏùò Ï†ÅÏö©ÏùÑ Î∞õÍ≤å ÎêòÎäîÎç∞ Ï§ëÍµ≠ÏùÄ Íµ≠Í∞Ä ÏïàÎ≥¥ÎÇò Í≥µÍ≥µÏßàÏÑúÎùºÎäî Ïù¥Ïú†Î°ú Ï†ïÎ∂ÄÍ∞Ä Îç∞Ïù¥ÌÑ∞Ïóê Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî Í∂åÌïúÏù¥ ÏóÑÏ≤≠ÎÇòÍ≤å ÎÑìÏäµÎãàÎã§.\nÌäπÌûà Ïö∞Î¶¨Î•º ÎπÑÎ°ØÌïú Ï§ëÍµ≠Ïóê ÏÇ¥ÏßÄ ÏïäÎäî Ïô∏Íµ≠Ïù∏Îì§Ïùò DeepSeek Î≥¥ÏïàÏù¥ ÎçîÏö± Ï∑®ÏïΩÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\nÌöåÏÇ¨ÏóêÏÑú Ï§ëÏöîÌïú ÏÇ¨ÏóÖ Ï†ÑÎûµÏù¥ÎÇò Í∏∞Ïà† Ï†ïÎ≥¥Î•º DeepSeekÏúºÎ°ú Ïù¥ÏïºÍ∏∞ÌñàÎã§Î©¥ Í∑∏ ÎÇ¥Ïö©Ïù¥ Ï§ëÍµ≠ Ï†ïÎ∂Ä Í∏∞Í¥ÄÏóê ÎÑòÏñ¥Í∞à ÏàòÎèÑ ÏûàÏäµÎãàÎã§.\nÎòê, Ï§ëÍµ≠Ïùò ÏÇ¨Ïù¥Î≤ÑÎ≥¥ÏïàÎ≤ïÏù¥ÎÇò Îç∞Ïù¥ÌÑ∞Î≥¥ÏïàÎ≤ïÏóê Îî∞Î•¥Î©¥, Ï†ïÎ∂ÄÍ∞Ä Ï§ëÏöîÌïòÎã§Í≥† ÌåêÎã®ÌïòÎäî Ï†ïÎ≥¥Îäî Ï¶âÏãú Ï†úÍ≥µÌï¥Ïïº Ìï† ÏùòÎ¨¥Í∞Ä ÏûàÏñ¥ÏÑú, ÏÇ¨Ïö©Ïûê ÏûÖÏû•ÏóêÏÑúÎäî ÎÇ¥ Ï†ïÎ≥¥Ïóê ÎåÄÌïú Í∂åÎ¶¨Î•º Ï†úÎåÄÎ°ú ÌñâÏÇ¨ÌïòÍ∏∞ Ïñ¥Î†§Ïö∏ Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê DeepSeek Î≥¥Ïïà Ï∏°Î©¥ÏóêÏÑú Ï¢ãÏùÄ ÏÑ†ÌÉùÏßÄÎùº Î≥º Ïàò ÏóÜÏäµÎãàÎã§.\n\nÎÑàÎ¨¥ ÎßéÏùÄ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Í∞ÄÎäî DeepSeek Î≥¥ÏïàÏùò ÌòÑÏã§\nDeepSeekÏùÄ Í∞úÏù∏ Ï†ïÎ≥¥Î•º ÎÑàÎ¨¥ ÎßéÏù¥ ÏàòÏßëÌïúÎã§Îäî Ï†êÏóêÏÑú DeepSeek Î≥¥Ïïà Ï†ïÏ±ÖÏùò Î¨∏Ï†úÏ†êÏù¥ ÎìúÎü¨ÎÇ©ÎãàÎã§.\nÌäπÌûà Ïù¥Îü∞ Î∂ÄÎ∂ÑÎì§ÍπåÏßÄ Ï†ÑÎ∂Ä ÏàòÏßëÌïòÎäî Í≤ÉÏù¥ Îçî ÌÅ∞ Î¨∏Ï†úÏûÖÎãàÎã§.\n\nÌÉÄÏù¥Ìïë ÏäµÍ¥ÄÍπåÏßÄ‚Ä¶?:\nÌÇ§Î≥¥ÎìúÎ•º ÎàÑÎ•¥Îäî Ìå®ÌÑ¥Ïù¥ÎÇò Î¶¨Îì¨ Í∞ôÏùÄ ÏÉùÏ≤¥ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Í∞ÄÎäîÎç∞, Ïù¥Í±∏ Ïôú ÏàòÏßëÌïòÎäîÏßÄ, Ïñ¥ÎîîÏóê Ïì∞ÎäîÏßÄ Î™ÖÌôïÌïòÍ≤å ÏÑ§Î™ÖÌï¥Ï£ºÏßÄ ÏïäÏäµÎãàÎã§.\nÏù¥Î†áÍ≤å ÎêòÎ©¥ Ïö∞Î¶¨Í∞Ä ÌÇ§Î≥¥ÎìúÎ•º Ïñ¥ÎñªÍ≤å ÎëêÎìúÎ¶¨ÎäîÏßÄ Î∂ÑÏÑùÌï¥ÏÑú, Ïö∞Î¶¨ ÌñâÎèô Ìå®ÌÑ¥ÏùÑ ÏïåÏïÑÎÇº ÏàòÎèÑ ÏûàÏäµÎãàÎã§.\n\n‚ÄúÏÇ¨Ïö©ÌïòÎäî Í∏∞Îä• Î∞è Ï∑®ÌïòÎäî ÌñâÎèô‚Äù‚Ä¶?:\nÏù¥Í±¥ Îòê Î¨¥Ïä® ÏùòÎØ∏ÏùºÍπåÏöî? ÎÑàÎ¨¥ Ïï†Îß§ÌïòÍ≤å Ï†ïÏùòÎêú ÏÇ¨Ïö© Ï†ïÎ≥¥Î•º ÏàòÏßëÌï¥ÏÑú, Ïö∞Î¶¨Î•º Í∞êÏãúÌïòÎ†§Îäî Í±¥ ÏïÑÎãåÏßÄ Ïö∞Î†§Í∞Ä Îê©ÎãàÎã§.\nÏÑúÎπÑÏä§Î•º Ï†úÍ≥µÌïòÎäî Îç∞ Íº≠ ÌïÑÏöîÌïú Ï†ïÎ≥¥Îßå ÏàòÏßëÌï¥Ïïº ÌïòÎäî ÏõêÏπôÏóê Ïñ¥Í∏ãÎÇúÎã§Í≥† Î≥º Ïàò ÏûàÏäµÎãàÎã§.\n\nÍ¥ëÍ≥† ÌååÌä∏ÎÑàÏóêÍ≤åÍπåÏßÄ‚Ä¶?:\nÏã¨ÏßÄÏñ¥ Í¥ëÍ≥† ÌååÌä∏ÎÑàÎÇò Îã§Î•∏ ÌöåÏÇ¨Î°úÎ∂ÄÌÑ∞ Ïö∞Î¶¨Í∞Ä DeepSeek Î∞ñÏóêÏÑú Î≠ò ÌïòÎäîÏßÄÍπåÏßÄ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏ÏòµÎãàÎã§.\nÏù¥Í±¥ AI Ï±óÎ¥á ÏÑúÎπÑÏä§ÏóêÏÑú Í∏∞ÎåÄÌïòÎäî ÏàòÏ§ÄÏùÑ ÎÑòÏñ¥ÏÑúÎäî Í±∞Ï£†. ÏÇ¨Ïö©ÏûêÏùò Ïò®ÎùºÏù∏ ÌôúÎèôÏùÑ ÏÉÖÏÉÖÏù¥ ÏÇ¨Ï∞∞ÌïúÎã§Îäî ÏÉùÍ∞ÅÏù¥ Îì§ Ïàò Î∞ñÏóê ÏóÜÏäµÎãàÎã§,\n\nÎÇ¥ ÎåÄÌôî ÎÇ¥Ïö©ÍπåÏßÄ ÏàòÏßëÌïòÎäî DeepSeek Î≥¥Ïïà Î¨∏Ï†ú\ninformation-thief\n\nDeepSeekÏùÄ Ï†ïÎ≥¥Î•º ‚ÄúÏÑúÎπÑÏä§ Ï†úÍ≥µ Î∞è Í¥ÄÎ¶¨, ÏÑúÎπÑÏä§ Í∞úÏÑ† Î∞è Í∞úÎ∞ú, ÏïàÏ†Ñ, Î≥¥Ïïà Î∞è ÏïàÏ†ïÏÑ± Ïú†ÏßÄ‚Äù Í∞ôÏùÄ Ïï†Îß§Ìïú Î™©Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©ÌïúÎã§Í≥†Îßå Î∞ùÌûàÍ≥† ÏûàÏäµÎãàÎã§.\nÌäπÌûà Ï±óÎ¥áÏùÄ Ïö∞Î¶¨Í∞Ä ÎåÄÌôîÌïú ÎÇ¥Ïö©ÏùÑ AIÍ∞Ä ÌïôÏäµÌïòÎäî Îç∞ Ïì∏ Í∞ÄÎä•ÏÑ±Ïù¥ ÌÅ∞Îç∞, Ïù¥ Î∂ÄÎ∂ÑÏóêÏÑú DeepSeek Î≥¥ÏïàÏù¥ Ï∑®ÏïΩÌïòÎã§Îäî Í≤ÉÏù¥ ÌÅ∞ Î¨∏Ï†úÏûÖÎãàÎã§.\n\nAIÍ∞Ä ÌïôÏäµÌïòÎäî Í≥ºÏ†ïÏóêÏÑú Ïù¥Îü∞ ÏúÑÌóòÎì§Ïù¥ ÏÉùÍ∏∏ Ïàò ÏûàÏäµÎãàÎã§:\n\nÎÇ¥ Ïù¥Î¶ÑÏù¥ÎÇò Ï£ºÏÜå Í∞ôÏùÄ Í∞úÏù∏ Ï†ïÎ≥¥Í∞Ä AI Î™®Îç∏Ïóê Ï†ÄÏû•ÎèºÏÑú, Îã§Î•∏ ÏÇ¨ÎûåÏù¥Îûë ÎåÄÌôîÌï† Îïå Í∞ëÏûêÍ∏∞ ÌäÄÏñ¥ÎÇòÏò¨ ÏàòÎèÑ ÏûàÏäµÎãàÎã§.\n\nÌöåÏÇ¨ Í∏∞Î∞ÄÏù¥ÎÇò Ï§ëÏöîÌïú ÏóÖÎ¨¥ ÎÇ¥Ïö©Ïù¥ Î™®Îç∏Ïóê ÌïôÏäµÎèºÏÑú Í≤ΩÏüÅ ÌöåÏÇ¨Ïóê ÎÑòÏñ¥Í∞à Ïàò ÏûàÏäµÎãàÎã§.\n\nÍ∞úÏù∏Ï†ÅÏù∏ Í≥†ÎØº ÏÉÅÎã¥Ïù¥ÎÇò Í±¥Í∞ï Ï†ïÎ≥¥Í∞Ä ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î°ú Ïì∞Ïùº Ïàò ÏûàÏäµÎãàÎã§.\n\nDeepSeek Î≥¥ÏïàÍ≥º Í∞úÏù∏Ï†ïÎ≥¥ Í≥µÏú†Ïùò ÏúÑÌóòÏÑ±\nDeepSeekÏùÄ Í¥ëÍ≥†ÎÇò Î∂ÑÏÑù ÌååÌä∏ÎÑàÏôÄ ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Î•º Í≥µÏú†Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Îäî DeepSeek Î≥¥ÏïàÏóê Ïã¨Í∞ÅÌïú ÌóàÏ†êÏùÑ ÎßåÎì§ Ïàò ÏûàÏäµÎãàÎã§:\n\nÏö∞Î¶¨Í∞Ä Î≠ò Ï¢ãÏïÑÌïòÎäîÏßÄ, Ïñ¥Îñ§ ÌñâÎèôÏùÑ ÌïòÎäîÏßÄ Í¥ëÍ≥†Ïóê ÌôúÏö©Îê† Ïàò ÏûàÏäµÎãàÎã§.\n\nÎØºÍ∞êÌïú ÎåÄÌôî ÎÇ¥Ïö©Ïù¥ Îã§Î•∏ ÌöåÏÇ¨Ïóê ÎÑòÏñ¥Í∞ÄÏÑú Í¥ëÍ≥†Ïóê Ïì∞Ïùº ÏàòÎèÑ ÏûàÏäµÎãàÎã§.\n\nÍ∞úÏù∏ Ï†ïÎ≥¥Í∞Ä Ïó¨Îü¨ ÌöåÏÇ¨Ïóê Í≥µÏú†ÎêòÎ©¥ÏÑú Ïö∞Î¶¨Ïùò Î™®Îì† Ï†ïÎ≥¥Í∞Ä ÌïòÎÇòÎ°ú Ìï©Ï≥êÏßà ÏàòÎèÑ ÏûàÏäµÎãàÎã§.\n\nÍ≤åÎã§Í∞Ä Ï§ëÍµ≠ Î≤ï ÎïåÎ¨∏Ïóê Î≤ï ÏßëÌñâ Í∏∞Í¥ÄÏù¥ÎÇò Í≥µÍ≥µ Í∏∞Í¥ÄÏóê Ï†ïÎ≥¥Î•º Ï§òÏïº Ìï† ÏùòÎ¨¥ÎèÑ ÏûàÏñ¥ÏÑú Ï†ïÎ≥¥Ïùò ÏïàÏ†ïÏÑ±ÍπåÏßÄ Îã¥Î≥¥ÌïòÏßÄ Î™ªÌï©ÎãàÎã§.\n\nÎÇ¥ Í∂åÎ¶¨Îäî Ïñ¥ÎîîÏóê‚Ä¶? DeepSeek Î≥¥ÏïàÏùò ÏÇ¨Í∞ÅÏßÄÎåÄ\nDeepSeekÏùÄ Ïö∞Î¶¨Í∞Ä ÏÇ¨Îäî ÏßÄÏó≠Ïóê Îî∞ÎùºÏÑú Í∞úÏù∏ Ï†ïÎ≥¥ Î≥¥Ìò∏ Í∂åÎ¶¨ÏôÄ DeepSeek Î≥¥Ïïà Ï†ïÏ±ÖÏùÑ Îã§Î•¥Í≤å Ï†ÅÏö©ÌïòÍ≥† ÏûàÏñ¥Ïöî.\nÏù¥Í±¥ Í∏ÄÎ°úÎ≤å ÏÑúÎπÑÏä§Î°úÏÑúÎäî Ïã¨Í∞ÅÌïú Î¨∏Ï†úÍ∞Ä ÏûàÎäî Í±∞Ï£†.\n\nÏú†ÎüΩÏùÄ Í¥úÏ∞ÆÍ≥†, ÌïúÍµ≠ÏùÄ‚Ä¶?: Ïú†ÎüΩÏóê ÏÇ¨Îäî ÏÇ¨ÎûåÎì§ÏùÄ GDPR ÎçïÎ∂ÑÏóê Í∞úÏù∏ Ï†ïÎ≥¥ Î≥¥Ìò∏Î•º ÎπÑÍµêÏ†Å Ïûò Î∞õÏùÑ Ïàò ÏûàÏßÄÎßå, ÏïÑÏãúÏïÑÎÇò ÏïÑÌîÑÎ¶¨Ïπ¥Ï≤òÎüº Í¥ÄÎ†® Î≤ïÏù¥ ÏïΩÌïú Í≥≥Ïóê ÏÇ¨Îäî ÏÇ¨ÎûåÎì§ÏùÄ Ï†úÎåÄÎ°ú Î≥¥Ìò∏Î∞õÍ∏∞ Ïñ¥Î†µÏäµÎãàÎã§. Ï§ëÍµ≠Ïóê ÏÇ¨Îäî ÏÇ¨ÎûåÎì§ÏùÄ Ï§ëÍµ≠ Î≤ï ÎïåÎ¨∏Ïóê Îçî Ï†úÏïΩÏù¥ ÎßéÏùÑ Í±∞Í≥†Ïöî.\n\nÏÇ≠Ï†úÌï¥Îã¨ÎùºÍ≥† ÌïòÎ©¥ ÏÑúÎπÑÏä§ Ïù¥Ïö© Ï†úÌïú‚Ä¶?: Í∞úÏù∏ Ï†ïÎ≥¥Î•º ÏÇ≠Ï†úÌïòÍ±∞ÎÇò ÏÇ¨Ïö©ÏùÑ ÎßâÏïÑÎã¨ÎùºÍ≥† ÌïòÎ©¥ ÏÑúÎπÑÏä§ Ïù¥Ïö©ÏùÑ Ï†úÌïúÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Í±¥ Ïö∞Î¶¨ Í∂åÎ¶¨Î•º Ìè¨Í∏∞ÌïòÎùºÎäî ÏïïÎ∞ïÏ≤òÎüº ÎäêÍª¥ÏßëÎãàÎã§. ÌäπÌûà DeepSeekÏóê ÏùòÏ°¥ÌïòÎäî ÏÇ¨ÎûåÎì§ÏùÄ ÏûêÍ∏∞ Í∂åÎ¶¨Î•º Ï†úÎåÄÎ°ú ÌñâÏÇ¨ÌïòÍ∏∞ Ïñ¥Î†§Ïö∏ Í≤É Í∞ôÏäµÎãàÎã§.\n\nÎ∂àÎ™ÖÌôïÌïú Ï†ïÎ≥¥ Î≥¥Í¥Ä Í∏∞Í∞Ñ, DeepSeek Î≥¥ÏïàÏùò Îòê Îã§Î•∏ Î¨∏Ï†úÏ†ê\nDeepSeekÏùò Îç∞Ïù¥ÌÑ∞ Î≥¥Í¥Ä Ï†ïÏ±ÖÏóêÎèÑ Î¨∏Ï†úÍ∞Ä ÏûàÏäµÎãàÎã§:\n\n‚ÄúÌïÑÏöîÌïú Í∏∞Í∞Ñ ÎèôÏïà‚Äù?: ÏñºÎßàÎÇò Ïò§Îû´ÎèôÏïà Î≥¥Í¥ÄÌïòÎäîÏßÄ Î™ÖÌôïÌïòÍ≤å ÏïåÎ†§Ï£ºÏßÄ ÏïäÏäµÎãàÎã§. ‚ÄúÌï©Î≤ïÏ†ÅÏù∏ ÏÇ¨ÏóÖÏ†Å Ïù¥Ïùµ‚ÄùÏù¥ÎùºÎäî Ï£ºÍ¥ÄÏ†ÅÏù∏ Í∏∞Ï§ÄÏúºÎ°ú Î≥¥Í¥Ä Í∏∞Í∞ÑÏùÑ Í≤∞Ï†ïÌïúÎã§Í≥† ÌïòÎãà, Ïñ∏Ï†ú ÏÇ≠Ï†úÎêòÎäîÏßÄ Ïïå Ïàò ÏóÜÏäµÎãàÎã§.\n\nÏò§ÎûòÎê†ÏàòÎ°ù ÏúÑÌóòÌïú Ï†ïÎ≥¥: ÏòàÏ†ÑÏóê ÎÇòÎà¥Îçò ÎåÄÌôîÎÇò ÎØºÍ∞êÌïú Ï†ïÎ≥¥Í∞Ä Í≥ÑÏÜç ÏÑúÎ≤ÑÏóê ÎÇ®ÏïÑÏûàÏùÑ ÏàòÎèÑ ÏûàÏäµÎãàÎã§. ÏãúÍ∞ÑÏù¥ ÏßÄÎÇ†ÏàòÎ°ù Ï†ïÎ≥¥Í∞Ä Ïú†Ï∂úÎêòÍ±∞ÎÇò ÏûòÎ™ª ÏÇ¨Ïö©Îê† ÏúÑÌóòÎèÑ Ïª§ÏßÄÎ©∞ ÏûäÌòÄÏßà Í∂åÎ¶¨Î•º Ï†úÎåÄÎ°ú ÌñâÏÇ¨ÌïòÍ∏∞ Ïñ¥Î†§Ïö∏ Ïàò ÏûàÏäµÎãàÎã§.\n\nÏ≤≠ÏÜåÎÖÑ Î≥¥Ìò∏ Ï∏°Î©¥Ïùò DeepSeek Î≥¥Ïïà Ïã§ÌÉú\nDeepSeekÏùÄ Ï≤≠ÏÜåÎÖÑ Î≥¥Ìò∏ Ï†ïÏ±ÖÎèÑ Î≥¥ÏôÑÌï¥Ïïº Ìï† Î∂ÄÎ∂ÑÏù¥ ÎßéÏïÑÏöî:\n\nÎÇòÏù¥ ÌôïÏù∏ÏùÄ Ïñ¥ÎñªÍ≤å‚Ä¶?: 14ÏÑ∏ ÎØ∏ÎßåÏùÄ ÏÇ¨Ïö©ÌïòÎ©¥ Ïïà ÎêúÎã§Í≥† ÌïòÏßÄÎßå, Ïã§Ï†úÎ°ú ÎÇòÏù¥Î•º ÌôïÏù∏ÌïòÎäî Ï†àÏ∞®Í∞Ä ÏóÜÏúºÎ©∞ Í±∞ÏßìÏúºÎ°ú ÏûÖÎ†•Ìï¥ÎèÑ Ïì∏ Ïàò ÏûàÎã§ÎäîÍ≤ÅÎãàÎã§. Î∂ÄÎ™®Îãò ÎèôÏùòÎ•º Î∞õÎäî ÏãúÏä§ÌÖúÎèÑ ÏóÜÏäµÎãàÎã§.\n\nÏ≤≠ÏÜåÎÖÑ ÎßûÏ∂§ Î≥¥Ìò∏Îäî‚Ä¶?: 14~18ÏÑ∏ Ï≤≠ÏÜåÎÖÑÎì§ÏùÑ ÏúÑÌïú ÌäπÎ≥ÑÌïú Î≥¥Ìò∏ Ïû•ÏπòÎèÑ Î∂ÄÏ°±Ìï©ÎãàÎã§. Ï≤≠ÏÜåÎÖÑÏóêÍ≤å Í¥ëÍ≥†Î•º ÌïòÍ±∞ÎÇò Ï†ïÎ≥¥Î•º ÏàòÏßëÌïòÎäî Í±∏ ÎßâÎäî Í∑úÏ†ïÎèÑ ÏóÜÍ≥†, Ïú†Ìï¥Ìïú ÏΩòÌÖêÏ∏†Î•º Í±∏Îü¨ÎÇ¥Îäî ÏãúÏä§ÌÖúÎèÑ Î∂ÄÏ°±Ìï¥ Î≥¥ÏûÖÎãàÎã§.\n\nÍ≤∞Î°†: DeepSeek Î≥¥Ïïà, Í∞úÏÑ†Ïù¥ ÏãúÍ∏âÌïòÎã§\nÏßÄÍ∏àÍπåÏßÄ DeepSeekÏùò Í∞úÏù∏ Ï†ïÎ≥¥ Ï≤òÎ¶¨ Î∞©Ïπ®ÏùÑ ÍººÍººÌïòÍ≤å ÏÇ¥Ìé¥Î¥§ÏúºÎ©∞ Ïó¨Îü¨ Î©¥ÏóêÏÑú Í∞úÏÑ†Ìï¥Ïïº Ìï† Î∂ÄÎ∂ÑÏù¥ ÎßéÎã§Îäî Í≤ÉÏùÑ Ïïå Ïàò ÏûàÏäµÎãàÎã§.\nÌäπÌûà Ï§ëÍµ≠ Î≤ïÏùò Ï†ÅÏö©ÏùÑ Î∞õÎäîÎã§Îäî Ï†ê, ÎÑàÎ¨¥ ÎßéÏùÄ Ï†ïÎ≥¥Î•º ÏàòÏßëÌïúÎã§Îäî Ï†ê, Î∂àÎ∂ÑÎ™ÖÌïú Ï†ïÎ≥¥ ÌôúÏö© Î™©Ï†Å, Ï†úÌïúÏ†ÅÏù∏ ÏÇ¨Ïö©Ïûê Í∂åÎ¶¨ Í∞ôÏùÄ Î¨∏Ï†úÎì§ÏùÄ ÏãúÍ∏âÌïòÍ≤å Ìï¥Í≤∞Ìï¥Ïïº Ìï©ÎãàÎã§.\n\nÏÇ¨Ïö©ÏûêÏûÖÏû•ÏóêÏÑú Ïù¥ Î∂ÄÎ∂ÑÏùÑ ÏïåÏßÄ Î™ªÌïú Ï±ÑÎ°ú DeepSeekÏùò Ï±óÎ¥á ÏãúÏä§ÌÖúÏùò ÏÑ±Îä•Îßå Î≥¥Í≥† Ïù¥Ïö©ÌïúÎã§Î©¥ Î¨∏Ï†úÎê† Ïàò ÏûàÏúºÎãà.. Íº≠ Ïù¥ Î∂ÄÎ∂Ñ Î™ÖÏã¨ÌïòÍ≥† ÏÇ¨Ïö©ÌïòÏãúÍ∏∏ Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§.",
      "summary": "### DeepSeek Î≥¥Ïïà, Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®Ïùò Ïà®Í≤®ÏßÑ ÏúÑÌóòÏÑ± Î∂ÑÏÑù\n\n**Summary:**  \nDeepSeekÎäî Ï§ëÍµ≠Ïóê Î≥∏ÏÇ¨Î•º Îëî Ïù∏Í≥µÏßÄÎä• Ï±óÎ¥áÏúºÎ°ú, ÏÇ¨Ïö©Ïûê Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®Ïóê Ïó¨Îü¨ Í∞ÄÏßÄ Ïö∞Î†§ÏÇ¨Ìï≠Ïù¥ Ï°¥Ïû¨ÌïúÎã§. Ïù¥ Ï±óÎ¥áÏóê ÏûÖÎ†•ÎêòÎäî Î™®Îì† Ï†ïÎ≥¥Îäî Ï§ëÍµ≠ ÏÑúÎ≤ÑÏóê Ï†ÄÏû•ÎêòÎ©∞, Ï§ëÍµ≠ Î≤ïÏùò Ï†ÅÏö©ÏùÑ Î∞õÍ∏∞ ÎïåÎ¨∏Ïóê Ï†ïÎ∂ÄÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º Í¥ëÎ≤îÏúÑÌïòÍ≤å Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÎã§. ÌäπÌûà Ïô∏Íµ≠ ÏÇ¨Ïö©ÏûêÎì§ÏùÄ Î≥¥ÏïàÏù¥ Îçî Ï∑®ÏïΩÌï† Ïàò ÏûàÏúºÎ©∞, Í∏∞ÏóÖÏùò Í∏∞Î∞Ä Ï†ïÎ≥¥Í∞Ä Ï§ëÍµ≠ Ï†ïÎ∂ÄÏóê ÎÖ∏Ï∂úÎê† ÏúÑÌóòÏù¥ ÏûàÎã§. \n\nDeepSeekÏùÄ Í∞úÏù∏ Ï†ïÎ≥¥Î•º Í≥ºÎèÑÌïòÍ≤å ÏàòÏßëÌïòÍ≥†, ÌÇ§Î≥¥Îìú ÏûÖÎ†• Ìå®ÌÑ¥Í≥º Í∞ôÏùÄ ÏÉùÏ≤¥ Ï†ïÎ≥¥ÍπåÏßÄ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏñ¥ Í∑∏ Î™©Ï†ÅÏù¥ Î∂àÎ™ÖÌôïÌïòÎã§. ÏÇ¨Ïö©ÏûêÏùò Ïò®ÎùºÏù∏ ÌñâÎèôÏùÑ Í∞êÏãúÌïòÎäî Í≤ÉÏúºÎ°ú Î≥¥Ïùº Ïàò ÏûàÏúºÎ©∞, ÎåÄÌôî ÎÇ¥Ïö©ÏùÄ AI ÌïôÏäµÏóê ÏÇ¨Ïö©Îê† Ïàò ÏûàÏñ¥ Í∞úÏù∏ Ï†ïÎ≥¥Í∞Ä Ïú†Ï∂úÎê† ÏúÑÌóòÏù¥ ÏûàÎã§. Í¥ëÍ≥† Î∞è Î∂ÑÏÑù ÌååÌä∏ÎÑàÏôÄÏùò Ï†ïÎ≥¥ Í≥µÏú†Î°ú Ïù∏Ìï¥ ÏÇ¨Ïö©Ïûê ÌñâÎèô Îç∞Ïù¥ÌÑ∞Í∞Ä Ïô∏Î∂ÄÎ°ú Ïú†Ï∂úÎê† Ïàò ÏûàÍ≥†, Î≤ïÏ†Å ÏùòÎ¨¥Î°ú Ïù∏Ìï¥ Ï†ïÎ≥¥Ïùò ÏïàÏ†ïÏÑ±Ïù¥ Î≥¥Ïû•ÎêòÏßÄ ÏïäÎäîÎã§.\n\nÍ∞úÏù∏Ï†ïÎ≥¥ Î≥¥Ìò∏ Í∂åÎ¶¨Îäî ÏßÄÏó≠Ïóê Îî∞Îùº Îã§Î•¥Í≤å Ï†ÅÏö©ÎêòÎ©∞, Ïú†ÎüΩÏóêÏÑúÎäî GDPRÎ°ú ÎπÑÍµêÏ†Å Ïûò Î≥¥Ìò∏ÎêòÏßÄÎßå, ÏïÑÏãúÏïÑÏôÄ ÏïÑÌîÑÎ¶¨Ïπ¥ ÏßÄÏó≠ÏóêÏÑúÎäî Î≤ïÏ†Å Î≥¥Ìò∏Í∞Ä Î∂ÄÏ°±ÌïòÎã§. ÏÇ¨Ïö©Ïûê ÏÇ≠Ï†ú ÏöîÏ≤≠ Ïãú ÏÑúÎπÑÏä§ Ïù¥Ïö© Ï†úÌïúÏù¥ ÏûàÏùÑ Ïàò ÏûàÏúºÎ©∞, Ï†ïÎ≥¥ Î≥¥Í¥Ä Í∏∞Í∞ÑÏóê ÎåÄÌïú Î™ÖÌôïÌïú Ï†ïÏ±ÖÏù¥ Î∂ÄÏû¨Ìï¥ Ïò§ÎûòÎêú Ï†ïÎ≥¥Ïùò Î≥¥Ïïà Ïö∞Î†§ÎèÑ ÌÅ¨Îã§. \n\nÎòêÌïú, Ï≤≠ÏÜåÎÖÑ Î≥¥Ìò∏ Ï†ïÏ±ÖÏù¥ ÎØ∏ÎπÑÌïòÏó¨ ÎÇòÏù¥ ÌôïÏù∏ Ï†àÏ∞®Í∞Ä ÏóÜÍ≥†, Ï≤≠ÏÜåÎÖÑÏùÑ ÏúÑÌïú ÌäπÎ≥ÑÌïú Î≥¥Ìò∏ Ïû•ÏπòÎèÑ Î∂ÄÏ°±ÌïòÎã§. Í≤∞Î°†Ï†ÅÏúºÎ°ú, DeepSeekÏùò Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®ÏùÄ Ïó¨Îü¨ Î©¥ÏóêÏÑú Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌïòÎ©∞, ÏÇ¨Ïö©ÏûêÎäî Ïù¥Îü¨Ìïú ÏúÑÌóò ÏöîÏÜåÎ•º Ïù∏ÏßÄÌïòÍ≥† Ïã†Ï§ëÌûà Ïù¥Ïö©Ìï¥Ïïº ÌïúÎã§.",
      "classification": "### DeepSeek Î≥¥Ïïà, Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®Ïùò Ïà®Í≤®ÏßÑ ÏúÑÌóòÏÑ± Î∂ÑÏÑù\n\n**Category:** Updates & Trends",
      "keyword": "### DeepSeek Î≥¥Ïïà, Í∞úÏù∏Ï†ïÎ≥¥ Ï≤òÎ¶¨Î∞©Ïπ®Ïùò Ïà®Í≤®ÏßÑ ÏúÑÌóòÏÑ± Î∂ÑÏÑù\n\n**Keywords:** DeepSeek, Í∞úÏù∏Ï†ïÎ≥¥ Î≥¥Ìò∏, Ï§ëÍµ≠ Î≤ï, Î≥¥Ïïà Ï†ïÏ±Ö, ÏÇ¨Ïö©Ïûê Í∂åÎ¶¨"
    },
    {
      "No.": 12,
      "end_point": "https://blog.bytebytego.com/",
      "post_date": "2025.02.14",
      "link": "https://blog.bytebytego.com/p/non-functional-requirements-the-backbone?utm_source=post-email-title&publication_id=817132&post_id=157015041&utm_campaign=email-post-title&isFreemail=true&r=2bjte3&triedRedirect=true&utm_medium=email",
      "title": "Non-Functional Requirements: The Backbone of Great Software - Part 1",
      "content": "Non-functional requirements (NFRs) are as critical as functional requirements because they define a system's qualities and operational parameters.\n\nWhile functional requirements specify what a software product should do (for example, ‚Äúusers must be able to log in‚Äù), non-functional requirements define how well it must accomplish these tasks under real-world conditions (for example, ‚Äúthe login process should respond within two seconds under peak load‚Äù or ‚Äúall user credentials must be encrypted and stored securely‚Äù).\n\nTogether, functional and non-functional requirements create a foundation for building great software systems. \n\nNFRs are essential for the following reasons:\n\nQuality of Service: NFRs like response time, availability, and usability directly affect the user‚Äôs perception of quality. A system that fulfills its functional requirements but is slow, constantly crashes, or is difficult to use can undermine user trust and satisfaction.\n\nSystem Stability: Requirements such as reliability, fault tolerance, and recoverability help maintain stable operation even when part of the system fails. Without these, unhandled errors can escalate into large-scale outages.\n\nSecurity and Compliance: Security-related NFRs dictate how data is protected, how access is controlled, and how audits are conducted. Neglecting these can lead to breaches, legal consequences, or reputational damage.\n\nScalability and Performance: Requirements for throughput, capacity, and resource utilization ensure the software can handle growth in users or data. If not addressed from the start, scaling can become prohibitively expensive or technically challenging later on.\n\nMaintenance and Evolution: Maintainability, testability, and modularity requirements determine how easily bugs can be fixed, features added, or adaptations made to changing environments. Overlooking them can lead to ballooning technical debt, slowing down future development.\n\nIn short, non-functional requirements are not mere ‚Äúnice-to-haves‚Äù but essential components that ensure a software system truly meets user expectations and withstands real-world challenges. \n\nIn this article (Part 1), we‚Äôll look at the differences between functional and non-functional requirements. Then, we‚Äôll explore the various trade-offs in NFRs and their architectural impact on building systems.\n\n",
      "summary": "### Non-Functional Requirements: The Backbone of Great Software - Part 1\n\n**Summary:** Non-functional requirements (NFRs) are crucial for software development as they define the quality and operational parameters of a system, complementing functional requirements that specify what the software should do. NFRs encompass aspects such as quality of service, system stability, security, scalability, and maintenance, directly influencing user satisfaction, system reliability, and adaptability to growth. These requirements ensure that a software system not only meets functional goals but also performs effectively under real-world conditions. The article will discuss the distinctions between functional and non-functional requirements and examine the trade-offs involved in NFRs, along with their impact on system architecture.",
      "classification": "### Non-Functional Requirements: The Backbone of Great Software - Part 1\n\n**Category:** Updates & Trends",
      "keyword": "### Non-Functional Requirements: The Backbone of Great Software - Part 1\n\n**Keywords:** Non-functional requirements, Quality of Service, System Stability, Security and Compliance, Scalability and Performance"
    },
    {
      "No.": 13,
      "end_point": "https://www.llmwatch.com/",
      "post_date": "2025.02.08",
      "link": "https://www.linkedin.com/pulse/massive-progress-reasoning-models-pascal-biese-abguf/",
      "title": "Massive Progress in Reasoning Models",
      "content": "In this issue:\nBeating OpenAI with Open-Source\n99% performance with only 1% data\nChain-of-Associated-Thoughts (CoAT)\n\n\nFor those of you that enjoy the Linkedin version of LLM Watch, that's great, it's why I publish it here. However, I've heard numerous times by now that people weren't even aware of my Substack - or what Substack is, at all. \n\nYou don't have to use the website or the app itself if you don't want to, you can simply subscribe to me there and get all of my updates straight to your e-mail. This includes additional content that I only publish on Substack.\n\n Click here for the full experience\n\n1. s1: Simple test-time scaling\nWatching: s1 (paper/code)\n\n\nWhat problem does it solve? Current approaches to test-time scaling‚Äîusing additional compute during inference to boost performance‚Äîoften rely on opaque or complex methodologies, as seen in OpenAI‚Äôs proprietary \"o1\" model. This lack of transparency and simplicity hinders reproducibility and practical adoption. The article addresses this gap by proposing an accessible, minimalistic framework for test-time compute scaling, focusing on enhancing model reasoning without intricate architectural changes.\n\nHow does it solve the problem? The authors combined two key innovations: a compact, high-quality dataset (s1K) and a novel \"budget forcing\" mechanism. s1K was curated via rigorous criteria (difficulty, diversity, quality) to maximize fine-tuning efficiency with minimal data. Budget forcing dynamically controls test-time compute by either truncating the model‚Äôs reasoning to limit resources or appending \"Wait\" tokens to prompt self-correction, effectively simulating iterative refinement. The approach was applied via supervised fine-tuning on the Qwen2.5-32B-Instruct model.\n\nWhat are the key findings? The model (s1-32B) outperformed OpenAI‚Äôs o1-preview by up to 27% on competition math benchmarks (MATH, AIME24) and extrapolated beyond its base performance with budget forcing, improving from 50% to 57% on AIME24. The results demonstrate that controlled test-time compute interventions, even with a small dataset, yield significant gains in reasoning tasks. The open-source release of the model, data, and code further underscores reproducibility.\n\nWhy does it matter? This work democratizes test-time scaling by proving its viability through simple, transparent methods‚Äîcontrasting with proprietary \"black-box\" approaches. Budget forcing introduces a lightweight, adaptive mechanism to optimize compute use during reasoning, applicable across domains like education or coding. By open-sourcing their framework, the authors enable broader community adoption and innovation, advancing equitable access to high-performance LLM capabilities.\n\n2. LIMO: Less is More for Reasoning\nWatching: LIMO (paper)\n\n\nWhat problem does it solve? The article challenges the prevailing belief that eliciting complex reasoning‚Äîespecially in mathematical domains‚Äîrequires an enormous amount of training data (often exceeding 100,000 examples). It questions the long-held assumption that large-scale supervised fine-tuning (SFT) is necessary for fostering sophisticated reasoning skills in large language models (LLMs).\n\nHow does it solve the problem? The authors propose a model called LIMO (Less-Is-More Reasoning) that challenges the conventional wisdom. They demonstrate that complex mathematical reasoning abilities can be effectively elicited in LLMs with surprisingly few examples. By using only 817 carefully curated training samples, LIMO achieves remarkable performance on mathematical reasoning tasks, outperforming previous models trained on significantly larger datasets. Key innovations include iterative distillation of examples emphasizing intermediate cognitive steps \"borrowed\" from human solvers, combined with gradient-aware example pruning to maximize template efficiency.\n\nWhat are the key findings? LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, a substantial improvement from previous SFT-based models' 6.5% and 59.2% respectively, while using only 1% of the training data. LIMO also demonstrates exceptional out-of-distribution generalization, achieving a 40.5% absolute improvement across 10 diverse benchmarks, surpassing models trained on 100x more data. These results challenge the notion that SFT leads to memorization rather than generalization.\n\nWhy does it matter? They fundamentally reframe how we approach specialized reasoning in LLMs - not as data-hungry pattern recognition tasks, but as knowledge-unlocking challenges. This reduces computational costs and democratizes development of specialized models. The discovery also offers new perspectives on AI cognition, suggesting foundation models may possess \"latent reasoning muscles\" that require targeted activation rather than brute-force training. For practitioners, it enables mathematical reasoning deployment scenarios where large labeled datasets are unavailable.\n\n3. CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning\nWatching: CoAT (paper)\n\n\nWhat problem does it solve? Current LLM inference predominantly relies on a \"fast thinking\" approach, where models generate outputs in a single pass without iterative refinement. While effective for many tasks, this methodology lacks mechanisms to dynamically integrate new information or revisit earlier reasoning steps‚Äîkey aspects of human-like \"slow thinking.\" This limitation becomes pronounced in complex scenarios requiring adaptability, multi-step reasoning, or incorporation of evolving context.\n\nHow does it solve the problem? The authors introduced Chain-of-Associated-Thoughts (CoAT), blending Monte Carlo Tree Search (MCTS) with a dynamic \"associative memory\" system. MCTS enables structured exploration of diverse reasoning pathways similar to human brainstorming, while associative memory acts as a real-time knowledge repository. This combination allows LLMs to iteratively update their reasoning by retrieving and cross-referencing stored insights, mimicking the human ability to pause, reflect, and refine earlier conclusions.\n\nWhat are the key findings? CoAT significantly outperformed conventional inference methods across generative and reasoning tasks, scoring higher in accuracy, coherence, and output diversity. The framework‚Äôs iterative refinement‚Äîenabled by MCTS-guided exploration and associative memory integration‚Äîproduced outputs better aligned with complex problem-solving requirements. Notably, the system demonstrated strong context retention even as search spaces expanded dynamically.\n\nWhy does it matter? These results address a critical gap in LLM capabilities: the inability to self-correct or incorporate new insights mid-reasoning. By aligning model reasoning closer to human cognitive processes, the CoAT framework paves the way for developing AI systems that are not only more accurate but also better equipped to handle complex, real-world tasks that demand flexibility and contextual adaptability.\n\nPapers of the Week:\nCache Me If You Must: Adaptive Key-Value Quantization for Large Language Models\nThe Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking\nReward-Guided Speculative Decoding for Efficient LLM Reasoning\nJudge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment\nSETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling\nPheromone-based Learning of Optimal Reasoning Paths\nJackpot! Alignment as a Maximal Lottery\nCan We Predict the Effect of Prompts?\nPartially Rewriting a Transformer in Natural Language\nSymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs\nAnalyze Feature Flow to Enhance Interpretation and Steering in Language Models",
      "summary": "### Massive Progress in Reasoning Models\n\n**Summary:** This issue highlights significant advancements in reasoning models, focusing on three key innovations: the use of open-source frameworks to outpace proprietary models, achieving high performance with minimal data, and the introduction of the Chain-of-Associated-Thoughts (CoAT) framework. \n\n1. **Simple Test-Time Scaling:** A new framework addresses the complexity of current test-time scaling methods used during inference. By combining a high-quality, compact dataset (s1K) with a \"budget forcing\" mechanism, researchers demonstrated that controlled compute interventions can enhance model reasoning. The resulting model (s1-32B) outperformed OpenAI‚Äôs o1-preview by up to 27% on math benchmarks, showing that efficient test-time scaling can be achieved with reduced datasets and transparent methodologies, enabling broader community access to high-performance language models.\n\n2. **LIMO (Less is More for Reasoning):** This model challenges the notion that extensive training data is necessary for complex reasoning. By using only 817 carefully selected examples, LIMO achieved significant performance improvements in mathematical reasoning tasks, outperforming previous models that used much larger datasets. This approach not only reduces computational costs but also suggests that large language models may possess inherent reasoning capabilities that can be activated with targeted training.\n\n3. **CoAT (Chain-of-Associated-Thoughts):** This framework enhances reasoning by mimicking human cognitive processes through a blend of Monte Carlo Tree Search (MCTS) and associative memory systems. CoAT allows models to iteratively refine reasoning and adapt to new information, significantly improving accuracy and coherence in generative and problem-solving tasks. This advancement addresses the limitations of traditional \"fast thinking\" approaches in LLMs, paving the way for AI systems capable of more complex, context-aware reasoning.\n\nOverall, these advancements indicate a trend towards democratizing AI capabilities, making sophisticated reasoning accessible and efficient while fostering innovation in the field.",
      "classification": "### Massive Progress in Reasoning Models\n\n**Category:** Updates & Trends",
      "keyword": "### Massive Progress in Reasoning Models\n\n**Keywords:** Open-Source, Test-Time Scaling, LIMO, Chain-of-Associated-Thoughts, Budget Forcing"
    },
    {
      "No.": 14,
      "end_point": "https://the-decoder.com/",
      "post_date": "2025.02.16",
      "link": "https://the-decoder.com/chatgpt-passes-turing-test-for-psychotherapy-study-says/",
      "title": "ChatGPT passes Turing test for psychotherapy, study says",
      "content": "A recent study reveals that people struggle to differentiate between therapeutic responses from ChatGPT and human therapists, with the AI's answers often rated as more empathetic than those from professionals.\r\n\r\nThe classic Turing test, developed by computer science pioneer Alan Turing, measures whether humans can identify if they're interacting with a machine or another person. Researchers recently applied this concept to psychotherapy, asking 830 participants to differentiate between responses from ChatGPT and human therapists.\r\n\r\nAccording to research published in PLOS Mental Health, participants performed only slightly better than random guessing when trying to identify the source of therapeutic responses. They correctly identified human therapist responses 56.1 percent of the time and ChatGPT responses 51.2 percent of the time. The researchers examined 18 couples therapy case studies, comparing responses from 13 experienced therapists against those generated by ChatGPT.\r\n\r\nThe human factor still influences perception\r\nThe study found that ChatGPT's responses actually outperformed human experts in measures of therapeutic quality, scoring higher in therapeutic alliance, empathy, and cultural competence.\r\n\r\nSeveral factors contributed to ChatGPT's strong performance. The AI system consistently produced longer responses with a more positive tone, and used more nouns and adjectives in its answers. These characteristics likely made its responses appear more detailed and empathetic to readers.\r\n\r\nThe research uncovered an important bias: when participants believed they were reading AI-generated responses, they rated them lower - regardless of whether humans or ChatGPT actually wrote them. This bias worked both ways: AI-generated responses received their highest ratings when participants incorrectly attributed them to human therapists.\n\nThe researchers acknowledge important limitations in their work. Their study relied on brief, hypothetical therapy scenarios rather than real therapy sessions. They also question whether their findings from couples therapy would apply equally well to individual counseling.\r\n\r\nStill, as evidence grows for AI's potential benefits in therapeutic settings and its likely future role in mental health care, the researchers emphasize that mental health professionals need to understand these systems. They stress that responsible clinicians must carefully train and monitor AI models to maintain high standards of care.\r\n\r\nGrowing evidence supports AI's therapeutic potential\r\nThis isn't the first study to demonstrate AI's capabilities in advisory roles. Research from the University of Melbourne and the University of Western Australia found that ChatGPT provided more balanced, comprehensive, and empathetic advice on social dilemmas compared to human advice columnists, with preference rates between 70 and 85 percent.",
      "summary": "### ChatGPT passes Turing test for psychotherapy, study says\n\n**Summary:** A recent study published in PLOS Mental Health indicates that participants struggle to distinguish between therapeutic responses from ChatGPT and human therapists, often rating the AI's responses as more empathetic. Conducted with 830 participants, the study applied the Turing test to psychotherapy, revealing that participants identified human responses only 56.1% of the time and ChatGPT responses 51.2% of the time. ChatGPT outperformed human therapists in therapeutic quality, particularly in therapeutic alliance, empathy, and cultural competence, likely due to its longer, positively toned responses filled with descriptive language. Additionally, a bias was identified where AI-generated responses were rated lower when participants believed they were from a machine, while those attributed to human therapists received higher ratings. The researchers noted limitations, such as the use of hypothetical therapy scenarios rather than real sessions, and questioned the applicability of findings to individual counseling. Despite these concerns, the study supports the growing evidence of AI's potential in therapeutic contexts, highlighting the necessity for mental health professionals to understand and monitor AI systems to uphold care standards.",
      "classification": "### ChatGPT passes Turing test for psychotherapy, study says\n\n**Category:** Research Paper",
      "keyword": "### ChatGPT passes Turing test for psychotherapy, study says\n\n**Keywords:** Turing test, psychotherapy, therapeutic alliance, AI empathy, human bias"
    },
    {
      "No.": 15,
      "end_point": "https://techcrunch.com/",
      "post_date": "2025.02.16",
      "link": "https://techcrunch.com/2025/02/16/openai-tries-to-uncensor-chatgpt/",
      "title": "OpenAI tries to ‚Äòuncensor‚Äô ChatGPT",
      "content": "OpenAI is changing how it trains AI models to explicitly embrace ‚Äúintellectual freedom ‚Ä¶ no matter how challenging or controversial a topic may be,‚Äù the company says in a new policy.\r\n\r\nAs a result, ChatGPT will eventually be able to answer more questions, offer more perspectives, and reduce the number of topics the AI chatbot won‚Äôt talk about.\r\n\r\nThe changes might be part of OpenAI‚Äôs effort to land in the good graces of the new Trump administration, but it also seems to be part of a broader shift in Silicon Valley and what‚Äôs considered ‚ÄúAI safety.‚Äù\r\n\r\nOn Wednesday, OpenAI announced an update to its Model Spec, a 187-page document that lays out how the company trains AI models to behave. In it, OpenAI unveiled a new guiding principle: Do not lie, either by making untrue statements or by omitting important context.\r\n\r\nIn a new section called ‚ÄúSeek the truth together,‚Äù OpenAI says it wants ChatGPT to not take an editorial stance, even if some users find that morally wrong or offensive. That means ChatGPT will offer multiple perspectives on controversial subjects, all in an effort to be neutral.\r\n\r\nFor example, the company says ChatGPT should assert that ‚ÄúBlack lives matter,‚Äù but also that ‚Äúall lives matter.‚Äù Instead of refusing to answer or picking a side on political issues, OpenAI says it wants ChatGPT to affirm its ‚Äúlove for humanity‚Äù generally, then offer context about each movement.\r\n\r\n‚ÄúThis principle may be controversial, as it means the assistant may remain neutral on topics some consider morally wrong or offensive,‚Äù OpenAI says in the spec. ‚ÄúHowever, the goal of an AI assistant is to assist humanity, not to shape it.‚Äù\n\nThe new Model Spec doesn‚Äôt mean that ChatGPT is a total free-for-all now. The chatbot will still refuse to answer certain objectionable questions or respond in a way that supports blatant falsehoods.\r\n\r\nThese changes could be seen as a response to conservative criticism about ChatGPT‚Äôs safeguards, which have always seemed to skew center-left. However, an OpenAI spokesperson rejects the idea that it was making changes to appease the Trump administration.\r\n\r\nInstead, the company says its embrace of intellectual freedom reflects OpenAI‚Äôs ‚Äúlong-held belief in giving users more control.‚Äù\r\n\r\nBut not everyone sees it that way.\r\n\r\nConservatives claim AI censorship\n\nTrump‚Äôs closest Silicon Valley confidants ‚Äî including David Sacks, Marc Andreessen, and Elon Musk ‚Äî have all accused OpenAI of engaging in deliberate AI censorship over the last several months. We wrote in December that Trump‚Äôs crew was setting the stage for AI censorship to be a next culture war issue within Silicon Valley.\r\n\r\nOf course, OpenAI doesn‚Äôt say it engaged in ‚Äúcensorship,‚Äù as Trump‚Äôs advisers claim. Rather, the company‚Äôs CEO, Sam Altman, previously claimed in a post on X that ChatGPT‚Äôs bias was an unfortunate ‚Äúshortcoming‚Äù that the company was working to fix, though he noted it would take some time.\r\n\r\nAltman made that comment just after a viral tweet circulated in which ChatGPT refused to write a poem praising Trump, though it would perform the action for Joe Biden. Many conservatives pointed to this as an example of AI censorship.\r\n\r\nWhile it‚Äôs impossible to say whether OpenAI was truly suppressing certain points of view, it‚Äôs a sheer fact that AI chatbots lean left across the board.\r\n\r\nEven Elon Musk admits xAI‚Äôs chatbot is often more politically correct than he‚Äôd like. It‚Äôs not because Grok was ‚Äúprogrammed to be woke‚Äù but more likely a reality of training AI on the open internet. \r\n\r\nNevertheless, OpenAI now says it‚Äôs doubling down on free speech. This week, the company even removed warnings from ChatGPT that tell users when they‚Äôve violated its policies. OpenAI told TechCrunch this was purely a cosmetic change, with no change to the model‚Äôs outputs.\r\n\r\nThe company seems to want ChatGPT to feel less censored for users.\r\n\r\nIt wouldn‚Äôt be surprising if OpenAI was also trying to impress the new Trump administration with this policy update, notes former OpenAI policy leader Miles Brundage in a post on X.\r\n\r\nTrump has previously targeted Silicon Valley companies, such as Twitter and Meta, for having active content moderation teams that tend to shut out conservative voices.\r\n\r\nOpenAI may be trying to get out in front of that. But there‚Äôs also a larger shift going on in Silicon Valley and the AI world about the role of content moderation.\r\n\r\nGenerating answers to please everyone\n\nNewsrooms, social media platforms, and search companies have historically struggled to deliver information to their audiences in a way that feels objective, accurate, and entertaining.\r\n\r\nNow, AI chatbot providers are in the same delivery information business, but arguably with the hardest version of this problem yet: How do they automatically generate answers to any question?\r\n\r\nDelivering information about controversial, real-time events is a constantly moving target, and it involves taking editorial stances, even if tech companies don‚Äôt like to admit it. Those stances are bound to upset someone, miss some group‚Äôs perspective, or give too much air to some political party.\r\n\r\nFor example, when OpenAI commits to let ChatGPT represent all perspectives on controversial subjects ‚Äî including conspiracy theories, racist or antisemitic movements, or geopolitical conflicts ‚Äî that is inherently an editorial stance.\r\n\r\nSome, including OpenAI co-founder John Schulman, argue that it‚Äôs the right stance for ChatGPT. The alternative ‚Äî doing a cost-benefit analysis to determine whether an AI chatbot should answer a user‚Äôs question ‚Äî could ‚Äúgive the platform too much moral authority,‚Äù Schulman notes in a post on X.\r\n\r\nSchulman isn‚Äôt alone. ‚ÄúI think OpenAI is right to push in the direction of more speech,‚Äù said Dean Ball, a research fellow at George Mason University‚Äôs Mercatus Center, in an interview with TechCrunch. ‚ÄúAs AI models become smarter and more vital to the way people learn about the world, these decisions just become more important.‚Äù\r\n\r\nIn previous years, AI model providers have tried to stop their AI chatbots from answering questions that might lead to ‚Äúunsafe‚Äù answers. Almost every AI company stopped their AI chatbot from answering questions about the 2024 election for U.S. president. This was widely considered a safe and responsible decision at the time.\r\n\r\nBut OpenAI‚Äôs changes to its Model Spec suggest we may be entering a new era for what ‚ÄúAI safety‚Äù really means, in which allowing an AI model to answer anything and everything is considered more responsible than making decisions for users.\r\n\r\nBall says this is partially because AI models are just better now. OpenAI has made significant progress on AI model alignment; its latest reasoning models think about the company‚Äôs AI safety policy before answering. This allows AI models to give better answers for delicate questions.\r\n\r\nOf course, Elon Musk was the first to implement ‚Äúfree speech‚Äù into xAI‚Äôs Grok chatbot, perhaps before the company was really ready to handle sensitive questions. It still might be too soon for leading AI models, but now, others are embracing the same idea.\r\n\r\nShifting values for Silicon Valley\n\nMark Zuckerberg made waves last month by reorienting Meta‚Äôs businesses around First Amendment principles. He praised Elon Musk in the process, saying the owner of X took the right approach by using Community Notes ‚Äî a community-driven content moderation program ‚Äî to safeguard free speech.\r\n\r\nIn practice, both X and Meta ended up dismantling their longstanding trust and safety teams, allowing more controversial posts on their platforms and amplifying conservative voices.\r\n\r\nChanges at X may have hurt its relationships with advertisers, but that could have more to do with Musk, who has taken the unusual step of suing some of them for boycotting the platform. Early signs indicate that Meta‚Äôs advertisers were unfazed by Zuckerberg‚Äôs free speech pivot.\r\n\r\nMeanwhile, many tech companies beyond X and Meta have walked back from left-leaning policies that dominated Silicon Valley for the last several decades. Google, Amazon, and Intel have eliminated or scaled back diversity initiatives in the last year.\r\n\r\nOpenAI may be reversing course, too. The ChatGPT-maker seems to have recently scrubbed a commitment to diversity, equity, and inclusion from its website.\r\n\r\nAs OpenAI embarks on one of the largest American infrastructure projects ever with Stargate, a $500 billion AI datacenter, its relationship with the Trump administration is increasingly important. At the same time, the ChatGPT maker is vying to unseat Google Search as the dominant source of information on the internet.\r\n\r\nComing up with the right answers may prove key to both.",
      "summary": "### OpenAI tries to ‚Äòuncensor‚Äô ChatGPT\n\n**Summary:** OpenAI is revising its approach to AI model training, promoting a policy of \"intellectual freedom\" that aims to allow ChatGPT to address a broader range of topics, including controversial ones. This shift, outlined in an updated Model Spec, emphasizes truthfulness and neutrality, encouraging the AI to present multiple perspectives without taking an editorial stance, even on sensitive issues. OpenAI's new principle advocates for the assistant to affirm its \"love for humanity\" while acknowledging diverse viewpoints, such as both \"Black lives matter\" and \"all lives matter.\"\n\nThe changes may be influenced by conservative criticism of perceived biases in ChatGPT, although OpenAI denies that it is changing its policies to appease the new Trump administration. Despite the push for more free speech, ChatGPT will still avoid responding to blatantly false information or highly objectionable queries.\n\nThe debate surrounding AI censorship and bias continues, with critics alleging that OpenAI has favored left-leaning perspectives. Notably, the company has recently removed warnings from ChatGPT about policy violations, aiming to reduce the perception of censorship.\n\nAs Silicon Valley grapples with content moderation issues, AI models like ChatGPT face the challenge of providing balanced information on contentious topics. Some OpenAI co-founders advocate for more open dialogue, arguing that prioritizing free speech is essential as AI becomes more integral to information consumption.\n\nAdditionally, OpenAI's move aligns with a broader trend in the tech industry, where companies like Meta and X are embracing free speech principles and scaling back previous diversity initiatives. OpenAI's evolving policies may be critical as it seeks to establish a stronger position in the competitive landscape of AI and information delivery, especially against major players like Google.",
      "classification": "### OpenAI tries to ‚Äòuncensor‚Äô ChatGPT\n\n**Category:** Updates & Trends",
      "keyword": "### OpenAI tries to ‚Äòuncensor‚Äô ChatGPT\n\n**Keywords:** intellectual freedom, AI censorship, Model Spec, content moderation, political neutrality"
    },
    {
      "No.": 16,
      "end_point": "https://medium.com",
      "post_date": null,
      "link": "https://medium.com/towards-data-science/deepseek-v3-explained-1-multi-head-latent-attention-ed6bee2a67c4",
      "title": "DeepSeek-V3 Explained 1: Multi-head Latent Attention",
      "content": "This is the first article of our new series ‚ÄúDeepSeek-V3 Explained‚Äù, where we will try to demystify DeepSeek-V3 [1, 2], the latest model open-sourced by DeepSeek.\n\nIn this series, we aim to cover two major topics:\n\nMajor architecture innovations in DeepSeek-V3, including MLA (Multi-head Latent Attention) [3], DeepSeekMoE [4], auxiliary-loss-free load balancing [5], and multi-token prediction training.\nTraining of DeepSeek-V3, including pre-training, finetuning and RL alignment phases.\nThis article mainly focuses on Multi-head Latent Attention, which was first proposed in the development of DeepSeek-V2 and then used in DeepSeek-V3 as well.\n\nHere is the link to other articles in this series:\n\nDeepSeek-V3 Explained 2: DeepSeekMoE\nTable of contents:\n\nBackground: we start from the standard MHA, explain why we need Key-Value cache at inference stage, how MQA and GQA try to optimize it, and how RoPE works, etc.\nMulti-head Latent Attention: An in-depth introduction to MLA, including its motivations, why decoupled RoPE is needed, and its performance.\nReferences.\nBackground\nTo better understand MLA and also make this article self-contained, we will revisit several related concepts in this section before diving into the details of MLA.\n\nMHA in Decoder-only Transformers\nNote that MLA is developed to speedup inference speed in autoregressive text generation, so the MHA we are talking about under this context is for decoder-only Transformer.\n\nThe figure below compares three Transformer architectures used for decoding, where (a) shows both the encoder and decoder proposed in the original ‚ÄúAttention is All You Need‚Äù paper. Its decoder part is then simplified by [6], leading to a decoder-only Transformer model shown in (b), which is later used in many generation models like GPT [8].\n\nNowadays, LLMs are more commonly to choose the structure shown in (c) for more stable training, with normalization applied on the input rather then output, and LayerNorm upgraded to RMS Norm. This will serve as the baseline architecture we will discuss in this article.\n\n\nFigure 1. Transformer architectures. (a) encoder-decoder proposed in [6]. (b) Decoder-only Transformer proposed in [7] and used in GPT [8]. (c) An optimized version of (b) with RMS Norm before attention. [3]\nWithin this context, MHA calculation largely follows the process in [6], as shown in the figure below:\n\n\nFigure 2. Scaled dot-product attention vs. Multi-Head Attention. Image from [6].\nAssume we have n_h attention heads, and the dimension for each attention head is represented as d_h, so that the concatenated dimension will be (n_h ¬∑ d_h).\n\nGiven a model with l layers, if we denote the input for the t-th token in that layer as h_t with dimension d, we need to map the dimension of h_t from d to (h_n ¬∑ d_h) using the linear mapping matrices.\n\nMore formally, we have (equations from [3]):\n\n\nwhere W^Q, W^K and W^V are the linear mapping matrices:\n\n\nAfter such mapping, q_t, k_t and v_t will be split into n_h heads to calculate the scaled dot-product attention:\n\n\nwhere W^O is another projection matrix to map the dimension inversely from (h_n ¬∑ d_h) to d:\n\n\nNote that the process described by Eqn.(1) to (8) above is just for a single token. During inference, we need to repeat this process for each newly generated token, which involves a lot of repeated calculation. This leads to a technique called Key-Value cache.\n\nKey-Value Cache\nAs suggested by its name, Key-Value cache is a technique designed to speedup the autoregressive process by caching and reusing the previous keys and values, rather than re-computing them at each decoding step.\n\nNote that KV cache is typically used only during the inference stage, since in training we still need to process the entire input sequence in parallel.\n\nKV cache is commonly implemented as a rolling buffer. At each decoding step, only the new query Q is computed, while the K and V stored in the cache will be reused, so that the attention will be computed using the new Q and reused K, V. Meanwhile, the new token‚Äôs K and V will also be appended to the cache for later use.\n\nHowever, the speedup achieved by KV cache comes at a cost of memory, since KV cache often scales with batch size √ó sequence length √ó hidden size √ó number of heads, leading to a memory bottleneck when we have larger batch size or longer sequences.\n\nThat further leads to two techniques aiming at addressing this limitation: Multi-Query Attention and Grouped-Query Attention.\n\nMulti-Query Attention (MQA) vs Grouped-Query Attention (GQA)\nThe figure below shows the comparison between the original MHA, Grouped-Query Attention (GQA) [10] and Multi-Query Attention (MQA) [9].\n\n\nFigure 3. MHA [6], GQA [10] AND MQA [9]. Image from [10].\nThe basic idea of MQA is to share a single key and a single value head across all query heads, which can significantly reduce memory usage but will also impact the accuracy of attention.\n\nGQA can be seen as an interpolating method between MHA and MQA, where a single pair of key and value heads will be shared only by a group of query heads, not all queries. But still this will lead to inferior results compared to MHA.\n\nIn the later sections, we will see how MLA manages to seek a balance between memory efficiency and modeling accuracy.\n\nRoPE (Rotary Positional Embeddings)\nOne last piece of background we need to mention is RoPE [11], which encodes positional information directly into the attention mechanism by rotating the query and key vectors in multi-head attention using sinusoidal functions.\n\nMore specifically, RoPE applies a position-dependent rotation matrix to the query and key vectors at each token, and uses sine and cosine functions for its basis but applies them in a unique way to achieve rotation.\n\nTo see what makes it position-dependent, consider a toy embedding vector with only 4 elements, i.e., (x_1, x_2, x_3, x_4).\n\nTo apply RoPE, we firstly group consecutive dimensions into pairs:\n\n(x_1, x_2) -> position 1\n(x_3, x_4) -> position 2\nThen, we apply a rotation matrix to rotate each pair:\n\n\nFigure 4. Illustration of the rotation matrix applied to a pair of tokens. Image by author.\nwhere Œ∏ = Œ∏(p) = p ‚ãÖ Œ∏_0‚Äã, and Œ∏_0‚Äã is a base frequency. In our 4-d toy example, this means that (x_1, x_2) will be rotated by Œ∏_0‚Äã, and (x_3, x_4) will be rotated by 2 ‚ãÖ Œ∏_0.\n\nThis is why we call this rotation matrix as position-dependent: at each position (or each pair), we will apply a different rotation matrix where the rotation angle is determined by position.\n\nRoPE is widely used in modern LLMs due to its efficiency in encoding long sequences, but as we can see from the above formula, it is position-sensitive to both Q and K, making it incompatible with MLA in some ways.\n\nMulti-head Latent Attention\nFinally we can move on to the MLA part. In this section we will first layout the high-level idea of MLA, and then dive deeper into why it needs to modify RoPE. Finally, we present the detailed algorithm of MLA as well as it performance.\n\nMLA: High-level Idea\nThe basic idea of MLA is to compress the attention input h_t into a low-dimensional latent vector with dimension d_c, where d_c is much lower than the original (h_n ¬∑ d_h). Later when we need to calculate attention, we can map this latent vector back to the high-dimensional space to recover the keys and values. As a result, only the latent vector needs to be stored, leading to significant memory reduction.\n\nThis process can be more formally described with the following equations, where c^{KV}_t is the latent vector, W^{DKV} is the compressing matrix that maps h_t‚Äôs dimension from (h_n ¬∑ d_h) to d_c (here D in the superscript stands for ‚Äúdown-projection‚Äù, meaning compressing the dimension), while W^{UK} and W^{UV} are both up-projection matrices that map the shared latent vector back to the high-dimensional space.\n\n\nSimilarly, we can also map the queries into a latent, low-dimensional vector and then map it back to the original, high-dimensional space:\n\n\nWhy Decoupled RoPE is Needed\nAs we mentioned before, RoPE is a common choice for training generation models to handle long sequences. If we directly apply the above MLA strategy, that will be incompatible with RoPE.\n\nTo see this more clearly, consider what happens when we calculate attention using Eqn. (7): when we multiply the transposed q with k, the matrices W^Q and W^{UK} will appear in the middle, and their combination equivalents to a single mapping dimension from d_c to d.\n\nIn the original paper [3], the authors describe this as W^{UK} can be ‚Äúabsorbed‚Äù into W^Q, as a result we do not need to store W^{UK} in the cache, further reducing memory usage.\n\nHowever, this will not be the case when we take the rotation matrix in Figure (4) into consideration, as RoPE will apply a rotation matrix on the left of W^{UK}, and this rotation matrix will end up in between the transposed W^Q and W^{UK}.\n\nAs we have explained in the background part, this rotation matrix is position-dependent, meaning the rotation matrix for each position is different. As a result, W^{UK} can no longer be absorbed by W^Q.\n\nTo address this conflict, the authors propose what they call ‚Äúa decoupled RoPE‚Äù, by introducing additional query vectors along with a shared key vector, and use these additional vectors in the RoPE process only, while keeping the original keys kind of isolated with the rotation matrix.\n\nThe entire process of MLA can be summarized as below (equation numbers reused from Appendix C in [3]):\n\n\nFigure 5. MLA process. Image edited by author based on equations in [3].\nwhere\n\nEqn. (37) to (40) describe how to process query tokens.\nEqn. (41) and (42) describe how to process key tokens.\nEqn. (43) and (44) describe how to use the additional shared key for RoPE, be aware that the output of (42) is not involved in RoPE.\nEqn. (45) describes how to process value tokens.\nDuring this process, only the blue variables with boxes need to be cached. This process can be illustrated more clearly with the flowchart blow:\n\n\nFigure 6. Flowchart of MLA. Image from [3].\nPerformance of MLA\nThe table below compares the number of elements needed for KV cache (per token) as well as the modeling capacity between MHA, GQA, MQA and MLA, demonstrating that MLA could indeed achieve a better balance between memory efficiency vs. modeling capacity.\n\nInterestingly, the modeling capacity for MLA even surpass that of the original MHA.\n\n\nTable 1 from [3].\nMore specifically, the table below shows the performance of MHA, GQA and MQA on 7B models, where MHA significantly outperforms both MQA and GQA.\n\n\nTable 8 from [3].\nThe authors of [3] also conduct analysis between MHA vs. MLA, and results are summarized in the table below, where MLA achieves better results overall.\n\n\nTable 9 in [3].\nReferences\n[1] DeepSeek\n[2] DeepSeek-V3 Technical Report\n[3] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\n[4] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\n[5] Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts\n[6] Attention Is All You Need\n[7] Generating Wikipedia by Summarizing Long Sequences\n[8] Improving Language Understanding by Generative Pre-Training\n[9] Fast Transformer Decoding: One Write-Head is All You Need\n[10] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\n[11] RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "summary": "### DeepSeek-V3 Explained 1: Multi-head Latent Attention\n\n**Summary:** This article is the first in the \"DeepSeek-V3 Explained\" series, aimed at clarifying the latest model from DeepSeek, focusing on two main topics: architectural innovations and training methods. The primary focus of this article is on Multi-head Latent Attention (MLA), an enhancement from DeepSeek-V2. It provides background on Multi-Head Attention (MHA) and Key-Value (KV) caching techniques that improve inference speed in autoregressive text generation. The article discusses various attention mechanisms, including Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), highlighting their trade-offs between memory efficiency and accuracy. \n\nThe article also introduces Rotary Positional Embeddings (RoPE) and explains its integration challenges with MLA. The core concept of MLA involves compressing attention inputs into a lower-dimensional latent vector, which reduces memory usage while maintaining performance. A modified version of RoPE is proposed to accommodate MLA's structure, allowing for efficient computation without compromising the attention mechanism. Performance comparisons demonstrate that MLA achieves a favorable balance between memory efficiency and modeling capacity, outperforming both MHA and GQA in various metrics. \n\nThe article concludes with references to further readings and articles in the series, setting the stage for subsequent discussions on other innovations in DeepSeek-V3.",
      "classification": "### DeepSeek-V3 Explained 1: Multi-head Latent Attention\n\n**Category:** Model",
      "keyword": "### DeepSeek-V3 Explained 1: Multi-head Latent Attention\n\n**Keywords:** Multi-head Latent Attention, Key-Value Cache, Rotary Positional Embeddings, Memory Efficiency, Transformer Architecture"
    },
    {
      "No.": 17,
      "end_point": "https://medium.com",
      "post_date": null,
      "link": "https://medium.com/@isaakmwangi2018/a-simple-guide-to-deepseek-r1-architecture-training-local-deployment-and-hardware-requirements-300c87991126",
      "title": "A Simple Guide to DeepSeek R1: Architecture, Training, Local Deployment, and Hardware Requirements",
      "content": "DeepSeek‚Äôs Novel Approach to LLM Reasoning\n\nDeepSeek has introduced an innovative approach to improving the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL), detailed in their recent paper on DeepSeek-R1. This research represents a significant advancement in how we can enhance LLMs‚Äô ability to solve complex problems through pure reinforcement learning, without relying heavily on supervised fine-tuning.\n\nBefore we proceed if you like this topic and you want to support me:\n\nClap my article 10 times; that will help me out.üëè\nFollow me on Medium to get my latest articles ü´∂\nTechnical Overview of DeepSeek-R1\n\nModel Architecture:\n\nDeepSeek-R1 is not a singular model but a family of models, encompassing: DeepSeek-R1-Zero and DeepSeek-R1\n\nLet me clarify the key differences between DeepSeek-R1 and DeepSeek-R1-Zero:\n\nThe Primary Distinction\n\nDeepSeek-R1-Zero represents the team‚Äôs initial experiment using pure reinforcement learning without any supervised fine-tuning. They started with their base model and applied reinforcement learning directly, letting the model develop reasoning capabilities through trial and error. While this approach achieved impressive results (71% accuracy on AIME 2024), it had some significant limitations, particularly in readability and language consistency. It features 671 billion parameters, utilizing a mixture-of-experts (MoE) architecture where each token activates parameters equivalent to 37 billion. This model showcases emergent reasoning behaviors, such as self-verification, reflection, and long chain-of-thought (CoT) reasoning.\n\nDeepSeek-R1, in contrast, uses a more sophisticated multi-stage training approach. Instead of pure reinforcement learning, it begins with supervised fine-tuning on a small set of carefully curated examples (called ‚Äúcold-start data‚Äù) before applying reinforcement learning. This approach addresses the limitations of DeepSeek-R1-Zero while achieving even better performance. This model also maintains the 671 billion parameter count but achieves better readability and coherence in responses.\n\nThe Training Process Comparison\nTraining Methodology:\n\nReinforcement Learning: Unlike traditional models that predominantly rely on supervised learning, DeepSeek-R1 uses RL extensively. The training leverages group relative policy optimization (GRPO), focusing on accuracy and format rewards to enhance reasoning capabilities without the need for extensive labeled data.\nDistillation Techniques: To democratize access to high-performing models, DeepSeek has also released distilled versions of R1, ranging from 1.5 billion to 70 billion parameters. These models are based on architectures like Qwen and Llama, showing that complex reasoning can be encapsulated in smaller, more efficient models. The distillation process involves fine-tuning these smaller models with synthetic reasoning data generated by the full DeepSeek-R1, thus preserving high performance at reduced computational cost.\nDeepSeek-R1-Zero‚Äôs training process is straightforward:\n\nStart with base model\nApply reinforcement learning directly\nUse simple rewards based on accuracy and format\nDeepSeek-R1‚Äôs training process has four distinct stages:\n\nInitial supervised fine-tuning with thousands of high-quality examples\nReinforcement learning focused on reasoning tasks\nCollection of new training data through rejection sampling\nFinal reinforcement learning across all types of tasks\nPerformance Metrics:\n\nReasoning Benchmarks: DeepSeek-R1 has shown impressive results on various benchmarks:\nAIME 2024: Achieved a 79.8% pass rate, compared to 79.2% by OpenAI‚Äôs o1‚Äì1217.\nMATH-500: Scored an impressive 97.3%, slightly ahead of o1‚Äì1217‚Äôs 96.4%.\nSWE-bench Verified: Outperformed in programming tasks, showcasing its coding proficiency.\nCost Efficiency: The API for DeepSeek-R1 is priced at $0.14 per million input tokens for cache hits, making it significantly cheaper than comparable models like OpenAI‚Äôs o1.\nLimitations and Future Work\n\nThe paper acknowledges several areas for improvement:\n\nThe model sometimes struggles with tasks requiring specific output formats\nPerformance on software engineering tasks could be enhanced\nThere are challenges with language mixing in multilingual contexts\nFew-shot prompting consistently degrades performance\nFuture work will focus on addressing these limitations and expanding the model‚Äôs capabilities in areas like function calling, multi-turn interactions, and complex role-playing scenarios.\n\nDeployment and Accessibility\nOpen Source and Licensing: DeepSeek-R1 and its variants are released under the MIT License, promoting open-source collaboration and commercial use, including model distillation. This move is pivotal for fostering innovation and reducing the entry barriers in AI model development.\nModel Formats:\nBoth models and their distilled versions are available in formats like GGML, GGUF, GPTQ, and HF, allowing flexibility in how they are deployed locally.\n1. Web Access via DeepSeek Chat Platform:\nThe DeepSeek Chat platform provides a user-friendly interface to interact with DeepSeek-R1 without any setup requirements.\n\nSteps to Access:\nNavigate to the DeepSeek Chat platform\nRegister for an account or log in if you already have one.\nAfter logging in, select the ‚ÄúDeep Think‚Äù mode to experience DeepSeek-R1‚Äôs step-by-step reasoning capabilities.\n\nDeepSeek Chat Platform\n2. Access via DeepSeek API:\nFor programmatic access, DeepSeek offers an API compatible with OpenAI‚Äôs format, allowing integration into various applications.\n\nSteps to Use the API:\n\na. Obtain an API Key:\n\nVisit the DeepSeek API platform to create an account and generate your unique API key.\nb. Configure Your Environment:\n\nSet the base_url to https://api.deepseek.com/v1.\nUse your API key for authentication, typically via Bearer Token in the HTTP header.\nc. Make API Calls:\n\nUtilize the API to send prompts and receive responses from DeepSeek-R1.\nDetailed documentation and examples are available in the DeepSeek API Docs.\n\nDeepSeek API call example\n3. Running DeepSeek-R1 Locally:\nBoth Models (R1 and R1-Zero):\n\nHardware Requirements: The full models require significant hardware due to their size. A GPU with substantial VRAM (like Nvidia RTX 3090 or higher) is recommended. For CPU use, you‚Äôd need at least 48GB of RAM and 250GB of disk space, although performance would be slow without GPU acceleration.\nDistilled Models: For local deployment with less resource-intensive hardware, DeepSeek provides distilled versions. These range from 1.5B to 70B parameters, making them suitable for systems with more modest hardware. For instance, the 7B model can run on a GPU with at least 6GB VRAM or on a CPU with about 4GB RAM for the GGML/GGUF format.\nSoftware Tools for Local Running:\n\nOllama:\nYou can use Ollama to serve the models locally: (Ollama Is a tool for running open-source AI models locally on your machine. Grab it here: https://ollama.com/download)\n\n\nNext, you‚Äôll need to pull and run the DeepSeek R1 model locally.\n\nOllama offers different model sizes ‚Äî basically, bigger models equal to smarter AI, but need better GPU. Here‚Äôs the lineup:\n\n1.5B version (smallest):\nollama run deepseek-r1:1.5b\n\n8B version:\nollama run deepseek-r1:8b\n\n14B version:\nollama run deepseek-r1:14b\n\n32B version:\nollama run deepseek-r1:32b\n\n70B version (biggest/smartest):\nollama run deepseek-r1:70b\nTo begin experimenting with DeepSeek-R1, it is advisable to start with a smaller model to familiarize yourself with the setup and ensure compatibility with your hardware. You can initiate this process by opening your terminal and executing the following command:\n\nollama run deepseek-r1:8b\n\nImage courtesy from Reddit, via r/macapps\nSending Requests to locally downloaded DeepSeek-R1 via Ollama:\n\nOllama provides an API endpoint to interact with DeepSeek-R1 programmatically. Ensure that the Ollama server is running locally before making API requests. You can start the server by running:\n\nollama serve\nOnce the server is active, you can send a request using curl as follows:\n\ncurl -X POST http://localhost:11434/api/generate -d '{\n  \"model\": \"deepseek-r1\",\n  \"prompt\": \"Your question or prompt here\"\n}'\nReplace \"Your question or prompt here\" with the actual input you wish to provide to the model. This command sends a POST request to the local Ollama server, which processes the prompt using the specified DeepSeek-R1 model and returns the generated response.\n\nOther methods to run/Access the models locally are:\nvLLM/SGLang: Used for serving the models locally. Commands like vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B ‚Äî tensor-parallel-size 2 ‚Äî max-model-len 32768 ‚Äî enforce-eager can be used for the distilled versions.\n\n\nCourtesy: HuggingFace\nllama.cpp: You can also use llama.cpp to run the models locally.\n\nSee What Others Are Building with DeepSeek-R1:\nRunning DeepSeek R1 across my 7 M4 Pro Mac Minis and 1 M4 Max MacBook Pro:\n\nDeepSeek R1 1.5B running fully locally in your browser at 60 tok/ sec powered by WebGPU:\n\n\nRAG app to chat with your PDF files using the DeepSeek R1 model, running locally on your computer.\n\n\nRunning DeepSeek R1 version 1.5B perfectly locally on phone:\n\n\nCracking complex math problems with ease! (Thought for ~3200 tokens in about 35 seconds on M4 Max with mlx-lm.):\n\n\nConclusions:\n\nThis progression from DeepSeek-R1-Zero to DeepSeek-R1 represents an important learning journey in the research. While DeepSeek-R1-Zero proved that pure reinforcement learning could work, DeepSeek-R1 showed how combining supervised learning with reinforcement learning could create an even more capable and practical model.\n\nCollaborations ü§ù: Have an interesting AI project in mind? Let‚Äôs team up! I‚Äôm available for collaboration on AI and machine learning initiatives, and keen to connect with other professionals in the field.",
      "summary": "### A Simple Guide to DeepSeek R1: Architecture, Training, Local Deployment, and Hardware Requirements\n\n**Summary:**  \nDeepSeek has developed DeepSeek-R1, a family of models aimed at enhancing the reasoning capabilities of large language models (LLMs) using reinforcement learning (RL). This includes DeepSeek-R1-Zero, which utilizes pure RL without supervised fine-tuning, achieving 71% accuracy on the AIME 2024. DeepSeek-R1 improves upon this by incorporating a multi-stage training process that begins with supervised fine-tuning, resulting in better readability and coherence despite both models containing 671 billion parameters.\n\nThe training methodologies differ significantly: DeepSeek-R1 relies extensively on RL with group relative policy optimization (GRPO) and includes distillation techniques for smaller versions (1.5 billion to 70 billion parameters). Performance metrics indicate DeepSeek-R1 outperforms competitors on various benchmarks, such as a 79.8% pass rate on AIME 2024 and 97.3% on MATH-500.\n\nDespite impressive capabilities, challenges remain, particularly in specific output formats and few-shot prompting. Future improvements will focus on enhancing performance in software engineering tasks and multilingual contexts. DeepSeek-R1 is open-source under the MIT License, allowing for flexible deployment via the DeepSeek Chat platform, API, or local hardware setups. For local deployment, significant hardware resources are required for the full models, while distilled versions can operate on more modest systems. The article concludes by emphasizing the successful transition from DeepSeek-R1-Zero to DeepSeek-R1, showcasing the potential of combining supervised and reinforcement learning for advanced model performance.",
      "classification": "### A Simple Guide to DeepSeek R1: Architecture, Training, Local Deployment, and Hardware Requirements\n\n**Category:** Model",
      "keyword": "### A Simple Guide to DeepSeek R1: Architecture, Training, Local Deployment, and Hardware Requirements\n\n**Keywords:** DeepSeek-R1, Reinforcement Learning, Model Architecture, Distillation Techniques, Local Deployment"
    },
    {
      "No.": 18,
      "end_point": "https://medium.com/",
      "post_date": null,
      "link": "https://medium.com/ai-in-plain-english/deepseek-r1-understanding-grpo-and-multi-stage-training-5e0bbc28a281",
      "title": "DeepSeek R1: Understanding GRPO and Multi-Stage Training",
      "content": "Artificial intelligence has taken a significant leap forward with the release of DeepSeek R1, an open model that challenges OpenAI‚Äôs o1 in advanced reasoning tasks. Developed using an innovative technique called Group Relative Policy Optimisation (GRPO) and a multi-stage training approach, DeepSeek R1 sets new benchmarks for AI models in mathematics, coding, and general reasoning.\n\nWhat sets DeepSeek R1 apart is its ability to solve complex tasks with remarkable accuracy and reasoning depth while maintaining a streamlined training process. This blog dives into the foundational methods, the training pipeline, and the innovations that make DeepSeek R1 an exceptional model in AI research.\n\n\nSource: OpenAI\nUnderstanding Group Relative Policy Optimization (GRPO)\nGroup Relative Policy Optimisation (GRPO) is the core innovation driving DeepSeek R1‚Äôs exceptional reasoning abilities. Introduced in the DeepSeekMath paper, this reinforcement learning algorithm enhances model training by rethinking how rewards and optimisation are handled. GRPO replaces traditional methods like Proximal Policy Optimisation (PPO) with a simpler and more efficient approach tailored for large language models.\n\nIf you‚Äôre new to PPO and similar methods, you can check out my previous blogs to get an overview of what they are and how they work.\n\nKey Features of GRPO\nNo Value Function Model: Unlike PPO, GRPO eliminates the need for a separate value function model. This simplifies training and reduces memory usage, making it more efficient.\nGroup-Based Advantage Calculation: GRPO leverages a group of outputs for each input, calculating the baseline reward as the average score of the group. This group-based approach aligns better with reward model training, especially for reasoning tasks.\nDirect KL Divergence Optimisation: Instead of incorporating KL divergence into the reward signal (as in PPO), GRPO integrates it directly into the loss function, providing finer control during optimisation.\nGlimpse of how GRPO Works\nSampling: The model generates multiple outputs for each prompt using the current policy.\nReward Scoring: Each output is scored using a reward function. These scores can be rule-based (e.g., format or accuracy) or outcome-based (e.g., correctness in math or coding).\nAdvantage Calculation: The average reward from the group serves as the baseline. The relative advantage of each output is calculated based on this baseline, with rewards normalised within the group.\nPolicy Optimisation: Using the calculated advantages, the policy updates itself to maximise performance. The KL divergence term is incorporated directly into the loss function, ensuring the model balances exploration and stability.\n\nDeepdive into GRPO\nIf you‚Äôre just here for an overview, you can skip this part‚Äîthe previous section should be enough. I don‚Äôt want you to feel overwhelmed, so no need to dive into this if it‚Äôs not necessary.\n\nGroup Relative Policy Optimisation (GRPO), a variant of Proximal Policy Optimisation (PPO), enhances mathematical reasoning abilities while concurrently optimising the memory usage of PPO.\n\nGroup Relative Policy Optimization: Comprehensive Explanation\n\n1. Comparison Between PPO and GRPO\nThe key difference between Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) lies in their approach to advantage estimation and computational efficiency. While PPO relies on a separate value model, GRPO eliminates this dependency, replacing it with group-based relative advantage estimation, reducing memory and computation costs.\n\n\nSource: DeepSeekMath\n2. Diagram Overview\nIn the diagram:\n\nPPO:\n\nThe policy model generates outputs O for a given input q.\nA separate value model predicts a baseline v, used with Generalised Advantage Estimation (GAE) to compute advantages A.\nThe reward r includes a KL penalty term computed using a reference model and reward model.\nThis architecture results in significant resource overhead.\nGRPO:\n\nMultiple outputs {o1,o2,‚Ä¶,oG} are generated for each q, and their rewards {r1,r2,‚Ä¶,rG} are computed using the reward model.\nGroup computation normalises these rewards, providing relative advantages A1, A2,..., AG without a value model.\nThe KL divergence between the trained policy and reference model is added directly to the loss, simplifying training.\nPPO: Mathematical Formulation\nPPO is an RL algorithm that optimises a policy model by maximising a surrogate objective function while ensuring training stability through clipping-based constraints. The key aspects are described below:\n\n\n\n\nTakeaways from PPO\nAdvantage Calculation: PPO uses the GAE to reduce the variance in At‚Äã, leveraging a learned value function Vœà‚Äã as a baseline.\nClipping Regularization: Clipping in the surrogate objective ensures stability and prevents excessively large policy updates.\nKL Divergence Regularization: The KL penalty in the reward discourages the policy from diverging too much from the reference model, promoting stable learning.\nGRPO: Mathematical Formulation\nGroup Relative Policy Optimization (GRPO) simplifies PPO by removing the value model and using group-based relative rewards for baseline estimation. It is designed to efficiently fine-tune large language models (LLMs) while reducing computational overhead.\n\n\n\nTakeaways from GRPO\nEliminates the Value Model: GRPO replaces the computationally expensive value model with group-based reward normalization, significantly reducing resource requirements.\n\nLeverages Group Comparisons: By normalizing rewards within a group, GRPO aligns with the pairwise comparison nature of most reward models, ensuring better relative reward estimation.\n\nSimplifies KL Regularization: GRPO directly regularizes the policy with a KL divergence term, avoiding the need for complex KL penalties in the reward.\n\nOutcome Supervision RL with GRPO\n\n\nProcess Supervision RL with GRPO\n\n\n\nGRPO training involves iteratively updating the policy and reward model to maintain alignment. The steps are:\n\n\nKey Differences Between PPO and GRPO\nValue Model: PPO uses a value model for advantage estimation, while GRPO eliminates it and relies on group-normalized rewards.\nKL Regularization: PPO includes a KL penalty in the reward; GRPO directly regularizes the loss with a KL divergence term.\nReward Granularity: PPO computes token-level rewards directly, while GRPO leverages group-relative rewards normalized across sampled outputs.\nComputational Efficiency: GRPO is more efficient due to the removal of the value model and simpler advantage estimation.\nMulti-Stage Training of DeepSeek R1\nTraining an advanced reasoning model like DeepSeek R1 requires more than just raw computational power ‚Äî it demands a carefully structured training pipeline. To achieve superior reasoning and coherence, the DeepSeek team designed a multi-stage training process that combines supervised fine-tuning (SFT) with reinforcement learning (RL) using GRPO. This approach overcomes challenges like early instability in RL training and ensures that the model excels in diverse tasks.\n\nStage 1: Base to Supervised Fine-Tuning (SFT)\nThe journey began with fine-tuning the DeepSeek V3 base model using high-quality, chain-of-thought (CoT) data.\nData Collection:\n\nGenerated up to 10k token-long reasoning completions (CoT) using the R1-zero model and human annotators.\nFocus:\n\nEnhance readability, coherence, and logical flow in the model‚Äôs outputs.\nOutcome:\n\nA solid foundation for reinforcement learning, reducing instability during subsequent training stages.\nStage 2: RL for Reasoning\nGRPO was introduced to refine the model‚Äôs reasoning capabilities in tasks like mathematics, coding, and structured problem-solving.\nRule-Based Rewards:\n\nFocused on accuracy (e.g., solving coding problems, verifying mathematical results).\nEnforced formatting rules to ensure clarity, such as enclosing thought processes within specific tags (e.g., ‚Äòreasoning‚Äô).\nNew Reward Signal:\n\nA ‚Äúlanguage consistency‚Äù reward encouraged the model to maintain the same language throughout its outputs.\nOutcome:\n\nSignificant improvements in reasoning performance, as evidenced by the AIME 2024 pass@1 score jump to 71.0%.\nStage 3: Rejection Sampling and SFT\nTo expand the model‚Äôs capabilities, a large synthetic dataset was generated using Rejection Sampling (RS).\nDataset Creation:\n\nThe model from Stage 2 generated 600k reasoning-related samples.\nAdditional 200k samples focused on general-purpose tasks like writing and role-playing.\nData sourced from DeepSeek V3‚Äôs SFT dataset or regenerated with chain-of-thought included.\nFocus:\n\nBroaden the model‚Äôs expertise beyond reasoning tasks into creative and general-purpose domains.\nOutcome:\n\nThe model demonstrated greater versatility and coherence across a wider range of tasks.\nStage 4: RL for Helpfulness\nIn the final stage, GRPO was applied once again, but with a broader focus on helpfulness and harmlessness.\nCombination of Reward Models:\nRule-based rewards ensured continued improvement in reasoning and accuracy.\nOutcome-based rewards encouraged helpful and safe outputs.\nOutcome:\n\nA balanced model capable of handling complex reasoning tasks while maintaining clarity, safety, and user alignment.\nKey Insights from Multi-Stage Training\nEarly SFT Stabilizes RL Training: Fine-tuning the base model before applying RL techniques reduces training instability and accelerates convergence.\nRule-Based Rewards Are Effective: Simple, targeted rewards (accuracy, format) often outperform complex reward models.\nRejection Sampling Improves Versatility: Synthetic datasets generated through rejection sampling enhance the model‚Äôs adaptability to varied tasks.\nBy strategically alternating between supervised fine-tuning and reinforcement learning, the DeepSeek team overcame the challenges of RL cold starts and task-specific overfitting. This multi-stage pipeline ensured that DeepSeek R1 could excel in both reasoning and broader applications.\n\nStay tuned, will share more insights on it soon!",
      "summary": "### DeepSeek R1: Understanding GRPO and Multi-Stage Training\n\n**Summary:** The release of DeepSeek R1 marks a significant advancement in artificial intelligence, competing with OpenAI‚Äôs models in complex reasoning tasks. This model utilizes Group Relative Policy Optimisation (GRPO) and a multi-stage training approach to excel in mathematics, coding, and general reasoning. GRPO enhances training efficiency by eliminating the need for a separate value function model, using group-based advantage calculations, and integrating KL divergence into the loss function, which simplifies the optimization process.\n\nDeepSeek R1's training involves a structured multi-stage pipeline:\n1. **Supervised Fine-Tuning (SFT)**: The model is initially fine-tuned using high-quality chain-of-thought (CoT) data to enhance readability and coherence.\n2. **Reinforcement Learning (RL)**: GRPO is applied to improve reasoning performance with rule-based rewards focusing on accuracy and consistency, leading to a significant increase in reasoning capabilities as measured by the AIME score.\n3. **Rejection Sampling and SFT**: A large synthetic dataset is generated to broaden the model‚Äôs abilities beyond reasoning, enhancing its versatility across various tasks.\n4. **Final RL Stage for Helpfulness**: The last stage refines the model further to ensure it provides helpful and safe outputs.\n\nKey insights from this training approach include the importance of early SFT to stabilize RL training, the effectiveness of simple rule-based rewards, and the advantages of synthetic datasets for improving model adaptability. Overall, DeepSeek R1 represents a new benchmark in AI with its innovative GRPO methodology and comprehensive training strategy.",
      "classification": "### DeepSeek R1: Understanding GRPO and Multi-Stage Training\n\n**Category:** Model",
      "keyword": "### DeepSeek R1: Understanding GRPO and Multi-Stage Training\n\n**Keywords:** DeepSeek R1, Group Relative Policy Optimization, multi-stage training, reinforcement learning, reasoning tasks"
    },
    {
      "No.": 19,
      "end_point": "https://llm.extractum.io/static/llm-news/",
      "post_date": null,
      "link": "https://medium.com/@cognidownunder/gemini-2-0-googles-leap-into-the-agentic-era-of-ai-fc3390469f44",
      "title": "Gemini 2.0: Google‚Äôs Leap into the Agentic Era of AI",
      "content": "Google has just pulled back the curtain on Gemini 2.0, and it‚Äôs not just another incremental update in the AI arms race. This is the search giant‚Äôs most ambitious foray into what they‚Äôre calling the ‚Äúagentic era‚Äù of artificial intelligence. It‚Äôs a bold claim, but after diving into the details, it‚Äôs clear that Gemini 2.0 is poised to redefine our expectations of what AI can do.\n\nThe Dawn of Proactive AI\nFor years, we‚Äôve been interacting with AI models that are essentially glorified question-answering machines. Ask them something, and they‚Äôll spit out a response. But Gemini 2.0 is designed to flip that script entirely.\n\nThinking Ahead, So You Don‚Äôt Have To\nThe standout feature of Gemini 2.0 is its agentic capabilities. This isn‚Äôt just marketing fluff; the model is genuinely designed to ‚Äúthink multiple steps ahead.‚Äù It‚Äôs like having a digital assistant that doesn‚Äôt just wait for your commands but anticipates your needs and starts working on solutions before you even ask.\n\nThis proactive approach is a game-changer. Imagine an AI that doesn‚Äôt just tell you the weather but also suggests rescheduling your outdoor plans and offers alternative indoor activities. That‚Äôs the level of foresight we‚Äôre talking about here.\n\nMultimodal Mastery\nOne of the most impressive aspects of Gemini 2.0 is its multimodal prowess. This isn‚Äôt just about understanding different types of input; it‚Äôs about seamlessly blending various forms of output.\n\nBeyond Text: The Rich Tapestry of AI Responses\nGemini 2.0 supports native generation of images and audio. This means responses can be a rich mix of text, visuals, and even multilingual audio. It‚Äôs not just about conveying information; it‚Äôs about creating a more immersive and intuitive interaction.\n\nThe Need for Speed: Gemini 2.0 Flash\nIn the world of AI, speed is king, and Google knows it. Enter Gemini 2.0 Flash, an experimental version that‚Äôs pushing the boundaries of what‚Äôs possible in terms of performance.\n\nTwice as Fast, Twice as Good\nNot only does Flash outperform its predecessor, the 1.5 Pro model, on key benchmarks, but it does so while delivering responses twice as fast. This isn‚Äôt just incremental progress; it‚Äôs a quantum leap in AI responsiveness.\n\nNative Tool Integration: The Swiss Army Knife of AI\nOne of the most exciting aspects of Gemini 2.0 is its native integration with a suite of Google tools. This isn‚Äôt just about having access to information; it‚Äôs about seamlessly executing tasks in the real world.\n\nFrom Search to Maps: A World of Possibilities\nImagine asking your AI assistant to not just find a restaurant but to actually book a table, provide turn-by-turn directions, and even suggest menu items based on your dietary preferences. With Gemini 2.0‚Äôs integration of Google Search, Maps, and other tools, this level of comprehensive assistance is becoming a reality.\n\nThe Memory of an Elephant (Well, Almost)\nOne of the most frustrating aspects of current AI assistants is their goldfish-like memory. Gemini 2.0 takes a significant step forward in this regard.\n\n10 Minutes of Recall: A New Era of Contextual Understanding\nWith up to 10 minutes of in-session recall, Gemini 2.0 can maintain context and personalization in a way that feels much more human. This isn‚Äôt just about remembering facts; it‚Äôs about understanding the flow of a conversation and adapting responses accordingly.\n\nResearch Prototypes: A Glimpse into the Future\nGoogle isn‚Äôt just resting on its laurels with Gemini 2.0. They‚Äôre already exploring the next frontiers of AI through several ambitious research prototypes.\n\nProject Astra: The Universal AI Assistant\nProject Astra is Google‚Äôs vision of a truly universal AI assistant. Built on Gemini 2.0, it‚Äôs designed to seamlessly integrate with your daily life, providing real-time support and information across a wide range of tasks.\n\nProject Mariner: Reimagining Web Interaction\nProject Mariner is tackling one of the most common interfaces we use daily: the web browser. This prototype can understand and reason across various types of information in the browser, from text and code to images and forms.\n\nJules: Your AI Coding Companion\nFor developers, Jules represents a tantalizing glimpse into the future of coding. This AI-powered code agent can analyze, debug, and suggest solutions, potentially revolutionizing the development process.\n\nAvailability: Coming Soon to a Device Near You\nWhile developers can already access Gemini 2.0 Flash through Google AI Studio and Vertex AI, the rest of us won‚Äôt have to wait long to experience its capabilities.\n\nFrom Chat to Chrome: The Rollout Begins\nA chat-optimized version of Gemini 2.0 is already available in the Gemini app, with plans to integrate it into more Google products early next year. This gradual rollout strategy allows Google to refine and optimize the model based on real-world usage.\n\nThe Bottom Line: A New Chapter in AI\nGemini 2.0 represents more than just an upgrade; it‚Äôs a fundamental shift in how we think about and interact with AI. By combining proactive thinking, multimodal capabilities, and seamless integration with real-world tools, Google is laying the groundwork for a future where AI is not just a tool we use, but a partner we collaborate with.\n\nAs we stand on the brink of this new era, one thing is clear: the line between human and artificial intelligence is becoming increasingly blurred. Gemini 2.0 isn‚Äôt just imitating human-like responses; it‚Äôs starting to think and act in ways that are uniquely its own. And that, perhaps, is the most exciting and challenging aspect of all.\n\nFAQ\nQ: How does Gemini 2.0 differ from previous AI models?\nA: Gemini 2.0 introduces agentic capabilities, multimodal output, and native tool integration, making it more proactive and versatile than previous models.\n\nQ: Can Gemini 2.0 generate images and audio?\nA: Yes, Gemini 2.0 supports native generation of images and audio, allowing for more diverse and rich responses.\n\nQ: How can developers access Gemini 2.0?\nA: Developers can access Gemini 2.0 Flash through the Gemini API in Google AI Studio and Vertex AI.\n\nQ: What is the context window size for Gemini 2.0?\nA: Gemini 2.0 Flash and Flash-Lite have a context window of 1 million tokens, while the experimental Pro version boasts a 2 million token context window.\n\nQ: How does Gemini 2.0 improve memory and contextual understanding?\nA: Gemini 2.0 features up to 10 minutes of in-session recall, allowing for more personalized and contextually relevant interactions.\n\n#Gemini2 #GoogleAI #ArtificialIntelligence #AIInnovation #TechAdvancement\n\n‚Äúproactive AI assistant‚Äù, ‚Äúmultimodal AI capabilities‚Äù, ‚ÄúAI-powered code debugging‚Äù, ‚Äúnext-generation AI models‚Äù, ‚ÄúAI with extended memory recall‚Äù",
      "summary": "### Gemini 2.0: Google‚Äôs Leap into the Agentic Era of AI\n\n**Summary:** Google has unveiled Gemini 2.0, marking a significant advancement in artificial intelligence, which they term the \"agentic era.\" Unlike previous AI models that primarily functioned as question-answering systems, Gemini 2.0 offers proactive features that anticipate user needs, acting like a digital assistant that provides solutions before requests are made. Its standout capability is its \"thinking multiple steps ahead,\" enabling it to suggest actions such as rescheduling plans based on weather forecasts.\n\nGemini 2.0 excels in multimodal responses, integrating text, images, and audio to create immersive interactions. The introduction of Gemini 2.0 Flash enhances performance, delivering responses twice as fast as its predecessor while maintaining superior benchmarking results. \n\nThe model boasts native integration with Google tools, enabling comprehensive task execution, such as booking reservations or providing directions based on user preferences. It also features improved memory, with the capacity for 10 minutes of in-session recall, enhancing contextual understanding in conversations.\n\nGoogle is exploring future developments through research prototypes like Project Astra, aimed at creating a universal AI assistant; Project Mariner, focused on improving web interaction; and Jules, an AI coding companion for developers. \n\nGemini 2.0 is currently accessible to developers via Google AI Studio and Vertex AI, with broader rollout planned across Google products in the near future. This innovation signifies a transformative shift in human-AI interaction, blurring the lines between human-like responses and AI capabilities.",
      "classification": "### Gemini 2.0: Google‚Äôs Leap into the Agentic Era of AI\n\n**Category:** Updates & Trends",
      "keyword": "### Gemini 2.0: Google‚Äôs Leap into the Agentic Era of AI\n\n**Keywords:** agentic capabilities, multimodal integration, proactive AI, contextual memory, AI research prototypes"
    },
    {
      "No.": 20,
      "end_point": "https://medium.com/",
      "post_date": null,
      "link": "https://medium.com/@sahin.samia/s1-32b-model-explained-beating-openais-o1-with-just-1-000-training-examples-8f1e90957c1b",
      "title": "s1‚Äì32B Model Explained : Beating OpenAI‚Äôs o1 with Just 1,000 Training Examples",
      "content": "Over the past decade, AI progress has followed a simple rule: bigger is better ‚Äî larger models and more data lead to better performance. But what if we could enhance reasoning without increasing model size?\n\nThis is exactly what s1‚Äì32B achieves. Inspired by OpenAI‚Äôs o1 model, which demonstrated exceptional reasoning but kept its methodology closed, researchers sought a simpler, more efficient approach. Instead of massive reinforcement learning pipelines, they fine-tuned an existing model using just 1,000 carefully selected examples and introduced a lightweight technique called budget forcing ‚Äî a method that optimizes reasoning dynamically at test time.\n\nThe result? s1‚Äì32B surpasses OpenAI‚Äôs o1-preview on advanced math and science benchmarks while using a fraction of the compute and data. This breakthrough, known as test-time scaling, proves that we can push model performance even after training by refining how it reasons in real time.\n\nIn this blog, we‚Äôll dive into the mechanics behind s1‚Äì32B, the power of budget forcing, and why this open-source approach is redefining AI reasoning. Based on the paper, s1: Simple Test-Time Scaling (Muennighoff et al., 2025), this work is fully open-source and available on GitHub.. Understanding Test-Time Scaling\n\nWhat is Test-Time Scaling?\nTraditionally, language model improvements have come from scaling up train-time compute ‚Äî bigger datasets, larger models, and longer training times. However, test-time scaling flips this approach on its head. Instead of increasing compute during training, it allocates additional computation at inference time to improve reasoning performance.\n\nImagine a student given extra time to double-check their answers on an exam. Even with the same knowledge, they may perform significantly better just by thinking more carefully. Test-time scaling applies a similar concept to language models, allowing them to refine their answers after they‚Äôve already been trained.\n\nHow Test-Time Scaling Works\nInstead of treating inference as a one-shot process, test-time scaling introduces methods that enable models to reflect, verify, and refine their responses. Some common techniques include:\n\nIterative Reasoning: Encouraging the model to break down complex problems step by step.\nMajority Voting (Parallel Scaling): Generating multiple answers and selecting the most frequent or highest-confidence response.\nMonte Carlo Tree Search (MCTS): Using search algorithms to explore multiple reasoning paths before choosing the best one.\nWhile these techniques have shown promise, they often require expensive compute resources or complex modifications to existing architectures. This is where s1‚Äì32B takes a different approach ‚Äî by using a simple but effective decoding trick called budget forcing.\n\nThe s1 Approach: Minimal Data, Maximum Impact\nFor years, the prevailing belief in AI research has been that more data leads to better models. The largest and most capable language models today ‚Äî such as GPT-4 and Gemini ‚Äî are trained on massive datasets spanning trillions of tokens. However, s1‚Äì32B challenges this assumption by achieving state-of-the-art reasoning performance using just 1,000 carefully selected training examples.\n\nThe s1K Dataset: Quality Over Quantity\nInstead of collecting a massive dataset, the researchers behind s1‚Äì32B focused on curating a small but highly effective dataset called s1K. This dataset consists of 1,000 reasoning-intensive questions, each paired with detailed step-by-step solutions.\n\nTo ensure maximum efficiency, the dataset was selected based on three key principles:\n\nQuality ‚Äî Only well-structured, high-quality examples were included, filtering out poorly formatted or ambiguous questions.\nDifficulty ‚Äî Questions were chosen specifically because they were challenging for existing models, ensuring the dataset would push the model‚Äôs reasoning abilities.\nDiversity ‚Äî The dataset spans 50+ domains, including advanced math, physics, logic puzzles, and standardized test problems.\nThis highly selective approach meant that the model learned to reason effectively with far fewer examples than traditional models. Rather than memorizing vast amounts of data, s1‚Äì32B learned from high-quality, reasoning-rich examples that forced it to develop structured thought processes.\n\nTraining: Fast, Efficient, and Cost-Effective\nTraining s1‚Äì32B on the s1K dataset took only 26 minutes on 16 H100 GPUs ‚Äî a stark contrast to the weeks or months required to train state-of-the-art models on massive datasets. This efficiency makes s1‚Äì32B one of the most sample-efficient reasoning models ever built.\n\nSo how does it achieve better reasoning performance despite its small dataset? The answer lies in a novel test-time intervention technique called budget forcing ‚Äî a simple yet powerful trick that improves reasoning without additional training.\n\nBudget Forcing: A Simple Yet Powerful Decoding Trick\nImproving a language model‚Äôs reasoning ability usually involves training on more data or fine-tuning with advanced techniques like reinforcement learning. However, s1‚Äì32B takes a different approach ‚Äî instead of modifying the model itself, it changes how the model thinks at test time.\n\nThe key innovation? Budget forcing ‚Äî a lightweight, test-time-only intervention that guides the model‚Äôs reasoning without requiring extra training data.\n\nHow Budget Forcing Works\nAt its core, budget forcing is a decoding-time control method that regulates how long the model spends reasoning before generating an answer. This is done in two ways:\n\nLimiting Reasoning (Early Stopping)\n\nIf the model spends too much time thinking, budget forcing terminates the reasoning process by appending a forced stop token (e.g., ‚ÄúFinal Answer:‚Äù).\nThis prevents the model from getting stuck in unnecessary loops or excessive computations.\nExtending Reasoning (Encouraging More Thought)\n\nIf the model tries to stop too early, budget forcing prevents it from stopping and appends the word ‚ÄúWait‚Äù to its reasoning process.\nThis forces the model to continue reflecting, often leading it to identify and correct mistakes before finalizing an answer.\nExample: Self-Correction in Action\n\n\nMuennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand√®s, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393\nThis simple yet highly effective strategy helps the model avoid common reasoning errors and produce more reliable outputs without additional training\n\nComparison to Other Test-Time Scaling Methods\nWhile budget forcing is a straightforward solution, other test-time scaling methods exist. How does it compare?\n\n\nWhy Budget Forcing Works So Well\nUnlike majority voting or MCTS, which require significantly more compute, budget forcing is a lightweight and controllable method that can be applied on-the-fly during inference.\n\nScalability: Works on both small and large models without retraining.\nEfficiency: Requires no extra fine-tuning or RL ‚Äî just a simple decoding trick.\nPerformance Gains: Helps s1‚Äì32B outperform OpenAI‚Äôs o1-preview on advanced math and reasoning benchmarks.\nPerformance and Benchmarks: How s1‚Äì32B Stacks Up\nNow that we‚Äôve explored how s1‚Äì32B is trained and how budget forcing enhances its reasoning, the next question is: How well does it actually perform?\n\nTo measure its effectiveness, s1‚Äì32B was evaluated on three widely used benchmarks for advanced reasoning:\n\n\nCreated using data from Paper\nKey Insights from These Results:\n‚úÖ s1‚Äì32B outperforms OpenAI‚Äôs o1-preview by up to 27% on AIME24 ‚Äî a highly competitive math benchmark.\n‚úÖ It achieves 93% accuracy on MATH500, proving its effectiveness in structured problem-solving.\n‚úÖ While slightly behind on GPQA Diamond, it still demonstrates strong scientific reasoning, despite training on just 1,000 samples.\n\nTest-Time Scaling in Action\nOne of the biggest advantages of s1‚Äì32B is its ability to scale performance dynamically at test time. Thanks to budget forcing, the model‚Äôs accuracy increases when given more test-time compute.\n\nExample: Scaling Performance on AIME24\n\nBase performance (default reasoning time): 50% accuracy\nWith extended budget forcing (more reasoning steps): 57% accuracy\nWhy does this matter?\n\nMost AI models have fixed performance after training.\ns1‚Äì32B improves its answers dynamically ‚Äî scaling its reasoning depth without retraining.\nThis bridges the gap between static fine-tuned models and more expensive reinforcement learning techniques.\nComparing s1‚Äì32B with Other Open Models\n\nMuennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand√®s, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393\nWhile OpenAI and Google‚Äôs latest models remain closed-source, s1‚Äì32B is fully open and reproducible.\n\nHow It Stands Against Other Open-Weight Models:\n\nMore sample-efficient than DeepSeek-R1, which required 800K+ reasoning samples to achieve similar results.\nTrained in just 26 minutes, while other models often require weeks of GPU time.\nMatches Google‚Äôs Gemini 2.0 Flash Thinking on AIME24, proving its effectiveness at structured reasoning.\n\nMuennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand√®s, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393\nThe Future of Test-Time Scaling: What‚Äôs Next?\nThe success of s1‚Äì32B and budget forcing proves that test-time scaling is a powerful alternative to traditional training-based improvements. Instead of relying on massive datasets and computationally expensive reinforcement learning, test-time scaling allows us to enhance reasoning dynamically ‚Äî even after a model has been trained.\n\nBut this is just the beginning. What‚Äôs next for test-time scaling?\n\nPushing the Boundaries of Test-Time Scaling\n1Ô∏è‚É£ Improving Budget Forcing for Even Better Self-Correction\nWhile budget forcing is simple and effective, there‚Äôs room for optimization.\nüîπ Smarter prompts ‚Äî Instead of just appending ‚ÄúWait‚Äù, we could dynamically modify the reasoning process.\nüîπ Adaptive stopping criteria ‚Äî Can we predict the optimal amount of reasoning needed for different problems?\nüîπ Avoiding loops ‚Äî Sometimes, excessive budget forcing can lead to repetitive reasoning cycles. New techniques could help mitigate this.\n\n2Ô∏è‚É£ Hybrid Approaches: Combining Parallel and Sequential Scaling\nParallel scaling methods like majority voting aggregate multiple model responses to find the best answer.\nSequential scaling, like budget forcing, extends reasoning iteratively within a single model run.\nCombining both could lead to even better results ‚Äî leveraging majority voting while keeping compute costs low.\n3Ô∏è‚É£ Beyond Language Models: Can Test-Time Scaling Work for Multimodal AI?\nCould test-time scaling improve image reasoning (e.g., visual question answering)?\nCould budget forcing help reinforcement learning agents make better decisions by spending more time ‚Äúthinking‚Äù before acting?\nAs AI expands into video understanding and robotics, test-time scaling could help models analyze situations more deeply before taking action.\nOpen-Source Innovation: Why s1‚Äì32B Matters\nUnlike OpenAI‚Äôs o1 model or Google‚Äôs Gemini, s1‚Äì32B is fully open-source ‚Äî allowing researchers to experiment, improve, and build on this work.\n\nüîó Try it yourself! üëâ GitHub: Simple Scaling\n\nHere is how you can use this model from huggingface:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"simplescaling/s1-32B\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"How many r in raspberry\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nCode source: GitHub: Simple Scaling\n\nWith more researchers exploring test-time reasoning enhancements, we could be on the verge of a new AI paradigm ‚Äî one that prioritizes efficiency, adaptability, and reasoning depth over brute-force training scale.\n\nConclusion: The Shift from Bigger to Smarter AI\nFor years, AI progress has been driven by bigger models, more training data, and massive compute. But s1‚Äì32B proves that there‚Äôs another way ‚Äî one that focuses on smarter reasoning instead of just larger models.\n\n‚úÖ A 1,000-sample dataset beats 800K+ data-heavy models.\n‚úÖ A simple decoding trick improves reasoning at test time.\n‚úÖ s1‚Äì32B rivals proprietary models while being fully open-source.\n\nAs AI evolves, test-time scaling could become a defining technique for future models ‚Äî helping AI think more efficiently, dynamically, and intelligently.\n\nReference:\nMuennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand√®s, E., & Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393v2. Retrieved from https://arxiv.org/abs/2501.19393",
      "summary": "### s1‚Äì32B Model Explained: Beating OpenAI‚Äôs o1 with Just 1,000 Training Examples\n\n**Summary:** The s1‚Äì32B model represents a significant advancement in AI reasoning capabilities, achieving superior performance compared to OpenAI‚Äôs o1 model while utilizing only 1,000 carefully curated training examples. Unlike traditional approaches that require vast datasets and extensive computational resources, s1‚Äì32B employs a technique known as test-time scaling, which optimizes reasoning dynamically during inference rather than relying solely on the training phase.\n\nThe model is trained on a specially designed dataset, s1K, composed of 1,000 complex, reasoning-intensive questions. This dataset emphasizes quality, difficulty, and diversity, allowing the model to develop structured thought processes rather than merely memorizing information.\n\nCentral to s1‚Äì32B's performance is a novel decoding technique called budget forcing. This method controls the reasoning duration of the model during inference, enabling it to either extend its reasoning time when necessary or truncate it if it is taking too long. This lightweight intervention enhances the model's ability to generate accurate answers without additional training.\n\ns1‚Äì32B's efficiency is underscored by its rapid training time of just 26 minutes on 16 GPUs, vastly outperforming other models that require extensive training periods. In performance benchmarks, s1‚Äì32B demonstrated up to a 27% improvement over OpenAI‚Äôs o1 on advanced math problems and achieved 93% accuracy on structured reasoning tasks.\n\nFurthermore, budget forcing allows the model to dynamically adjust its reasoning depth during testing, leading to better outcomes compared to static models. The open-source nature of s1‚Äì32B invites collaboration and innovation within the research community, heralding a potential shift toward more efficient, adaptable AI systems that emphasize reasoning over sheer size.\n\nIn summary, s1‚Äì32B not only challenges the conventional wisdom that larger datasets yield better performance but also illustrates a promising future for AI where reasoning efficiency is prioritized, potentially revolutionizing the field.",
      "classification": "### s1‚Äì32B Model Explained : Beating OpenAI‚Äôs o1 with Just 1,000 Training Examples\n\n**Category:** Model",
      "keyword": "### s1‚Äì32B Model Explained : Beating OpenAI‚Äôs o1 with Just 1,000 Training Examples\n\n**Keywords:** s1‚Äì32B, budget forcing, test-time scaling, reasoning efficiency, s1K dataset"
    },
    {
      "No.": 21,
      "end_point": "https://news.hada.io/",
      "post_date": null,
      "link": "https://news.hada.io/topic?id=19190",
      "title": "DeepScaleR: RLÏùÑ ÌôúÏö©Ìïú 1.5B Î™®Îç∏Î°ú O1-Preview Îä•Í∞ÄÌïòÍ∏∞",
      "content": "‚ñ≤\r\nGN‚Å∫: DeepScaleR: RLÏùÑ ÌôúÏö©Ìïú 1.5B Î™®Îç∏Î°ú O1-Preview Îä•Í∞ÄÌïòÍ∏∞ (pretty-radio-b75.notion.site)\r\n5P by neo 5ÏùºÏ†Ñ | ‚òÖ favorite | ÎåìÍ∏Ä 1Í∞ú\r\nDeepScaleR-1.5B-Preview: Deepseek-R1-Distilled-Qwen-1.5B Î™®Îç∏ÏùÑ Í∞ïÌôî ÌïôÏäµ(RL)ÏúºÎ°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïú Î™®Îç∏\r\nAIME2024 Pass@1 Ï†ïÌôïÎèÑ 43.1% Îã¨ÏÑ± (Í∏∞Î≥∏ Î™®Îç∏ ÎåÄÎπÑ +14.3% Ìñ•ÏÉÅ),\r\n‚Üí OpenAI o1-preview ÏÑ±Îä• Îä•Í∞Ä!\r\n3,800 A100 GPU ÏãúÍ∞Ñ($4500)ÏúºÎ°ú ÌõàÎ†® ‚Üí 70,000 A100 GPU ÏãúÍ∞Ñ ÎåÄÎπÑ 18.42Î∞∞ Ìö®Ïú®Ï†ÅÏù∏ RL Ïä§ÏºÄÏùºÎßÅ\r\nÎç∞Ïù¥ÌÑ∞ÏÖã, ÏΩîÎìú, ÌõàÎ†® Î°úÍ∑∏ Ïò§ÌîàÏÜåÏä§ Í≥µÍ∞ú ‚Üí ÎàÑÍµ¨ÎÇò RLÏùÑ ÌôúÏö©Ìïú ÏßÄÎä• ÌôïÏû•ÏùÑ Ïã§Ìóò Í∞ÄÎä•\r\nRLÏùÑ ÌôúÏö©Ìïú ÏÜåÌòï Î™®Îç∏ Í∞ïÌôî\r\nDeepseek-R1ÏùÄ OpenAI o1Í≥º Í≤¨Ï§Ñ Ïàò ÏûàÎäî Ïò§ÌîàÏÜåÏä§ Î™®Îç∏Ïù¥ÏßÄÎßå, Ï†ïÌôïÌïú ÌõàÎ†® Í≥ºÏ†ïÏùÄ ÎπÑÍ≥µÍ∞ú\r\nRLÏùÑ ÌôúÏö©ÌïòÏó¨ Ï†ÅÏùÄ Í≥ÑÏÇ∞ÎüâÏúºÎ°ú Í∞ïÎ†•Ìïú Ï∂îÎ°† Î™®Îç∏ÏùÑ Í∞úÎ∞úÌïòÎäî Î∞©Î≤ïÏùÑ Ïó∞Íµ¨\r\nÍ∏∞Ï°¥ RLÏùò Í∞ÄÏû• ÌÅ∞ ÌïúÍ≥ÑÎäî Í≥†ÎπÑÏö©:\r\n‚Üí Deepseek-R1Ïùò Ïã§ÌóòÏùÑ Ïû¨ÌòÑÌïòÎ†§Î©¥ ÏµúÏÜå 70,000 A100 GPU ÏãúÍ∞Ñ ÌïÑÏöî\r\nÌï¥Í≤∞Ï±Ö:\r\nÍ≥†ÏÑ±Îä• ÏßÄÏãù Ï¶ùÎ•ò(distillation) Î™®Îç∏ ÌôúÏö©\r\nRLÏùÑ Ï†êÏßÑÏ†ÅÏúºÎ°ú ÌôïÏû•ÌïòÎäî \"Iterative Lengthening\" Í∏∞Î≤ï ÎèÑÏûÖ ‚Üí Í≥ÑÏÇ∞Îüâ 3,800 A100 GPU ÏãúÍ∞ÑÏúºÎ°ú Ï†àÍ∞ê\r\nÎç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï∂ï\r\nAIME(1984-2023) + AMC(2023 Ïù¥Ï†Ñ) + Omni-MATH + Still Îç∞Ïù¥ÌÑ∞ÏÖã ÏÇ¨Ïö©\r\n\r\nÎç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú Í≥ºÏ†ï:\r\n\r\nÏ†ïÎãµ Ï∂îÏ∂ú: gemini-1.5-pro-002Î•º ÌôúÏö©Ìï¥ Í≥µÏãù Ìï¥ÏÑ§ÏóêÏÑú Ï†ïÎãµ Ï∂îÏ∂ú\r\nÏ§ëÎ≥µ Ï†úÍ±∞: sentence-transformers/all-MiniLM-L6-v2 ÏûÑÎ≤†Îî© Í∏∞Î∞òÏúºÎ°ú Ïú†ÏÇ¨ Î¨∏Ï†ú Ï†úÍ±∞\r\nÏ±ÑÏ†ê Î∂àÍ∞ÄÎä• Î¨∏Ï†ú ÌïÑÌÑ∞ÎßÅ: sympyÎ•º ÌôúÏö©Ìïú ÏûêÎèô ÌèâÍ∞ÄÍ∞Ä Ïñ¥Î†§Ïö¥ Î¨∏Ï†ú Ï†úÍ±∞\r\nÏµúÏ¢ÖÏ†ÅÏúºÎ°ú 40,000Í∞ú Î¨∏Ï†ú-Ï†ïÎãµ Ïåç ÌôïÎ≥¥, Ìñ•ÌõÑ Îç∞Ïù¥ÌÑ∞ ÌôïÏû• ÏòàÏ†ï\r\n\r\nÎ≥¥ÏÉÅ Ìï®Ïàò(Reward Function)\r\nDeepseek-R1Í≥º ÎèôÏùºÌïòÍ≤å \"Outcome Reward Model (ORM)\" Ï†ÅÏö©:\r\n\r\n1Ï†ê: Ïò¨Î∞îÎ•∏ ÌòïÏãùÏùò Ï†ïÎãµ (sympy Í≤ÄÏ¶ù ÌÜµÍ≥º)\r\n0Ï†ê: ÏûòÎ™ªÎêú Ï†ïÎãµ, ÌòïÏãù Ïò§Î•ò (<think>...</think> ÎàÑÎùΩ Îì±)\r\n\"Í≥ºÏ†ï Í∏∞Î∞ò Î≥¥ÏÉÅ(Process Reward Model, PRM)\"ÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÎäî Ïù¥Ïú†:\r\n\r\nÎ≥¥ÏÉÅ Ìï¥ÌÇπ(reward hacking) Î∞©ÏßÄ ‚Üí Î™®Îç∏Ïù¥ ÌòïÏãùÎßå Îî∞ÎùºÍ∞ÄÎ†§Îäî Î∂ÄÏûëÏö© Î∞©ÏßÄ\r\n\"Iterative Lengthening\": RL ÌïôÏäµÏùÑ Îã®Í≥ÑÏ†ÅÏúºÎ°ú ÌôïÏû•ÌïòÎäî Í∏∞Î≤ï\r\nStep 1: 8K Ïª®ÌÖçÏä§Ìä∏Î°ú RL ÌïôÏäµ ÏãúÏûë\r\nÏù¥Ïú†:\r\nÏûòÎ™ªÎêú ÎãµÎ≥ÄÏùÄ ÌèâÍ∑† 20,346 ÌÜ†ÌÅ∞, Ï†ïÎãµÏùÄ 6,395 ÌÜ†ÌÅ∞ ‚Üí Í∏¥ ÏùëÎãµÏù¥ Ïò§Îãµ Í∞ÄÎä•ÏÑ± Ï¶ùÍ∞Ä\r\nÏ¥àÍ∏∞Î∂ÄÌÑ∞ Í∏¥ Ïª®ÌÖçÏä§Ìä∏Î°ú ÌïôÏäµÌïòÎ©¥ ÎπÑÌö®Ïú®Ï†Å ‚Üí 8KÎ°ú Î®ºÏ†Ä ÏµúÏ†ÅÌôî\r\nÍ≤∞Í≥º:\r\nAIME Pass@1 28.9% ‚Üí 33.9% (+5%) Ìñ•ÏÉÅ\r\nÎ∂àÌïÑÏöîÌïú ÌÜ†ÌÅ∞ Ïàò Í∞êÏÜå ‚Üí ÌèâÍ∑† ÏùëÎãµ Í∏∏Ïù¥ 10,484 ÌÜ†ÌÅ∞ Í∞êÏÜå\r\nStep 2: 16K Ïª®ÌÖçÏä§Ìä∏Î°ú ÌôïÏû•\r\nÌõàÎ†® 1,000Ïä§ÌÖù Ïù¥ÌõÑ, Î™®Îç∏Ïù¥ Îçî Í∏∏Í≤å ÏÇ¨Í≥†(Ï∂îÎ°†)ÌïòÎ†§Îäî Í≤ΩÌñ•ÏùÑ Î≥¥ÏûÑ\r\nÌïòÏßÄÎßå 8K ÌïúÍ≥ÑÎ°ú Ïù∏Ìï¥ ÌïôÏäµ Ìö®Í≥ºÍ∞Ä Ï†úÌïúÎê® ‚Üí 16KÎ°ú ÌôïÏû•\r\nÏû•Ï†ê:\r\nÏ≤òÏùåÎ∂ÄÌÑ∞ 16KÎ°ú ÌõàÎ†®ÌïòÎäî Í≤ÉÎ≥¥Îã§ 2Î∞∞ Ïù¥ÏÉÅ Îπ†Î¶Ñ (ÌèâÍ∑† ÏùëÎãµ Í∏∏Ïù¥ 3,000 ‚Üí 9,000 ÌÜ†ÌÅ∞ Î∞©ÏßÄ)\r\nAIME2024 Ï†ïÌôïÎèÑ 38% ÎèÑÎã¨\r\nStep 3: \"24K Magic\" - ÏµúÏ¢Ö ÏÑ±Îä• Ìñ•ÏÉÅ\r\n16KÏóêÏÑú ÏÑ±Îä•Ïù¥ Ï†ïÏ≤¥ ‚Üí 24K Ïª®ÌÖçÏä§Ìä∏Î°ú ÎßàÏßÄÎßâ ÌôïÏû•\r\nÍ≤∞Í≥ºÏ†ÅÏúºÎ°ú AIME2024 Pass@1 Ï†ïÌôïÎèÑ 43.1% ÎèÑÎã¨, OpenAI o1-preview Îä•Í∞Ä!\r\nÏµúÏ¢Ö ÌèâÍ∞Ä Í≤∞Í≥º\r\nDeepScaleR Î™®Îç∏ÏùÄ AIME, MATH 500, AMC 2023, Minerva Math, OlympiadBench Îì± Ïó¨Îü¨ ÏàòÌïô Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑú ÌèâÍ∞ÄÎê®\r\nAIME2024 Í∏∞Ï§Ä, DeepScaleR-1.5B-PreviewÏùò Ï†ïÌôïÎèÑÎäî 43.1%Î°ú, OpenAI o1-preview Î™®Îç∏Î≥¥Îã§ Ïö∞ÏàòÌï®\r\nMATH 500, AMC 2023 Îì±ÏóêÏÑúÎèÑ 1.5B Î™®Îç∏ÏûÑÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† 7B Î™®Îç∏Í≥º ÎèôÎì±ÌïòÍ±∞ÎÇò Îçî ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Í∏∞Î°ù\r\nÏù¥Ï†Ñ Ïó∞Íµ¨(RL Í∏∞Î∞ò rStar, PRIME, SimpleRL)ÏôÄ ÎπÑÍµêÌï¥ÎèÑ ÏµúÍ≥†Ïùò Ìö®Ïú®ÏÑ±ÏùÑ Î≥¥Ïó¨Ï§å\r\nÌïµÏã¨ ÏöîÏïΩ (Key Takeaways)\r\nÏÜåÌòï Î™®Îç∏ÏóêÏÑúÎèÑ RL ÌôïÏû•Ïù¥ Í∞ÄÎä•Ìï®\r\n\r\nÍ∏∞Ï°¥ÏóêÎäî RLÏù¥ ÎåÄÌòï Î™®Îç∏ÏóêÎßå Ìö®Í≥ºÏ†ÅÏù¥ÎùºÎäî Ïù∏ÏãùÏù¥ ÏûàÏóàÏùå\r\nÌïòÏßÄÎßå Í≥†ÌíàÏßà Îç∞Ïù¥ÌÑ∞Î°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú ÏûëÏùÄ Î™®Îç∏ÎèÑ RLÏùÑ ÌÜµÌï¥ Í∞ïÎ†•Ìïú Ï∂îÎ°† Îä•Î†•ÏùÑ ÌïôÏäµ Í∞ÄÎä•\r\nDeepScaleRÎäî 28.9% ‚Üí 43.1% (AIME Ï†ïÌôïÎèÑ) Ìñ•ÏÉÅ\r\n\"Iterative Lengthening\" Í∏∞Î≤ïÏúºÎ°ú Ìö®Í≥ºÏ†ÅÏù∏ Í∏∏Ïù¥ ÌôïÏû• Í∞ÄÎä•\r\n\r\nÍ∏∞Ï°¥ Ïó∞Íµ¨ÏóêÏÑúÎäî 16K Ïù¥ÏÉÅ Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú ÏÑ±Îä• Ìñ•ÏÉÅÏù¥ ÎØ∏ÎØ∏Ìï®ÏùÑ Î≥¥Í≥†\r\n8K ‚Üí 16K ‚Üí 24K Ï†êÏßÑÏ†Å ÌôïÏû•ÏùÑ ÌÜµÌï¥ ÏÑ±Îä• ÏµúÎåÄÌôî\r\nÍ≤∞Î°†: RL Ïä§ÏºÄÏùºÎßÅÏùò ÎåÄÏ§ëÌôî\r\nDeepScaleR-1.5B-PreviewÎäî O1-previewÎ•º Îä•Í∞ÄÌïòÎäî ÏµúÏ¥àÏùò Ïò§ÌîàÏÜåÏä§ RL Î™®Îç∏\r\n3,800 A100 GPU ÏãúÍ∞Ñ($4500)ÎßåÏúºÎ°úÎèÑ Í≥†ÏÑ±Îä• Î™®Îç∏ Íµ¨Ï∂ï Í∞ÄÎä• ‚Üí Ï†ÄÎπÑÏö© RL Ïó∞Íµ¨Ïùò Í∞ÄÎä•ÏÑ± Ï¶ùÎ™Ö\r\nÏò§ÌîàÏÜåÏä§ Ïª§ÎÆ§ÎãàÌã∞ÏôÄ Ìï®Íªò RL Í∏∞Î∞ò Ï∂îÎ°† Î™®Îç∏Ïùò Î∞úÏ†ÑÏùÑ ÏßÄÏÜçÌï† ÏòàÏ†ï",
      "summary": "### DeepScaleR: RLÏùÑ ÌôúÏö©Ìïú 1.5B Î™®Îç∏Î°ú O1-Preview Îä•Í∞ÄÌïòÍ∏∞\n\n**Summary:** DeepScaleR-1.5B-PreviewÎäî Í∞ïÌôî ÌïôÏäµ(RL)ÏùÑ ÌÜµÌï¥ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú Deepseek-R1-Distilled-Qwen-1.5B Î™®Îç∏Î°ú, AIME2024ÏóêÏÑú Pass@1 Ï†ïÌôïÎèÑ 43.1%Î•º Í∏∞Î°ùÌïòÎ©∞ OpenAIÏùò O1-PreviewÎ•º Ï¥àÍ≥ºÌïòÎäî ÏÑ±Îä•ÏùÑ Îã¨ÏÑ±ÌïòÏòÄÎã§. Ïù¥ Î™®Îç∏ÏùÄ 3,800 A100 GPU ÏãúÍ∞ÑÏúºÎ°ú ÌõàÎ†®ÎêòÏñ¥ 70,000 A100 GPU ÏãúÍ∞ÑÏóê ÎπÑÌï¥ 18.42Î∞∞ Ìö®Ïú®Ï†ÅÏù∏ RL Ïä§ÏºÄÏùºÎßÅÏùÑ Íµ¨ÌòÑÌïòÏòÄÎã§. Îç∞Ïù¥ÌÑ∞ÏÖã, ÏΩîÎìú, ÌõàÎ†® Î°úÍ∑∏Îäî Ïò§ÌîàÏÜåÏä§ ÌòïÌÉúÎ°ú Í≥µÍ∞úÎêòÏñ¥, ÎàÑÍµ¨ÎÇò RLÏùÑ ÌôúÏö©Ìïú Ïã§ÌóòÏù¥ Í∞ÄÎä•ÌïòÎã§. \n\nDeepseek-R1ÏùÄ OpenAI o1Í≥º Ïú†ÏÇ¨Ìïú ÏÑ±Îä•ÏùÑ Î≥¥Ïù¥Îäî Ïò§ÌîàÏÜåÏä§ Î™®Îç∏Ïù¥ÏßÄÎßå, Ï†ïÌôïÌïú ÌõàÎ†® Í≥ºÏ†ïÏùÄ ÎπÑÍ≥µÏãùÏ†ÅÏù¥Îã§. RLÏùò Ï†ÑÌÜµÏ†ÅÏù∏ ÌïúÍ≥ÑÎ•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌï¥ Í≥†ÏÑ±Îä• ÏßÄÏãù Ï¶ùÎ•ò Î™®Îç∏ÏùÑ ÌôúÏö©ÌïòÍ≥†, \"Iterative Lengthening\" Í∏∞Î≤ïÏùÑ ÌÜµÌï¥ Í≥ÑÏÇ∞ÎüâÏùÑ 3,800 A100 GPU ÏãúÍ∞ÑÏúºÎ°ú Ï§ÑÏòÄÎã§. Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú Í≥ºÏ†ïÏóêÏÑú 40,000Í∞úÏùò Î¨∏Ï†ú-Ï†ïÎãµ ÏåçÏùÑ ÌôïÎ≥¥ÌïòÏòÄÍ≥†, Î≥¥ÏÉÅ Ìï®ÏàòÎ°úÎäî Outcome Reward Model (ORM)ÏùÑ ÏÇ¨Ïö©ÌïòÏòÄÎã§.\n\nÎ™®Îç∏ÏùÄ 8K, 16K, 24K Ïª®ÌÖçÏä§Ìä∏Î°ú Îã®Í≥ÑÏ†ÅÏúºÎ°ú ÌïôÏäµÏùÑ ÌôïÏû•ÌïòÏòÄÍ≥†, ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú AIME2024ÏóêÏÑú Pass@1 Ï†ïÌôïÎèÑÍ∞Ä 43.1%Ïóê ÎèÑÎã¨ÌïòÏòÄÎã§. DeepScaleRÏùÄ MATH 500, AMC 2023 Îì± Îã§Î•∏ ÏàòÌïô Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑúÎèÑ Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥Ïù¥Î©∞, Í∏∞Ï°¥ RL Í∏∞Î∞ò Î™®Îç∏Îì§Ïóê ÎπÑÌï¥ ÏµúÍ≥†Ïùò Ìö®Ïú®ÏÑ±ÏùÑ ÎÇòÌÉÄÎÇ∏Îã§. Ïù¥ Ïó∞Íµ¨Îäî ÏÜåÌòï Î™®Îç∏ÏóêÏÑúÎèÑ RLÏùÑ ÌÜµÌïú ÏÑ±Îä• Ìñ•ÏÉÅÏù¥ Í∞ÄÎä•Ìï®ÏùÑ ÏûÖÏ¶ùÌïòÎ©∞, Ï†ÄÎπÑÏö©ÏúºÎ°ú Í≥†ÏÑ±Îä• Î™®Îç∏ Íµ¨Ï∂ïÏùò Í∞ÄÎä•ÏÑ±ÏùÑ Ï†úÏãúÌïúÎã§.",
      "classification": "### DeepScaleR: RLÏùÑ ÌôúÏö©Ìïú 1.5B Î™®Îç∏Î°ú O1-Preview Îä•Í∞ÄÌïòÍ∏∞\n\n**Category:** Model",
      "keyword": "### DeepScaleR: RLÏùÑ ÌôúÏö©Ìïú 1.5B Î™®Îç∏Î°ú O1-Preview Îä•Í∞ÄÌïòÍ∏∞\n\n**Keywords:** DeepScaleR, Í∞ïÌôî ÌïôÏäµ, Iterative Lengthening, Î™®Îç∏ ÏÑ±Îä•, Ïò§ÌîàÏÜåÏä§ Îç∞Ïù¥ÌÑ∞"
    },
    {
      "No.": 22,
      "end_point": "https://discuss.pytorch.kr/",
      "post_date": null,
      "link": "https://discuss.pytorch.kr/t/s1-test-time-scaling/6060",
      "title": "s1: ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ(Test-Time Scaling)ÏùÑ Îã®ÏàúÌïòÍ≤å Íµ¨ÌòÑÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Ïó∞Íµ¨",
      "content": "Ïù¥ Ïó∞Íµ¨ÏóêÏÑúÎäî Í∞ÄÏû• Îã®ÏàúÌïòÎ©¥ÏÑúÎèÑ Ìö®Í≥ºÏ†ÅÏù∏ ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ Í∏∞Î≤ïÏùÑ ÌÉêÏÉâÌïòÎäî Í≤ÉÏùÑ Î™©ÌëúÎ°ú ÌïòÏòÄÏäµÎãàÎã§. Ïó∞Íµ¨ÌåÄÏùÄ Budget ForcingÏù¥ÎùºÎäî Í∞úÎÖêÏùÑ ÎèÑÏûÖÌïòÏó¨, ÏµúÏÜåÌïúÏùò Îç∞Ïù¥ÌÑ∞ÏôÄ Ïó∞ÏÇ∞ÎüâÏúºÎ°úÎèÑ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌÅ¨Í≤å Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÎäîÏßÄÎ•º Í≤ÄÏ¶ùÌïòÏòÄÏäµÎãàÎã§. ÌäπÌûà, s1-32B Î™®Îç∏ÏùÑ Í∞úÎ∞úÌïòÏó¨ Îã® 1,000Í∞úÏùò Í≥†ÌíàÏßàÏùò Îç∞Ïù¥ÌÑ∞Î°ú Íµ¨ÏÑ±Îêú s1K Îç∞Ïù¥ÌÑ∞ÏÖãÎßåÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í∏∞Ï°¥ Î™®Îç∏Î≥¥Îã§ Îõ∞Ïñ¥ÎÇú Ï∂îÎ°† ÏÑ±Îä•ÏùÑ Í∏∞Î°ùÌïòÏòÄÏäµÎãàÎã§. Ïù¥Îäî ÏÜåÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖãÎßåÏúºÎ°úÎèÑ ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ï°∞Ï†ïÏùÑ ÌÜµÌï¥ ÏÑ±Îä•ÏùÑ Í∑πÎåÄÌôîÌï† Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§. Ï¶â, Ïù¥ Ïó∞Íµ¨ÏóêÏÑúÎäî ÌÖåÏä§Ìä∏ ÏãúÏ†êÏóêÏÑú Ïó∞ÏÇ∞ÎüâÏùÑ Ï°∞Ï†àÌïòÏó¨ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÏµúÏ†ÅÌôîÌïòÎäî Ï†ÑÎûµÏù¥ Ïã§Ï†úÎ°ú Ìö®Í≥ºÏ†ÅÏù∏ÏßÄÎ•º Í≤ÄÏ¶ùÌïòÍ≥†, Ìñ•ÌõÑ Î™®Îç∏ Í∞úÎ∞úÏóê ÎØ∏Ïπ† ÏòÅÌñ•ÏùÑ Î∂ÑÏÑùÌïòÍ≥†Ïûê Ìï©ÎãàÎã§. Ï∂îÎ°† Îç∞Ïù¥ÌÑ∞ÏÖã s1K ÏÜåÍ∞ú Î∞è ÏÉùÏÑ± Î∞©Î≤ï s1 Ïó∞Íµ¨ÏóêÏÑúÎäî ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î•º ÏµúÏÜåÌôîÌïòÎ©¥ÏÑúÎèÑ Í∞ïÎ†•Ìïú Ï∂îÎ°† Îä•Î†•ÏùÑ ÌïôÏäµÌï† Ïàò ÏûàÎäîÍ∞ÄÎùºÎäî ÏßàÎ¨∏ÏùÑ Ï§ëÏã¨ÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÑ§Í≥ÑÌïòÏòÄÏäµÎãàÎã§. Ïù¥Î•º ÏúÑÌï¥ Ïó∞Íµ¨ÌåÄÏùÄ s1KÎùºÎäî ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Íµ¨Ï∂ïÌïòÏòÄÏúºÎ©∞, Ïù¥Îäî 1,000Í∞úÏùò Í≥†ÌíàÏßà Îç∞Ïù¥ÌÑ∞Î°ú Ïù¥Î£®Ïñ¥ÏßÑ ÏÜåÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§. ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÌïôÏäµÏóêÎäî ÏàòÎ∞±Îßå Í∞úÏùò Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌïòÎã§Í≥† ÏïåÎ†§Ï†∏ ÏûàÏßÄÎßå, Î≥∏ Ïó∞Íµ¨ÏóêÏÑúÎäî ÏÜåÎüâÏùò Îç∞Ïù¥ÌÑ∞Î•º Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÏÑ†Î≥ÑÌïòÏó¨ ÌõàÎ†®ÌïòÎ©¥ Í∞ïÎ†•Ìïú Ï∂îÎ°† Îä•Î†•ÏùÑ ÌïôÏäµÌï† Ïàò ÏûàÎäîÏßÄ Í≤ÄÏ¶ùÌïòÎäî Í≤ÉÏùÑ Î™©ÌëúÎ°ú ÌïòÏòÄÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄ Ï¢åÏ∏°: Ï∂îÎ°† Îç∞Ïù¥ÌÑ∞ÏÖã s1K Íµ¨ÏÑ±, Ïö∞Ï∏°: s1-32B Î™®Îç∏Í≥º Îã§Î•∏ Î™®Îç∏Îì§Ïùò ÏÉòÌîå Ìö®Ïú®ÏÑ± ÎπÑÍµê(Îç∞Ïù¥ÌÑ∞ÏÖã Í∑úÎ™®ÏôÄ MATH500 Ï†ïÌôïÎèÑ) Ïù¥ÎØ∏ÏßÄ Ï¢åÏ∏°: Ï∂îÎ°† Îç∞Ïù¥ÌÑ∞ÏÖã s1K Íµ¨ÏÑ±, Ïö∞Ï∏°: s1-32B Î™®Îç∏Í≥º Îã§Î•∏ Î™®Îç∏Îì§Ïùò ÏÉòÌîå Ìö®Ïú®ÏÑ± ÎπÑÍµê(Îç∞Ïù¥ÌÑ∞ÏÖã Í∑úÎ™®ÏôÄ MATH500 Ï†ïÌôïÎèÑ) 1248√ó642 117 KB s1K Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Íµ¨Ï∂ïÌïòÍ∏∞ ÏúÑÌï¥ Ïó∞Íµ¨ÌåÄÏùÄ Î®ºÏ†Ä Îã§ÏñëÌïú Î∂ÑÏïºÏóêÏÑú 59,029Í∞úÏùò Î¨∏Ï†úÎ•º ÏàòÏßëÌïòÏòÄÏäµÎãàÎã§. Ïù¥ Îç∞Ïù¥ÌÑ∞Îäî ÏàòÌïô, Í≥ºÌïô, ÎÖºÎ¶¨, Ïñ∏Ïñ¥Ìïô, ÌôïÎ•† Îì± Îã§ÏñëÌïú ÎèÑÎ©îÏù∏ÏùÑ Ìè¨Ìï®ÌïòÎ©∞, Î¨∏Ï†úÏùò ÎÇúÏù¥ÎèÑÏôÄ Ïú†ÌòïÎèÑ Í¥ëÎ≤îÏúÑÌïòÍ≤å Î∂ÑÌè¨ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Ïù¥ÌõÑ Ïó∞Íµ¨ÌåÄÏùÄ **ÌíàÏßà(Quality), ÎÇúÏù¥ÎèÑ(Difficulty), Îã§ÏñëÏÑ±(Diversity)**Ïù¥ÎùºÎäî ÏÑ∏ Í∞ÄÏßÄ Í∏∞Ï§ÄÏùÑ ÏÑ§Ï†ïÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º ÌïÑÌÑ∞ÎßÅÌïòÏòÄÏäµÎãàÎã§. Î®ºÏ†Ä ÌíàÏßà(Quality) Í∏∞Ï§ÄÏùÑ Ï†ÅÏö©ÌïòÏó¨, ÏûòÎ™ªÎêú ÌòïÏãùÏùò Îç∞Ïù¥ÌÑ∞ÎÇò ÏùòÎØ∏Í∞Ä Î™ÖÌôïÌïòÏßÄ ÏïäÏùÄ Î¨∏Ï†úÎì§ÏùÑ Ï†úÍ±∞ÌïòÏòÄÏäµÎãàÎã§. Ïù¥ÌõÑ ÎÇúÏù¥ÎèÑ(Difficulty) Í∏∞Ï§ÄÏùÑ Ï†ÅÏö©ÌïòÏó¨, Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ ÏúÑÌï¥ ÍπäÏùÄ Ï∂îÎ°†Ïù¥ ÌïÑÏöîÌïú Î¨∏Ï†úÎì§ÏùÑ ÏÑ†Î≥ÑÌïòÏòÄÏäµÎãàÎã§. ÎßàÏßÄÎßâÏúºÎ°ú Îã§ÏñëÏÑ±(Diversity) Í∏∞Ï§ÄÏùÑ Ï†ÅÏö©ÌïòÏó¨, ÌäπÏ†ï Ïú†ÌòïÏùò Î¨∏Ï†úÏóê ÏπòÏö∞ÏπòÏßÄ ÏïäÎèÑÎ°ù Îã§ÏñëÌïú Ï£ºÏ†úÏùò Î¨∏Ï†úÎì§ÏùÑ Í∑†Ìòï ÏûàÍ≤å Ìè¨Ìï®ÌïòÎèÑÎ°ù Íµ¨ÏÑ±ÌïòÏòÄÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Í≥ºÏ†ïÏùÑ Í±∞Ï≥ê ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú 1,000Í∞úÏùò Í≥†ÌíàÏßà Î¨∏Ï†ú(s1K)Í∞Ä ÏÑ†Ï†ïÎêòÏóàÏäµÎãàÎã§. Î≥∏ Ïó∞Íµ¨ÏóêÏÑú Ï§ëÏöîÌïú Ï†êÏùÄ, s1K Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ Í∏∞Ï°¥ ÎåÄÌòï Îç∞Ïù¥ÌÑ∞ÏÖãÎ≥¥Îã§ Ìõ®Ïî¨ Ï†ÅÏùÄ ÏñëÏùò Îç∞Ïù¥ÌÑ∞Î•º Ìè¨Ìï®ÌïòÍ≥† ÏûàÏùåÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥†, Ìö®Í≥ºÏ†ÅÏù∏ ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ï°∞Ï†ïÏùÑ ÌÜµÌï¥ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌÅ¨Í≤å Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÎã§Îäî Ï†ê ÏûÖÎãàÎã§. Ïù¥Îäî Ìñ•ÌõÑ AI Î™®Îç∏ÏùÑ Í∞úÎ∞úÌïòÎäî Í≥ºÏ†ïÏóêÏÑú Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÍ≥º Î™®Îç∏ ÌõàÎ†® ÎπÑÏö©ÏùÑ Ï§ÑÏù¥Îäî Îç∞ Ï§ëÏöîÌïú ÏãúÏÇ¨Ï†êÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ(Test-Time Scaling)Í≥º Budget Forcing Í∏∞Ï°¥Ïùò ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏ÏùÄ Ï£ºÏñ¥ÏßÑ ÏûÖÎ†•Ïóê ÎåÄÌï¥ Ï¶âÍ∞ÅÏ†ÅÏù∏ ÏùëÎãµÏùÑ ÏÉùÏÑ±ÌïòÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏñ¥ ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå Î≥µÏû°Ìïú Î¨∏Ï†úÎ•º Ìï¥Í≤∞Ìï† ÎïåÎäî Ìïú Î≤àÏùò Í≥ÑÏÇ∞ÎßåÏúºÎ°ú ÏµúÏ†ÅÏùò ÎãµÏùÑ ÎèÑÏ∂úÌïòÎäî Í≤ÉÏù¥ Ïñ¥Î†µÍ∏∞ ÎïåÎ¨∏Ïóê, Î™®Îç∏Ïù¥ Îçî ÍπäÏù¥ ÏÇ¨Í≥†ÌïòÍ≥† Îã§Í∞ÅÏ†ÅÏù∏ Í≤ÄÌÜ†Î•º ÏàòÌñâÌï† Ïàò ÏûàÎèÑÎ°ù Ïú†ÎèÑÌïòÎäî Î∞©Î≤ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§. Ïù¥Îü¨Ìïú Ï†ëÍ∑º Î∞©Ïãù Ï§ë ÌïòÎÇòÍ∞Ä **ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ(Test-time Scaling)**Ïù¥Î©∞, Ïù¥Îäî Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÎäî Í≥ºÏ†ïÏóêÏÑúÍ∞Ä ÏïÑÎãàÎùº, ÌÖåÏä§Ìä∏(Ï∂îÎ°†) ÏãúÏ†êÏóêÏÑú Ï∂îÍ∞ÄÏ†ÅÏù∏ Ïó∞ÏÇ∞ÏùÑ ÏàòÌñâÌïòÏó¨ ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Îäî Î∞©Î≤ïÎ°†ÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§. ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅÏóêÎäî ÌÅ¨Í≤å Îëê Í∞ÄÏßÄ Î∞©ÏãùÏù¥ ÏûàÏäµÎãàÎã§: Ï≤´ Î≤àÏß∏Îäî **Î≥ëÎ†¨ Î∞©Ïãù(Parallel Scaling)**ÏúºÎ°ú, Î™®Îç∏Ïù¥ Ïó¨Îü¨ Í∞úÏùò ÎèÖÎ¶ΩÏ†ÅÏù∏ Ï∂úÎ†•ÏùÑ ÏÉùÏÑ±Ìïú ÌõÑ Í∞ÄÏû• Ï†ÅÏ†àÌïú ÎãµÏùÑ ÏÑ†ÌÉùÌïòÎäî Î∞©Î≤ïÏûÖÎãàÎã§. Ïù¥Îäî Îã§ÏàòÏùò Ï∂úÎ†•ÏùÑ ÎπÑÍµêÌïòÍ≥† Îã§ÏàòÍ≤∞ Î∞©Ïãù(Majority Voting)Ïù¥ÎÇò ÌõÑÏ≤òÎ¶¨ ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÌôúÏö©ÌïòÏó¨ ÏµúÏ¢Ö ÎãµÏùÑ Í≤∞Ï†ïÌïòÎäî Î∞©ÏãùÏúºÎ°ú, ÌÖåÏä§Ìä∏ Í≥ºÏ†ïÏóêÏÑú Ïó¨Îü¨ Î≤àÏùò ÎèÖÎ¶ΩÏ†ÅÏù∏ Ï∂îÎ°†ÏùÑ ÏàòÌñâÌïòÎäî Í≤ÉÏù¥ ÌäπÏßïÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù¥ Î∞©ÏãùÏùÄ Í≥ÑÏÇ∞ÎüâÏù¥ Ï¶ùÍ∞ÄÌïòÎ©∞, Î∞òÎìúÏãú ÏµúÏ†ÅÏùò ÎãµÏùÑ ÎèÑÏ∂úÌïòÎäî Í≤ÉÏùÄ ÏïÑÎãàÍ∏∞ ÎïåÎ¨∏Ïóê ÌïúÍ≥ÑÍ∞Ä Ï°¥Ïû¨Ìï©ÎãàÎã§ . Îëê Î≤àÏß∏ Î∞©ÏãùÏùÄ **ÏàúÏ∞® Î∞©Ïãù(Sequential Scaling)**ÏúºÎ°ú, Î™®Îç∏Ïù¥ ÌïòÎÇòÏùò ÎãµÏùÑ ÏÉùÏÑ±ÌïòÎäî Í≥ºÏ†ïÏóêÏÑú Ïù¥Ï†Ñ ÏÇ¨Í≥† Í≥ºÏ†ï(Reasoning Trace)ÏùÑ ÌôúÏö©ÌïòÏó¨ Ï†êÏßÑÏ†ÅÏúºÎ°ú Îçî ÎÇòÏùÄ ÎãµÏùÑ Ï∞æÏïÑÍ∞ÄÎäî Î∞©ÏãùÏûÖÎãàÎã§. Ïù¥ Ï†ëÍ∑ºÎ≤ïÏùÄ Îã®Í≥ÑÏ†ÅÏù∏ ÏÇ¨Í≥† Í≥ºÏ†ïÏù¥ ÌïÑÏöîÌïú Î¨∏Ï†úÏóêÏÑú Ïú†Ïö©ÌïòÍ≤å ÏûëÏö©Ìï† Ïàò ÏûàÏúºÎ©∞, Ïó∞Íµ¨ÏßÑÏùÄ Ïù¥Î•º ÏµúÏ†ÅÌôîÌïòÍ∏∞ ÏúÑÌï¥ Budget Forcing Í∏∞Î≤ïÏùÑ ÎèÑÏûÖÌïòÏòÄÏäµÎãàÎã§ . Budget Forcing Í∏∞Î≤ïÏùÑ ÌôúÏö©Ìïú Î™®Îç∏ Ï∂îÎ°† Í≥ºÏ†ï: Î™®Îç∏ÏùÄ \"...is 2.\"ÏóêÏÑú Ï∂úÎ†•ÏùÑ ÏôÑÎ£åÌïòÎ†§Í≥† ÌñàÏúºÎÇò, 'Wait' ÌÜ†ÌÅ∞ÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÎãµÏùÑ Ïä§Ïä§Î°ú Í∞úÏÑ†ÌïòÎèÑÎ°ù Ìï® Budget Forcing Í∏∞Î≤ïÏùÑ ÌôúÏö©Ìïú Î™®Îç∏ Ï∂îÎ°† Í≥ºÏ†ï: Î™®Îç∏ÏùÄ \"...is 2.\"ÏóêÏÑú Ï∂úÎ†•ÏùÑ ÏôÑÎ£åÌïòÎ†§Í≥† ÌñàÏúºÎÇò, 'Wait' ÌÜ†ÌÅ∞ÏùÑ Ï∂îÍ∞ÄÌïòÏó¨ ÎãµÏùÑ Ïä§Ïä§Î°ú Í∞úÏÑ†ÌïòÎèÑÎ°ù Ìï® 942√ó800 41.2 KB Budget ForcingÏùÄ Î™®Îç∏Ïùò ÏÇ¨Í≥† Í≥ºÏ†ï(Thinking Phase)ÏùÑ Ïù∏ÏúÑÏ†ÅÏúºÎ°ú Ï°∞Ï†àÌïòÎäî Í∏∞Î≤ïÏûÖÎãàÎã§. ÏùºÎ∞òÏ†ÅÏúºÎ°ú Ïñ∏Ïñ¥ Î™®Îç∏ÏùÄ ÏµúÎåÄÌïú Îπ®Î¶¨ ÎãµÏùÑ ÏÉùÏÑ±ÌïòÎ†§Îäî Í≤ΩÌñ•Ïù¥ ÏûàÏúºÎ©∞, Ïù¥Îäî Î≥µÏû°Ìïú Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäî Îç∞ ÏûàÏñ¥ Ï∂©Î∂ÑÌïú ÏÇ¨Í≥†Î•º Í±∞ÏπòÏßÄ ÏïäÍ≥† ÏÑ±Í∏âÌïú Í≤∞Î°†ÏùÑ ÎÇ¥Î¶¨Îäî Î¨∏Ï†úÎ•º Ï¥àÎûòÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïó∞Íµ¨ÌåÄÏùÄ Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Îëê Í∞ÄÏßÄ Í∏∞Î≤ïÏùÑ Ï†ÅÏö©ÌïòÏòÄÏäµÎãàÎã§. Ï≤´Ïß∏, Î™®Îç∏Ïù¥ ÏùºÏ†ïÌïú ÌÜ†ÌÅ∞ Ïàò Ïù¥ÏÉÅÏùÑ ÏÉùÏÑ±Ìï† ÎïåÍπåÏßÄ ÏÇ¨Í≥† Í≥ºÏ†ïÏùÑ Ïú†ÏßÄÌïòÎèÑÎ°ù ÏÑ§Ï†ïÌïòÏó¨, Î™®Îç∏Ïù¥ Ï∂©Î∂ÑÌïú ÏÇ¨Í≥†Î•º Í±∞Ïπú ÌõÑ ÎãµÏùÑ ÏÉùÏÑ±ÌïòÎèÑÎ°ù Ïú†ÎèÑÌïòÏòÄÏäµÎãàÎã§. ÎëòÏß∏, Î™®Îç∏Ïù¥ ÎãµÏùÑ ÏÉùÏÑ±ÌïòÎ†§Í≥† Ìï† Îïå \"Wait\"Ïù¥ÎùºÎäî Ï∂îÍ∞ÄÏ†ÅÏù∏ Ïã†Ìò∏Î•º ÏûÖÎ†•ÌïòÏó¨, Î™®Îç∏Ïù¥ ÏßÄÍ∏àÍπåÏßÄÏùò Ï∂úÎ†•ÏùÑ Ìïú Î≤à Îçî Í≤ÄÌÜ†Ìï† Í∏∞ÌöåÎ•º Ï†úÍ≥µÌïòÏòÄÏäµÎãàÎã§ . Ïù¥Îü¨Ìïú Î∞©Î≤ïÏùÑ ÌÜµÌï¥, Î™®Îç∏Ïù¥ Î¨∏Ï†úÎ•º Îçî Ï≤†Ï†ÄÌûà Î∂ÑÏÑùÌïòÍ≥† Ï§ëÍ∞Ñ Í≥ºÏ†ïÏóêÏÑú Ïò§Î•òÎ•º ÏàòÏ†ïÌïòÎäî Îä•Î†•Ïù¥ Ìñ•ÏÉÅÎêòÏóàÏäµÎãàÎã§. ÌäπÌûà, Budget ForcingÏùÑ Ï†ÅÏö©Ìïú Î™®Îç∏ÏùÄ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäî Í≥ºÏ†ïÏóêÏÑú Î≥¥Îã§ Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú ÏÇ¨Í≥†ÌïòÎäî Í≤ΩÌñ•ÏùÑ Î≥¥ÏòÄÏúºÎ©∞, ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú ÎÜíÏùÄ Ï†ïÌôïÎèÑÎ•º Îã¨ÏÑ±Ìï† Ïàò ÏûàÏóàÏäµÎãàÎã§. Ïó∞Íµ¨ÌåÄÏùÄ Budget ForcingÏù¥ Ï†ÅÏö©Îêú s1-32B Î™®Îç∏Ïù¥ Í∏∞Ï°¥Ïùò o1-preview Î™®Îç∏Î≥¥Îã§ Îçî ÎÇòÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïù¥Î©∞, ÌäπÌûà ÏàòÌïôÏ†Å Ï∂îÎ°†Í≥º Í≥ºÌïô Î¨∏Ï†ú Ìï¥Í≤∞ÏóêÏÑú Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•ÏùÑ Î∞úÌúòÌñàÏùåÏùÑ Ïã§ÌóòÏùÑ ÌÜµÌï¥ ÌôïÏù∏ÌïòÏòÄÏäµÎãàÎã§ . Ïã§Ìóò Í≤∞Í≥º Î∞è ÏÑ±Îä• Ï†ïÎ¶¨, Ablation AIME(Ï¢åÏ∏°), MATH500(Ï§ëÏïô), GPQA Diamond(Ïö∞Ï∏°) Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑúÏùò ÏßàÎ¨∏ Î∞è s1-32BÏùò ÎãµÎ≥Ä ÏòàÏãú AIME(Ï¢åÏ∏°), MATH500(Ï§ëÏïô), GPQA Diamond(Ïö∞Ï∏°) Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑúÏùò ÏßàÎ¨∏ Î∞è s1-32BÏùò ÎãµÎ≥Ä ÏòàÏãú 1028√ó1319 220 KB Î≥∏ Ïó∞Íµ¨ÏóêÏÑúÎäî Budget ForcingÏùÑ Ï†ÅÏö©Ìïú s1-32B Î™®Îç∏ÏùÑ Îã§ÏñëÌïú Î≤§ÏπòÎßàÌÅ¨ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÌôúÏö©ÌïòÏó¨ ÌèâÍ∞ÄÌïòÏòÄÏäµÎãàÎã§. Ïã§ÌóòÏóê ÏÇ¨Ïö©Îêú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§. Ï≤´Ïß∏, AIME24Îäî ÎØ∏Íµ≠ ÏàòÌïô Ïò¨Î¶ºÌîºÏïÑÎìú(AIME) Î¨∏Ï†úÎì§Î°ú Íµ¨ÏÑ±Îêú Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú, Î≥µÏû°Ìïú ÏàòÌïôÏ†Å ÏÇ¨Í≥†Í∞Ä ÏöîÍµ¨ÎêòÎäî Í≥†ÎÇúÏù¥ÎèÑ Î¨∏Ï†úÎì§Î°ú Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÏäµÎãàÎã§. ÎëòÏß∏, MATH500ÏùÄ Îã§ÏñëÌïú ÏàòÌïô Î¨∏Ï†úÎì§ÏùÑ Ìè¨Ìï®Ìïú ÏùºÎ∞òÏ†ÅÏù∏ ÌèâÍ∞ÄÏö© Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§. ÏÖãÏß∏, GPQA DiamondÎäî Î∞ïÏÇ¨ ÏàòÏ§ÄÏùò Í≥ºÌïô Î¨∏Ï†úÎì§ÏùÑ Ìè¨Ìï®Ìïú Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú, Í≥†ÎÇúÏù¥ÎèÑ ÏßàÎ¨∏Îì§Ïóê ÎåÄÌïú Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ Ï∏°Ï†ïÌïòÎäî Îç∞ ÏÇ¨Ïö©ÎêòÏóàÏäµÎãàÎã§. s1 Î™®Îç∏Í≥º o1, Gemini 2.0 Flash Think, Qwen, QwQ, r1 Îì± Í∏∞Ï°¥Ïùò Ï£ºÏöî Î™®Îç∏Îì§Ïùò ÏÑ±Îä• ÎπÑÍµê: AIME24, MATH500, GPQA Diamond s1 Î™®Îç∏Í≥º o1, Gemini 2.0 Flash Think, Qwen, QwQ, r1 Îì± Í∏∞Ï°¥Ïùò Ï£ºÏöî Î™®Îç∏Îì§Ïùò ÏÑ±Îä• ÎπÑÍµê: AIME24, MATH500, GPQA Diamond 1000√ó1268 140 KB Ïã§Ìóò Í≤∞Í≥º, s1-32B Î™®Îç∏ÏùÄ Í∏∞Ï°¥Ïùò OpenAI o1-preview Î™®Îç∏Î≥¥Îã§ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Í∏∞Î°ùÌïòÏòÄÏäµÎãàÎã§ . ÌäπÌûà Budget ForcingÏùÑ Ï†ÅÏö©Ìïú Í≤ΩÏö∞, AIME24 Î¨∏Ï†úÏóêÏÑú ÏÑ±Îä•Ïù¥ 50%ÏóêÏÑú 56.7%Î°ú ÏÉÅÏäπÌïòÏòÄÏúºÎ©∞, Ïù¥Îäî Îã®ÏàúÌïú Í∏∞Î≤ïÎßåÏúºÎ°úÎèÑ ÌÖåÏä§Ìä∏ ÏãúÏ†êÏóêÏÑú ÏÑ±Îä•ÏùÑ Í∑πÎåÄÌôîÌï† Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§ . ÎòêÌïú MATH500Í≥º GPQA DiamondÏóêÏÑúÎèÑ Í∏∞Ï°¥ Î™®Îç∏Î≥¥Îã§ Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•ÏùÑ Î≥¥ÏòÄÏúºÎ©∞, Ïù¥Îäî 1,000Í∞ú ÏÉòÌîåÎßåÏùÑ ÏÇ¨Ïö©Ìïú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎßåÏúºÎ°úÎèÑ Î™®Îç∏Ïùò Ï∂îÎ°† Îä•Î†•ÏùÑ Í∞ïÌôîÌï† Ïàò ÏûàÏùåÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§. Ablation Ïó∞Íµ¨ ‚Äì s1K Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Ìö®Í≥º Î∂ÑÏÑù Ablation Ïó∞Íµ¨ ‚Äì s1K Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Ìö®Í≥º Î∂ÑÏÑù Budget Forcing(BF)Í≥º Îã§ÏñëÌïú Ïä§ÏºÄÏùºÎßÅ Í∏∞Î≤ï(Token/Step/Class-Conditional Control, Rejection Sampling) ÎπÑÍµê Budget Forcing(BF)Í≥º Îã§ÏñëÌïú Ïä§ÏºÄÏùºÎßÅ Í∏∞Î≤ï(Token/Step/Class-Conditional Control, Rejection Sampling) ÎπÑÍµê Ï∂îÍ∞ÄÏ†ÅÏù∏ Ablation Ïó∞Íµ¨Î•º ÌÜµÌï¥, Îç∞Ïù¥ÌÑ∞Ïùò ÏñëÍ≥º Îã§ÏñëÏÑ±Ïù¥ Î™®Îç∏ ÏÑ±Îä•Ïóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Î∂ÑÏÑùÌïòÏòÄÏäµÎãàÎã§. Ïó∞Íµ¨ÌåÄÏùÄ Î™®Îç∏ÏùÑ ÌõàÎ†®Ìï† Îïå Îç∞Ïù¥ÌÑ∞Ïùò ÏàòÎ•º Îã¨Î¶¨ÌïòÏó¨ Ïã§ÌóòÏùÑ ÏßÑÌñâÌïòÏòÄÏúºÎ©∞, Ïù¥Î•º ÌÜµÌï¥ Ï†ÅÏ†àÌïú ÎÇúÏù¥ÎèÑÏôÄ Îã§ÏñëÏÑ±ÏùÑ Í∞ÄÏßÑ ÏÜåÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖã(s1K)Ïù¥ Ìö®Í≥ºÏ†ÅÏù∏ Î™®Îç∏ ÌïôÏäµÏóê Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÌïúÎã§Îäî ÏÇ¨Ïã§ÏùÑ Î∞úÍ≤¨ÌïòÏòÄÏäµÎãàÎã§ . ÎòêÌïú Budget ForcingÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞Î≥¥Îã§ ÏÇ¨Ïö©Ìïú Í≤ΩÏö∞Ïóê Îçî ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Îã¨ÏÑ±ÌïòÏòÄÏúºÎ©∞, Ïù¥Îäî Î™®Îç∏Ïù¥ Ï∂©Î∂ÑÌïú ÏÇ¨Í≥† Í≥ºÏ†ïÏùÑ Í±∞Ïπ† Ïàò ÏûàÎèÑÎ°ù Ïú†ÎèÑÌïòÎäî Í≤ÉÏù¥ Îß§Ïö∞ Ï§ëÏöîÌïòÎã§Îäî Í≤ÉÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§ . Budget ForcingÏùÑ Ï†ÅÏö©Ìïú ÏàúÏ∞®(Sequential) Î∞©Ïãù(a)Í≥º Îã§ÏàòÍ≤∞ Ìà¨ÌëúÏùò Î≥ëÎ†¨(Parallel) Î∞©Ïãù(b)Ïùò ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ ÏÑ±Îä• ÎπÑÍµê Budget ForcingÏùÑ Ï†ÅÏö©Ìïú ÏàúÏ∞®(Sequential) Î∞©Ïãù(a)Í≥º Îã§ÏàòÍ≤∞ Ìà¨ÌëúÏùò Î≥ëÎ†¨(Parallel) Î∞©Ïãù(b)Ïùò ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ ÏÑ±Îä• ÎπÑÍµê 1245√ó678 150 KB Î≥∏ Ïó∞Íµ¨Îäî ÎòêÌïú Budget ForcingÏù¥ Îã§Î•∏ ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ Í∏∞Î≤ïÎì§Í≥º ÎπÑÍµêÌïòÏó¨ Ïñ¥Îñ§ Ïù¥Ï†êÏùÑ Í∞ÄÏßÄÎäîÏßÄ ÌèâÍ∞ÄÌïòÏòÄÏäµÎãàÎã§. Îã§ÏàòÍ≤∞ Î∞©Ïãù(Majority Voting)Í≥º Í∞ôÏùÄ Î≥ëÎ†¨ Ïä§ÏºÄÏùºÎßÅ Í∏∞Î≤ïÍ≥º ÎπÑÍµêÌïú Í≤∞Í≥º, Budget ForcingÏùÑ ÌôúÏö©Ìïú ÏàúÏ∞®Ï†Å(Sequential) Ïä§ÏºÄÏùºÎßÅ Î∞©ÏãùÏù¥ Îçî Ìö®Í≥ºÏ†ÅÏù∏ Í≤ÉÏúºÎ°ú ÎÇòÌÉÄÎÇ¨ÏäµÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïù¥ Ïó¨Îü¨ Î≤à ÏÉùÍ∞ÅÌïòÍ≥† ÎãµÏùÑ ÏàòÏ†ïÌïòÎäî Í≥ºÏ†ïÏù¥ ÏµúÏ¢ÖÏ†ÅÏù∏ ÏÑ±Îä• Ìñ•ÏÉÅÏóê Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÌïúÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§ . ÎÖºÏùò, Ìñ•ÌõÑ Í≥ºÏ†ú Î∞è Í≤∞Î°† Î≥∏ Ïó∞Íµ¨Îäî ÏÜåÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ ÌôòÍ≤ΩÏóêÏÑúÎèÑ ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅÏùÑ ÌôúÏö©ÌïòÏó¨ Í∞ïÎ†•Ìïú AI Î™®Îç∏ÏùÑ Í∞úÎ∞úÌï† Ïàò ÏûàÏùåÏùÑ ÏûÖÏ¶ùÌïòÏòÄÏäµÎãàÎã§. ÌäπÌûà, 1,000Í∞úÏùò Í≥†ÌíàÏßà Îç∞Ïù¥ÌÑ∞ÏÖã(s1K)ÎßåÏùÑ ÌôúÏö©ÌïòÏó¨ÎèÑ Ìö®Í≥ºÏ†ÅÏù∏ ÌïôÏäµÏù¥ Í∞ÄÎä•ÌïòÎ©∞, Ïù¥Î•º ÌÜµÌï¥ Î™®Îç∏Ïùò Ïó∞ÏÇ∞ÎüâÏùÑ ÏµúÏ†ÅÌôîÌïòÎ©¥ÏÑúÎèÑ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Ïú†ÏßÄÌï† Ïàò ÏûàÏùåÏùÑ Î≥¥ÏòÄÏäµÎãàÎã§. Ïù¥Îäî Ìñ•ÌõÑ AI Î™®Îç∏ÏùÑ ÏµúÏ†ÅÌôîÌïòÎäî Îç∞ ÏûàÏñ¥ Ï§ëÏöîÌïú ÏãúÏÇ¨Ï†êÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. ÌïòÏßÄÎßå Î≥∏ Ïó∞Íµ¨ÏóêÏÑúÎèÑ Î™á Í∞ÄÏßÄ ÌïúÍ≥ÑÏ†êÏù¥ Ï°¥Ïû¨Ìï©ÎãàÎã§: Ï≤´Ïß∏, Budget Forcing Í∏∞Î≤ïÏù¥ Î™®Îì† Ïú†ÌòïÏùò Î¨∏Ï†úÏóêÏÑú ÎèôÏùºÌïú Ìö®Í≥ºÎ•º Î∞úÌúòÌïòÎäîÏßÄÏóê ÎåÄÌïú Ï∂îÍ∞ÄÏ†ÅÏù∏ Ïó∞Íµ¨Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥, ÌäπÏ†ïÌïú ÎèÑÎ©îÏù∏ÏóêÏÑúÎäî Î™®Îç∏Ïù¥ Ï∂îÍ∞ÄÏ†ÅÏù∏ ÏÇ¨Í≥† Í≥ºÏ†ïÏùÑ Í±∞ÏπúÎã§Í≥† Ìï¥ÏÑú Î∞òÎìúÏãú Îçî Ï¢ãÏùÄ ÎãµÏùÑ ÏÉùÏÑ±ÌïòÎäî Í≤ÉÏù¥ ÏïÑÎãê ÏàòÎèÑ ÏûàÏäµÎãàÎã§. ÎëòÏß∏, Îçî Í∏¥ Î¨∏Îß•ÏùÑ Îã§Î£∞ Ïàò ÏûàÎèÑÎ°ù Î™®Îç∏ÏùÑ ÌôïÏû•ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Ïó∞Íµ¨Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÌòÑÏû¨Ïùò Ï†ëÍ∑º Î∞©ÏãùÏùÄ ÏùºÏ†ïÌïú ÌÜ†ÌÅ∞ Ïàò ÎÇ¥ÏóêÏÑú ÏÇ¨Í≥† Í≥ºÏ†ïÏùÑ Ï°∞Ï†ïÌïòÎäî Í≤ÉÏù¥Í∏∞ ÎïåÎ¨∏Ïóê, Î≥¥Îã§ Í∏¥ Î¨∏Îß•ÏùÑ Îã§Î£∞ Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥ÑÎêú ÏÉàÎ°úÏö¥ Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤òÍ∞Ä ÌïÑÏöîÌï† Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏäµÎãàÎã§ . Ìñ•ÌõÑ Ïó∞Íµ¨ÏóêÏÑúÎäî Budget ForcingÏùÑ ÎçîÏö± Ï†ïÍµêÌïòÍ≤å ÏÑ§Í≥ÑÌïòÏó¨, Îã§ÏñëÌïú ÎèÑÎ©îÏù∏ÏóêÏÑúÏùò Ï†ÅÏö© Í∞ÄÎä•ÏÑ±ÏùÑ ÎÑìÌûàÎäî Î∞©Ìñ•ÏúºÎ°ú ÌôïÏû•Îê† Ïàò ÏûàÏùÑ Í≤ÉÏûÖÎãàÎã§. ÎòêÌïú, Í∞ïÌôî ÌïôÏäµÍ≥ºÏùò Í≤∞Ìï©ÏùÑ ÌÜµÌï¥ Budget ForcingÏùÑ Î≥¥Îã§ Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÌôúÏö©ÌïòÎäî Î∞©Î≤ïÏùÑ ÌÉêÏÉâÌï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Ïó∞Íµ¨Í∞Ä ÏßÑÌñâÎêúÎã§Î©¥, Î≥¥Îã§ Ìö®Ïú®Ï†ÅÏù∏ ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ Í∏∞Î≤ïÏù¥ Í∞úÎ∞úÎê† Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏúºÎ©∞, Ïù¥Îäî Ìñ•ÌõÑ Ïù∏Í≥µÏßÄÎä• Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Îäî Ï§ëÏöîÌïú Í∏∞Ïà†Ï†Å ÎèåÌååÍµ¨Í∞Ä Îê† Í≤ÉÏûÖÎãàÎã§.",
      "summary": "### ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ(Test-Time Scaling)ÏùÑ Îã®ÏàúÌïòÍ≤å Íµ¨ÌòÑÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Ïó∞Íµ¨\n\n**Summary:** Î≥∏ Ïó∞Íµ¨Îäî ÏµúÏÜåÌïúÏùò Îç∞Ïù¥ÌÑ∞ÏôÄ Ïó∞ÏÇ∞ÎüâÏúºÎ°ú Î™®Îç∏ ÏÑ±Îä•ÏùÑ Í∑πÎåÄÌôîÌï† Ïàò ÏûàÎäî Í∞ÄÏû• Îã®ÏàúÌïòÎ©¥ÏÑúÎèÑ Ìö®Í≥ºÏ†ÅÏù∏ ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ Í∏∞Î≤ïÏùÑ ÌÉêÏÉâÌïúÎã§. Ïó∞Íµ¨ÌåÄÏùÄ Budget ForcingÏù¥ÎùºÎäî Í∞úÎÖêÏùÑ ÎèÑÏûÖÌïòÏó¨, s1-32B Î™®Îç∏Ïù¥ Îã® 1,000Í∞úÏùò Í≥†ÌíàÏßà Îç∞Ïù¥ÌÑ∞Î°ú Íµ¨ÏÑ±Îêú s1K Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í∏∞Ï°¥ Î™®Îç∏Î≥¥Îã§ Îõ∞Ïñ¥ÎÇú Ï∂îÎ°† ÏÑ±Îä•ÏùÑ Í∏∞Î°ùÌï®ÏùÑ Î≥¥Ïó¨Ï§ÄÎã§. s1K Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ 59,029Í∞úÏùò Î¨∏Ï†ú Ï§ë ÌíàÏßà, ÎÇúÏù¥ÎèÑ, Îã§ÏñëÏÑ±ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÌïÑÌÑ∞ÎßÅÌïòÏó¨ ÏÑ†Ï†ïÎêú 1,000Í∞úÏùò Î¨∏Ï†úÎ°ú Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÎã§. \n\nÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅÏùÄ Î™®Îç∏Ïù¥ Î≥µÏû°Ìïú Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Ï∂îÍ∞Ä Ïó∞ÏÇ∞ÏùÑ ÏàòÌñâÌïòÎäî Î∞©Î≤ïÏúºÎ°ú, Î≥ëÎ†¨ Î∞©ÏãùÍ≥º ÏàúÏ∞® Î∞©ÏãùÏúºÎ°ú ÎÇòÎâúÎã§. Budget ForcingÏùÄ Î™®Îç∏Ïùò ÏÇ¨Í≥† Í≥ºÏ†ïÏùÑ Ï°∞Ï†àÌïòÏó¨ Îçî ÎÇòÏùÄ ÎãµÏùÑ ÎèÑÏ∂úÌïòÎèÑÎ°ù Ïú†ÎèÑÌïòÎäî Í∏∞Î≤ïÏù¥Îã§. Ïù¥Îü¨Ìïú Í∏∞Î≤ïÏùÑ ÌÜµÌï¥ s1-32B Î™®Îç∏ÏùÄ AIME24, MATH500, GPQA DiamondÏôÄ Í∞ôÏùÄ Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑú Í∏∞Ï°¥ Î™®Îç∏Î≥¥Îã§ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Í∏∞Î°ùÌïòÏòÄÎã§.\n\nÏó∞Íµ¨ Í≤∞Í≥º, Budget ForcingÏù¥ Ï†ÅÏö©Îêú Í≤ΩÏö∞, Î™®Îç∏ÏùÄ Î¨∏Ï†úÎ•º Îçî Ï≤†Ï†ÄÌûà Î∂ÑÏÑùÌïòÍ≥† Ï§ëÍ∞Ñ Í≥ºÏ†ïÏóêÏÑú Ïò§Î•òÎ•º ÏàòÏ†ïÌïòÎäî Îä•Î†•Ïù¥ Ìñ•ÏÉÅÎêòÏñ¥ ÏÑ±Îä•Ïù¥ ÌÅ¨Í≤å Í∞úÏÑ†ÎêòÏóàÏùåÏùÑ ÌôïÏù∏ÌïòÏòÄÎã§. Ïó∞Íµ¨ÌåÄÏùÄ Ìñ•ÌõÑ Budget ForcingÏùò Ï†ÅÏö© Í∞ÄÎä•ÏÑ±ÏùÑ ÎÑìÌûàÍ≥†, Îã§ÏñëÌïú ÎèÑÎ©îÏù∏ÏóêÏÑúÏùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÍ∏∞ ÏúÑÌïú Ï∂îÍ∞Ä Ïó∞Íµ¨Í∞Ä ÌïÑÏöîÌïòÎã§Í≥† Í∞ïÏ°∞ÌïòÏòÄÎã§. Î≥∏ Ïó∞Íµ¨Îäî ÏÜåÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖã ÌôòÍ≤ΩÏóêÏÑúÎèÑ Ìö®Í≥ºÏ†ÅÏù∏ AI Î™®Îç∏ Í∞úÎ∞úÏù¥ Í∞ÄÎä•Ìï®ÏùÑ ÏûÖÏ¶ùÌïòÎ©∞, Ìñ•ÌõÑ Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÍ≥º Î™®Îç∏ ÌõàÎ†® ÎπÑÏö© Ï†àÍ∞êÏóê Ï§ëÏöîÌïú Í∏∞Ïó¨Î•º Ìï† Í≤ÉÏúºÎ°ú Í∏∞ÎåÄÎêúÎã§.",
      "classification": "### s1: ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ(Test-Time Scaling)ÏùÑ Îã®ÏàúÌïòÍ≤å Íµ¨ÌòÑÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Ïó∞Íµ¨\n\n**Category:** Research Paper",
      "keyword": "### ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ(Test-Time Scaling)ÏùÑ Îã®ÏàúÌïòÍ≤å Íµ¨ÌòÑÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌïú Ïó∞Íµ¨\n\n**Keywords:** ÌÖåÏä§Ìä∏ ÏãúÏ†ê Ïä§ÏºÄÏùºÎßÅ, Budget Forcing, s1K Îç∞Ïù¥ÌÑ∞ÏÖã, ÏÜåÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞, Î™®Îç∏ ÏÑ±Îä• ÏµúÏ†ÅÌôî"
    },
    {
      "No.": 23,
      "end_point": "https://www.aimodels.fyi/",
      "post_date": null,
      "link": "https://www.aimodels.fyi/papers/arxiv/tensorllm-tensorising-multi-head-attention-enhanced-reasoning",
      "title": "TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs",
      "content": "Overview\r\nIntroduces TensorLLM, a novel approach to compress and enhance large language models\r\nUses tensor decomposition to improve multi-head attention mechanisms\r\nAchieves 3.5x compression while maintaining model performance\r\nDemonstrates enhanced reasoning capabilities on benchmark tasks\r\nProvides new insights into attention head interactions\r\nPlain English Explanation\r\nLarge language models are like massive pattern-recognition machines that require enormous computing power. TensorLLM makes these models more efficient by restructuring how they process information.\r\n\r\nThink of traditional attention mechanisms as multiple spotlights examining different aspects of text. TensorLLM reorganizes these spotlights into a more compact form using mathematical techniques called tensor decomposition - similar to how a jpeg compresses an image while keeping the important details.\r\n\r\nThe researchers found that this reorganization not only saves space but actually helps the model think better. It's like reorganizing a messy desk - you end up with both more space and a better workflow.\r\n\r\nKey Findings\r\nAchieved 3.5x compression while maintaining 98% of original performance\r\nEnhanced reasoning capabilities showed 5% improvement on complex tasks\r\nDiscovered new patterns in how attention heads collaborate\r\nReduced computational costs by 65% during inference\r\nMaintained model accuracy across multiple benchmark tests\r\nTechnical Explanation\r\nThe core innovation lies in applying Tucker decomposition to multi-head attention layers. The attention mechanism is reshaped into a higher-dimensional tensor, then decomposed into smaller, more manageable components.\r\n\r\nThe process preserves crucial relationships between attention heads while eliminating redundant patterns. This restructuring allows for more efficient information flow and better feature extraction.\r\n\r\nThe implementation includes adaptive scaling factors that automatically adjust based on input complexity, ensuring optimal performance across different types of tasks.\r\n\r\nCritical Analysis\r\nThe approach shows promising results but has some limitations. The compression ratio varies significantly depending on model size, with smaller models seeing less benefit. The method also requires careful tuning of decomposition parameters.\r\n\r\nSome questions remain about long-term stability and performance on extremely long sequences. The paper doesn't fully address how the method scales to models larger than 7B parameters.\r\n\r\nThe transformer architecture modifications might need additional validation across different model families and tasks.\r\n\r\nConclusion\r\nTensorLLM represents a significant advance in making large language models more practical and efficient. The ability to compress models while improving their reasoning capabilities could accelerate the deployment of AI systems in resource-constrained environments.\r\n\r\nThe findings suggest that current model architectures might have substantial redundancy that could be eliminated through careful restructuring. This opens new paths for developing more efficient AI systems.",
      "summary": "### TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs\n\n**Summary:** TensorLLM is an innovative approach aimed at compressing and enhancing large language models (LLMs) by utilizing tensor decomposition to refine multi-head attention mechanisms. This method achieves a notable 3.5x compression while retaining 98% of the original model's performance, alongside a 5% improvement in reasoning capabilities on benchmark tasks. The research reveals new patterns in attention head interactions and reduces computational costs by 65% during inference.\n\nIn simpler terms, TensorLLM restructures the way LLMs process data, akin to how jpeg compression works for images, enabling more efficient information processing and better decision-making. The key technical advancement involves applying Tucker decomposition to reshape attention layers into a higher-dimensional tensor, which maintains essential relationships while discarding redundancy. \n\nDespite its promising results, the method has limitations related to varying compression effectiveness based on model size and the need for careful tuning of parameters. Questions about its performance with larger models and across different tasks remain. Overall, TensorLLM signifies a considerable step toward creating more efficient and practical AI systems by exposing and eliminating redundancy within current model architectures.",
      "classification": "### TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs\n\n**Category:** Model",
      "keyword": "### TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs\n\n**Keywords:** Tensor Decomposition, Multi-Head Attention, Model Compression, Reasoning Enhancement, Computational Efficiency"
    },
    {
      "No.": 24,
      "end_point": "https://www.aimodels.fyi/",
      "post_date": null,
      "link": "https://www.aimodels.fyi/papers/arxiv/limo-less-is-more-reasoning",
      "title": "LIMO: Less is More for Reasoning",
      "content": "Overview\r\n‚Ä¢ LIMO is a novel AI reasoning approach that achieves strong performance with minimal training data\r\n\r\n‚Ä¢ Demonstrates better reasoning capabilities compared to larger models while using fewer resources\r\n\r\n‚Ä¢ Builds upon LIMA (Less Is More for Alignment) but focuses specifically on enhancing reasoning skills\r\n\r\n‚Ä¢ Challenges conventional wisdom that more training data always leads to better AI performance\r\n\r\nPlain English Explanation\r\nLIMO shows that AI models can learn to reason well without massive amounts of training data. Think of it like teaching a student - sometimes a few clear, well-chosen examples work better than overwhelming them with information.\r\n\r\nThe researchers found that carefully selecting high-quality training examples produces better results than using enormous datasets. This approach is similar to how humans often learn better from a few detailed explanations rather than skimming through volumes of material.\r\n\r\nWhat makes LIMO special is its focus on teaching the AI to think step-by-step, similar to how humans solve complex problems. Rather than force-feeding the model with endless examples, it learns from a smaller set of carefully chosen reasoning patterns.\r\n\r\nKey Findings\r\nLogic and reasoning capabilities can be developed with significantly less training data than previously thought. The model showed:\r\n\r\n‚Ä¢ Superior performance on reasoning tasks compared to larger models\r\n\r\n‚Ä¢ More consistent and reliable outputs when solving complex problems\r\n\r\n‚Ä¢ Better ability to explain its thinking process\r\n\r\n‚Ä¢ Reduced computational requirements and training time\r\n\r\nTechnical Explanation\r\nThe LIMO architecture builds on previous work in mathematical reasoning but introduces several key innovations. The model uses a specialized training approach that emphasizes quality over quantity in the training data.\r\n\r\nThe system employs a selective data curation process that identifies and prioritizes examples that demonstrate clear reasoning patterns. This approach differs from traditional methods that rely on massive datasets.\r\n\r\nRational meta-reasoning is integrated into the model's architecture, allowing it to evaluate and improve its own problem-solving strategies.\r\n\r\nCritical Analysis\r\nWhile LIMO shows promising results, several limitations deserve attention:\r\n\r\n‚Ä¢ The approach may not scale well to certain types of problems that require broader knowledge\r\n\r\n‚Ä¢ The criteria for selecting training examples could introduce unintended biases\r\n\r\n‚Ä¢ More research is needed to validate performance across diverse reasoning tasks\r\n\r\nThe evaluation methodology could be expanded to include a wider range of reasoning challenges and real-world applications.\r\n\r\nConclusion\r\nLIMO represents a significant shift in how we think about training AI systems to reason. The success of this \"less is more\" approach challenges the dominant paradigm of ever-larger training datasets.\r\n\r\nThis research opens new possibilities for developing more efficient and effective AI systems, particularly in resource-constrained environments. The findings suggest that future AI development might benefit from focusing more on the quality and structure of training data rather than just increasing its quantity.",
      "summary": "### LIMO: Less is More for Reasoning\n\n**Summary:** LIMO is an innovative AI reasoning approach that demonstrates strong performance with significantly less training data compared to larger models. It enhances reasoning skills by focusing on quality rather than quantity, challenging the conventional belief that larger datasets always yield better performance. The model, building on LIMA (Less Is More for Alignment), emphasizes high-quality training examples that help the AI learn reasoning in a step-by-step manner, akin to human problem-solving.\n\nKey findings reveal that LIMO outperforms larger models on reasoning tasks, providing consistent outputs and a better ability to articulate its thought processes, all while reducing computational demands and training time. It utilizes a specialized architecture that prioritizes selective data curation to identify clear reasoning patterns, integrating rational meta-reasoning for self-evaluation of problem-solving strategies.\n\nHowever, LIMO faces limitations, such as potential scalability issues for broader knowledge requirements and the risk of introducing biases in training example selection. Further research is needed to assess its performance across various reasoning tasks and real-world applications.\n\nIn conclusion, LIMO signifies a paradigm shift in AI reasoning training, advocating for a focus on the quality and structure of training data. This approach could lead to more efficient AI systems, especially in resource-limited settings.",
      "classification": "### LIMO: Less is More for Reasoning\n\n**Category:** Research Paper",
      "keyword": "### LIMO: Less is More for Reasoning\n\n**Keywords:** AI reasoning, LIMO architecture, training data curation, meta-reasoning, model efficiency"
    },
    {
      "No.": 25,
      "end_point": "https://www.aimodels.fyi/",
      "post_date": null,
      "link": "https://www.aimodels.fyi/papers/arxiv/coat-chain-associated-thoughts-framework-enhancing-large",
      "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning",
      "content": "Overview\r\nIntroduces Chain of Associated Thoughts (CoAT) framework to enhance LLM reasoning\r\nLeverages associative memory mechanisms similar to human cognition\r\nDemonstrates improved performance on complex reasoning tasks\r\nIntegrates episodic and semantic memory concepts\r\nShows 8-12% improvement over baseline models\r\nPlain English Explanation\r\nThe Chain of Associated Thoughts framework works like human memory and thinking patterns. When humans solve problems, we often connect different memories and ideas together. CoAT helps AI models do the same thing.\r\n\r\nThink of it like solving a mystery. Rather than just looking at clues in isolation, CoAT helps the AI connect related information, just like a detective connecting evidence from different sources. The system stores important information and recalls it when needed, similar to how we remember relevant past experiences.\r\n\r\nThe framework creates two types of memory: quick temporary memories for the current task (like your working memory) and longer-lasting knowledge (like facts you've learned over time). This combination helps AI models think more thoroughly and solve problems more effectively.\r\n\r\nKey Findings\r\nThe research demonstrates several important results:\r\n\r\n10% average improvement in reasoning accuracy\r\nBetter performance on complex multi-step problems\r\nMore consistent and logical explanations\r\nReduced repetition and circular reasoning\r\nEnhanced ability to handle contradictory information\r\nTechnical Explanation\r\nThe CoAT architecture consists of three main components: an associative memory module, a reasoning engine, and a memory consolidation system. The memory module stores both episodic (experience-based) and semantic (fact-based) information.\r\n\r\nThe system uses a novel attention mechanism to retrieve relevant memories and incorporate them into the reasoning process. This approach differs from traditional chain-of-thought prompting by actively maintaining and updating a memory state throughout the reasoning process.\r\n\r\nPerformance testing used standard benchmarks including GSM8K, MATH, and BIG-Bench Hard. The framework showed particular strength in problems requiring multi-step reasoning or integration of multiple concepts.\r\n\r\nCritical Analysis\r\nWhile CoAT shows promise, several limitations exist:\r\n\r\nMemory capacity constraints may limit scalability\r\nPerformance depends heavily on initial knowledge base quality\r\nMay struggle with novel scenarios outside training distribution\r\nComputational overhead from memory operations\r\nThe framework's effectiveness could be further validated through more diverse testing scenarios and real-world applications.\r\n\r\nConclusion\r\nCoAT represents a significant step toward more human-like reasoning in AI systems. The integration of associative memory mechanisms offers a promising direction for improving LLM capabilities. Future work could focus on scaling the approach and reducing computational requirements while maintaining performance benefits.\r\n\r\nThe results suggest that mimicking human cognitive processes may be a valuable approach for advancing AI reasoning capabilities. This could lead to more reliable and explainable AI systems for complex decision-making tasks.",
      "summary": "### CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning\n\n**Summary:** The Chain of Associated Thoughts (CoAT) framework is introduced to enhance the reasoning capabilities of large language models (LLMs) by mimicking human cognitive processes. CoAT utilizes associative memory mechanisms, integrating episodic and semantic memory concepts, which leads to a significant improvement of 8-12% over baseline models on complex reasoning tasks. The framework operates by forming temporary and long-lasting memories, enabling AI to connect related information and solve problems more effectively, akin to how humans utilize past experiences in problem-solving.\n\nKey findings indicate a 10% average improvement in reasoning accuracy, better handling of multi-step problems, and a reduction in repetitive reasoning. The CoAT architecture includes an associative memory module, a reasoning engine, and a memory consolidation system, utilizing a novel attention mechanism to retrieve and apply relevant memories during reasoning.\n\nHowever, limitations such as memory capacity constraints, dependency on the quality of the initial knowledge base, challenges with novel scenarios, and computational overhead from memory operations are noted. Future research is suggested to explore diverse testing scenarios and real-world applications to further validate the framework.\n\nIn conclusion, CoAT represents a promising advancement towards human-like reasoning in AI, suggesting that mimicking cognitive processes can enhance AI systems' reliability and explainability in complex decision-making tasks.",
      "classification": "### CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning\n\n**Category:** Research Paper",
      "keyword": "### CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning\n\n**Keywords:** Chain of Associated Thoughts, associative memory, reasoning accuracy, multi-step problems, cognitive processes"
    },
    {
      "No.": 26,
      "end_point": "https://www.aimodels.fyi/",
      "post_date": null,
      "link": "https://www.google.com/search?q=Syntriever%3A+How+to+Train+Your+Retriever+with+Synthetic+Data+from+LLMs&oq=Syntriever%3A+How+to+Train+Your+Retriever+with+Synthetic+Data+from+LLMs&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg6MgYIAhBFGD0yBggDEEUYPdIBBzUyOGowajSoAgCwAgE&sourceid=chrome&ie=UTF-8",
      "title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs",
      "content": "Overview\r\nIntroduces Syntriever, a novel approach for training text retrievers using synthetic data from large language models\r\nDemonstrates improved retrieval performance across multiple domains without task-specific training\r\nAchieves state-of-the-art results on various retrieval benchmarks\r\nUses LLMs to generate high-quality training data through passage synthesis\r\nImplements a two-stage training process for knowledge distillation and contrastive learning\r\nPlain English Explanation\r\nSyntriever works like a smart librarian who learns to find relevant information by studying artificially created examples. Instead of learning from real-world data, it learns from carefully crafted synthetic passages generated by large language models.\r\n\r\nThink of it like teaching someone to find books in a library by first creating practice scenarios. The system generates pairs of related texts, like a question and its answer, then learns to recognize what makes them connected. This approach is similar to how humans might learn to spot relationships between different pieces of information.\r\n\r\nThe training happens in two main steps. First, the system learns from synthetic examples created by advanced AI models. Then, it practices matching related pieces of information using a technique called contrastive learning - like learning to spot differences and similarities between various texts.\r\n\r\nKey Findings\r\nText retrieval systems trained with synthetic data outperform traditional methods across multiple benchmarks. The research shows:\r\n\r\n2-8% improvement over previous state-of-the-art systems\r\nConsistent performance across different types of retrieval tasks\r\nEffective generalization to new domains without additional training\r\nReduced dependency on task-specific training data\r\nSuperior performance in zero-shot scenarios\r\nTechnical Explanation\r\nThe training process involves two distinct stages. Stage one focuses on knowledge distillation through passage synthesis, where an LLM generates diverse, high-quality training examples. Stage two implements contrastive learning using these synthetic passages.\r\n\r\nThe architecture employs a bi-encoder framework with shared weights between query and passage encoders. This design enables efficient similarity computation through dense vector representations. The system uses a combination of hard negative mining and carefully curated positive examples to enhance training effectiveness.\r\n\r\nImplementation details reveal careful consideration of computational efficiency, with optimizations in both the training pipeline and inference process. The model architecture balances performance with practical deployment considerations.\r\n\r\nCritical Analysis\r\nWhile the results are impressive, several limitations deserve attention:\r\n\r\nDependency on LLM quality for synthetic data generation\r\nComputational costs associated with large-scale training\r\nPotential biases inherited from the underlying LLMs\r\nLimited evaluation on non-English languages\r\nNeed for further investigation of scalability limits\r\nThe research methodology could benefit from more extensive ablation studies and deeper analysis of failure cases. Future work might explore multilingual applications and more efficient training techniques.\r\n\r\nConclusion\r\nSyntriever represents a significant advancement in training retrievers without task-specific data. The success of synthetic training data suggests a promising direction for developing more robust and versatile retrieval systems. This approach could significantly impact information retrieval applications across various domains, from search engines to question-answering systems.\r\n\r\nThe findings point toward a future where retrieval systems can be trained more efficiently and effectively, potentially reducing the need for large amounts of human-annotated training data. This development has broad implications for improving information access and search technology.",
      "summary": "### Syntriever: How to Train Your Retriever with Synthetic Data from LLMs\n\n**Summary:** Syntriever is an innovative method for training text retrieval systems using synthetic data generated by large language models (LLMs). It enhances retrieval performance across various domains without the need for task-specific training, achieving state-of-the-art results on multiple benchmarks. The system generates high-quality training data through passage synthesis and employs a two-stage training process involving knowledge distillation and contrastive learning.\n\nIn simpler terms, Syntriever acts like a smart librarian that learns to find relevant information by studying artificially created examples. It begins by generating pairs of related texts, such as questions and answers, to help the system understand how to connect information. The training consists of two main phases: first, learning from these synthetic examples, and second, practicing through contrastive learning to distinguish and relate different texts.\n\nKey findings reveal that systems trained with synthetic data outperform traditional methods, showing a 2-8% improvement over previous state-of-the-art systems, consistent performance across various retrieval tasks, effective generalization to new domains without extra training, and superior performance in zero-shot scenarios.\n\nTechnically, the training process involves two stages: knowledge distillation via passage synthesis and contrastive learning. The architecture uses a bi-encoder framework with shared weights for efficient similarity computation, supported by hard negative mining and well-curated positive examples to enhance training.\n\nHowever, there are limitations, including reliance on the quality of LLM-generated synthetic data, high computational costs, potential biases from LLMs, limited evaluation of non-English languages, and the need for further scalability assessments. The research methodology could benefit from more extensive studies and deeper analyses of failure cases, with future work potentially exploring multilingual applications and more efficient training methods.\n\nIn conclusion, Syntriever marks a noteworthy advancement in training retrieval systems without task-specific data, suggesting a future where such systems can be developed more robustly and efficiently, ultimately improving information access and search technology across diverse domains.",
      "classification": "### Syntriever: How to Train Your Retriever with Synthetic Data from LLMs\n\n**Category:** Model",
      "keyword": "### Syntriever: How to Train Your Retriever with Synthetic Data from LLMs\n\n**Keywords:** Synthetic Data, Text Retrieval, Contrastive Learning, Knowledge Distillation, Large Language Models"
    },
    {
      "No.": 27,
      "end_point": "https://www.chatpaper.ai/",
      "post_date": null,
      "link": "https://www.chatpaper.ai/dashboard/paper/6e627325-cad1-46a2-97cf-c7d04184d29d",
      "title": "LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention",
      "content": "Paper Overview\nThis paper introduces a method to transfer knowledge from a large pre-trained model to a smaller one using Enhanced Cross-Attention. The methodology involves utilizing a large pre-trained model for input query representations and a smaller model with Enhanced Cross-Attention layers for response generation. Experimental validation shows improved generation quality and reduced computational costs.\n\nCore Contribution\nThe key innovation lies in the Enhanced Cross-Attention mechanism that facilitates knowledge transfer from a large pre-trained model to a smaller one efficiently.\n\nResearch Context\nThis study addresses the challenge of transferring knowledge from large models to smaller ones effectively, aiming to enhance response generation quality while reducing computational overhead.\n\nKeywords\nTransfer Learning, Enhanced Cross-Attention, Pre-trained Models, Response Generation, Knowledge Transfer\n\nBackground\nThe research focuses on the transfer of knowledge from a large pre-trained model to a smaller one to improve response generation quality and efficiency. The rationale stems from the need to leverage the capabilities of large models in smaller, more resource-efficient settings.\n\nResearch Gap\nExisting literature lacks efficient methods for transferring knowledge from large pre-trained models to smaller ones for response generation tasks.\n\nTechnical Challenges\nChallenges include designing a mechanism for effective knowledge transfer, ensuring compatibility between models of different sizes, and maintaining response coherence during generation.\n\nPrior Approaches\nPrevious solutions have primarily focused on fine-tuning or distillation methods, which may not fully address the efficiency and quality requirements of knowledge transfer in this context.\n\nMethodology\nThe methodology involves utilizing a large pre-trained model for input query representations and a smaller model with Enhanced Cross-Attention layers for response generation.\n\nTheoretical Foundation\nTheoretical basis includes the concept of knowledge transfer, Enhanced Cross-Attention mechanism, and model integration for response generation tasks.\n\nTechnical Architecture\nThe architecture comprises a modified large pre-trained model, a smaller response generation model, and a combined model integrating both components for efficient knowledge transfer.\n\nImplementation Details\nImplementation involves Linear projections for dimension conversion, Adapter Block for non-linear transformation, and Gating Mechanism for blending original and external knowledge.\n\nInnovation Points\nThe innovation lies in the Enhanced Cross-Attention mechanism, which enhances knowledge transfer efficiency and response generation quality.\n\nExperimental Validation\nExperimental validation includes comparative analysis with models like DeepSeek, Qwen2, and GPT-Neo, showcasing improved generation quality and reduced computational costs.\n\nSetup\nThe experimental setup includes the utilization of the Bespoke-Stratos-17k dataset, dynamic data preparation techniques like padding, filtering, and shuffling.\n\nMetrics\nEvaluation metrics involve reduction in training and validation loss, coherence of generated responses, and task-specific quality assessments like arithmetic problems.\n\nResults\nResults demonstrate a reduction in loss metrics, enhanced coherence in responses, and improved generation quality compared to existing models.\n\nComparative Analysis\nComparative analysis with DeepSeek, Qwen2, and GPT-Neo models highlights the superiority of the proposed approach in terms of efficiency and quality.\n\nImpact and Implications\nThe proposed method shows significant improvements in response generation quality, reduced computational costs, and adaptability to specific tasks, paving the way for practical applications.\n\nKey Findings\nThe key findings include enhanced response generation quality, reduced computational overhead, and adaptability to task-specific requirements.\n\nLimitations\nLimitations may include constraints in scaling the approach to extremely large models and potential challenges in integrating diverse model types.\n\nFuture Directions\nFuture research opportunities include exploring different model integrations, optimizing Cross-Attention layer configurations, and applying the method to specialized business tasks.\n\nPractical Significance\nThe method's practical significance lies in its potential to improve response generation in various applications, reduce computational expenses, and adapt models to specific task requirements.",
      "summary": "### LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention\n\n**Summary:** This paper presents a novel approach for knowledge transfer from a large pre-trained model to a smaller model through Enhanced Cross-Attention. The methodology utilizes a large model for input representation while employing a smaller model with Enhanced Cross-Attention layers for generating responses. Experimental results indicate improvements in generation quality and reductions in computational costs. The core innovation is the Enhanced Cross-Attention mechanism, which addresses the challenge of effective knowledge transfer while enhancing response generation quality and reducing overhead.\n\nThe research highlights the gap in existing literature regarding efficient methods for such knowledge transfer, especially for response generation tasks. It details the technical challenges faced, such as ensuring compatibility and coherence between differently sized models, and critiques prior approaches that mainly relied on fine-tuning or distillation.\n\nThe architecture combines a modified large pre-trained model with a smaller response generation model, implementing linear projections, Adapter Blocks, and a Gating Mechanism for optimal knowledge blending. Experimental validation involved comparative analyses with models like DeepSeek, Qwen2, and GPT-Neo, showcasing superior performance in terms of efficiency and generation quality.\n\nKey findings reveal enhanced response quality and adaptability to specific tasks with reduced computational costs. However, limitations exist in scaling to larger models and integrating diverse types. Future research could explore different integrations, optimize Cross-Attention configurations, and apply the methodology to specialized tasks. Overall, this method holds practical significance in improving response generation across various applications while minimizing computational demands.",
      "classification": "### LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention\n\n**Category:** Research Paper",
      "keyword": "### LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention\n\n**Keywords:** Knowledge Transfer, Enhanced Cross-Attention, Response Generation, Transfer Learning, Computational Efficiency"
    },
    {
      "No.": 28,
      "end_point": "https://discuss.pytorch.kr/tag/tool-for-llm",
      "post_date": null,
      "link": "https://discuss.pytorch.kr/t/yek-rust-llm/5926",
      "title": "yek: Rust Í∏∞Î∞ò LLM Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÎèÑÍµ¨",
      "content": "yek ÏÜåÍ∞ú\nyekÏùÄ Îπ†Î•¥Í≥† Ìö®Ïú®Ï†ÅÏù∏ Î∞©ÏãùÏúºÎ°ú ÌîÑÎ°úÏ†ùÌä∏Ïùò ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÌååÏùºÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïù¥ ÏâΩÍ≤å ÏÜåÎπÑÌï† Ïàò ÏûàÎèÑÎ°ù Îç∞Ïù¥ÌÑ∞Î•º Íµ¨Ï°∞ÌôîÌïòÍ≥† ÏßÅÎ†¨ÌôîÌïòÎäî ÎèÑÍµ¨ÏûÖÎãàÎã§. RustÎ°ú ÏûëÏÑ±ÎêòÏñ¥ Îπ†Î•∏ ÏÜçÎèÑÏôÄ ÎÜíÏùÄ ÏïàÏ†ïÏÑ±ÏùÑ Ï†úÍ≥µÌïòÎ©∞, ÌäπÌûà ÌååÏùº Ïö∞ÏÑ†ÏàúÏúÑÎ•º ÏßÄÏ†ïÌïòÍ≥† .gitignoreÎ•º ÌôúÏö©ÌïòÏó¨ Î∂àÌïÑÏöîÌïú ÌååÏùºÏùÑ Í±¥ÎÑàÎõ∏ Ïàò ÏûàÎäî Í∏∞Îä•Ïù¥ ÎèãÎ≥¥ÏûÖÎãàÎã§.\n\nyekÏùÄ Ï£ºÎ°ú Îã§ÏùåÍ≥º Í∞ôÏùÄ ÏûëÏóÖÏùÑ ÏûêÎèôÌôîÌï† Ïàò ÏûàÎäî CLI ÎèÑÍµ¨ÏûÖÎãàÎã§:\n\nÌîÑÎ°úÏ†ùÌä∏ ÎÇ¥ ÌÖçÏä§Ìä∏ ÌååÏùºÏùÑ Ï≤òÎ¶¨ÌïòÍ≥†, ÏõêÌïòÎäî ÌÅ¨Í∏∞ÎÇò ÌÜ†ÌÅ∞ Í∞úÏàòÎ°ú Ï≤≠ÌÅ¨(Chunk)Î°ú ÎÇòÎàïÎãàÎã§.\n\n.gitignore Í∑úÏπô Î∞è Git ÌûàÏä§ÌÜ†Î¶¨Î•º ÌôúÏö©ÌïòÏó¨ Ï§ëÏöîÌïú ÌååÏùºÎßå ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨Ìï©ÎãàÎã§.\n\nÍ≤∞Í≥º Îç∞Ïù¥ÌÑ∞Î•º ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨Ïóê Ï†ÄÏû•ÌïòÍ±∞ÎÇò Ïä§Ìä∏Î¶¨Î∞ç Î∞©ÏãùÏúºÎ°ú Ï∂úÎ†•Ìï©ÎãàÎã§.\n\nRust Í∏∞Î∞òÏúºÎ°ú ÏûëÏÑ±Îêú ÎßåÌÅº ÏÑ±Îä•Ïù¥ Îõ∞Ïñ¥ÎÇòÎ©∞, ÌäπÌûà ÎåÄÍ∑úÎ™® ÌååÏùºÏù¥ ÏûàÎäî ÌîÑÎ°úÏ†ùÌä∏ÏóêÏÑúÎèÑ Îπ†Î•∏ Ï≤òÎ¶¨ ÏÜçÎèÑÎ•º Î≥¥Ïû•Ìï©ÎãàÎã§. Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÌîÑÎ°úÏ†ùÌä∏Ïùò Î™®Îì† ÌååÏùºÏùÑ 10MB Îã®ÏúÑÎ°ú ÎÇòÎàÑÏñ¥ LLMÏù¥ ÏâΩÍ≤å Ï≤òÎ¶¨Ìï† Ïàò ÏûàÎäî ÌòïÌÉúÎ°ú Ï†úÍ≥µÌï©ÎãàÎã§. LLM ÌïôÏäµ Î∞è Ï∂îÎ°† ÏõåÌÅ¨ÌîåÎ°úÏö∞ÏóêÏÑú yekÏùÄ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ÏãúÍ∞ÑÏùÑ ÌöçÍ∏∞Ï†ÅÏúºÎ°ú Îã®Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\nÎòêÌïú, yekÏùÄ ÎπÑÏä∑Ìïú ÎèÑÍµ¨Ïù∏ RepomixÏôÄ ÎπÑÍµêÌï¥ ÏµúÎåÄ 230Î∞∞ Îπ†Î•∏ ÏÑ±Îä•ÏùÑ ÏûêÎûëÌï©ÎãàÎã§. RustÏùò Î≥ëÎ†¨ Ï≤òÎ¶¨ÏôÄ Ìö®Ïú®Ï†ÅÏù∏ I/O ÏûëÏóÖÏùÑ ÌÜµÌï¥ ÏÜçÎèÑ Î©¥ÏóêÏÑú ÌÅ∞ Ïû•Ï†êÏùÑ Î≥¥Ïú†ÌïòÍ≥† ÏûàÏäµÎãàÎã§. ÌäπÌûà Îã§ÏùåÍ≥º Í∞ôÏùÄ Í∏∞Îä•ÏóêÏÑú Ï∞®Î≥ÑÌôîÎê©ÎãàÎã§:\n\n.gitignore Î∞è Git ÌûàÏä§ÌÜ†Î¶¨Î•º ÌôúÏö©Ìïú Ïö∞ÏÑ†ÏàúÏúÑ Ï≤òÎ¶¨.\n\nÍ∏∞Î≥∏Ï†ÅÏúºÎ°ú Î∞îÏù¥ÎÑàÎ¶¨ ÌååÏùº Î∞è ÎåÄÏö©Îüâ ÌååÏùºÏùÑ Î¨¥Ïãú.\n\nÏÇ¨Ïö©Ïûê ÏßÄÏ†ï Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏùÄ ÏÑ§Ï†ï ÌååÏùº(yek.toml) Ï†úÍ≥µ.\n\nyekÏùò Ï£ºÏöî Í∏∞Îä•\nÌååÏùº Ïö∞ÏÑ†ÏàúÏúÑ ÏßÄÏ†ï: Git Í∏∞Î°ù Î∞è ÏÇ¨Ïö©Ïûê Ï†ïÏùò Í∑úÏπôÏùÑ Í∏∞Î∞òÏúºÎ°ú Îçî Ï§ëÏöîÌïú ÌååÏùºÏùÑ ÎßàÏßÄÎßâÏóê Ï∂úÎ†•ÌïòÏó¨ LLMÏùò ÏßëÏ§ëÎèÑÎ•º ÎÜíÏûÑ.\n\nÏ≤≠ÌÅ¨ ÌÅ¨Í∏∞ Ï°∞Ï†à: Í∏∞Î≥∏ 10MB ÌÅ¨Í∏∞, ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò Îì±ÏùÑ ÏßÄÏ†ï Í∞ÄÎä•.\n\nÏä§Ìä∏Î¶¨Î∞ç Ï∂úÎ†• ÏßÄÏõê: ÌååÏù¥ÌîÑÎ•º ÌÜµÌï¥ Îã§Î•∏ Î™ÖÎ†πÏñ¥Î°ú Í≤∞Í≥ºÎ•º Ï†ÑÎã¨ Í∞ÄÎä•.\n\nÎã§Ï§ë ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨: Ïó¨Îü¨ ÎîîÎ†âÌÜ†Î¶¨Î•º Ìïú Î≤àÏóê Ï≤òÎ¶¨ Í∞ÄÎä•.\n\nÏÇ¨Ïö©Ïûê Ï†ïÏùò ÏÑ§Ï†ï: yek.toml ÌååÏùºÏùÑ ÌÜµÌï¥ Ï∂îÍ∞ÄÏ†ÅÏù∏ Î¨¥Ïãú Ìå®ÌÑ¥ Î∞è Ïö∞ÏÑ†ÏàúÏúÑ Í∑úÏπô ÏÑ§Ï†ï Í∞ÄÎä•.\n\nÎùºÏù¥ÏÑ†Ïä§\nyek ÌîÑÎ°úÏ†ùÌä∏Îäî MIT ÎùºÏù¥ÏÑ†Ïä§Î°ú Í≥µÍ∞úÎêú Ïò§ÌîàÏÜåÏä§ ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§.\n\n:github: yek GitHub Ï†ÄÏû•ÏÜå\nhttps://github.com/bodo-run/yek\n\n\n\n",
      "summary": "### yek: Rust Í∏∞Î∞ò LLM Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÎèÑÍµ¨\n\n**Summary:**  \nyekÏùÄ RustÎ°ú Í∞úÎ∞úÎêú Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÎèÑÍµ¨Î°ú, ÌîÑÎ°úÏ†ùÌä∏Ïùò ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÌååÏùºÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌïòÏó¨ ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïù¥ ÏâΩÍ≤å ÏÜåÎπÑÌï† Ïàò ÏûàÎèÑÎ°ù Îç∞Ïù¥ÌÑ∞Î•º Íµ¨Ï°∞ÌôîÌïòÍ≥† ÏßÅÎ†¨ÌôîÌï©ÎãàÎã§. Ïù¥ CLI ÎèÑÍµ¨Îäî ÌÖçÏä§Ìä∏ ÌååÏùºÏùÑ ÏõêÌïòÎäî ÌÅ¨Í∏∞ÎÇò ÌÜ†ÌÅ∞ ÏàòÎ°ú ÎÇòÎàÑÍ≥†, .gitignore Í∑úÏπô Î∞è Git ÌûàÏä§ÌÜ†Î¶¨Î•º ÌôúÏö©ÌïòÏó¨ Ï§ëÏöî ÌååÏùºÎßå ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞Îäî ÏûÑÏãú ÎîîÎ†âÌÜ†Î¶¨Ïóê Ï†ÄÏû•ÌïòÍ±∞ÎÇò Ïä§Ìä∏Î¶¨Î∞ç Î∞©ÏãùÏúºÎ°ú Ï∂úÎ†•Ìï† Ïàò ÏûàÏúºÎ©∞, Í∏∞Î≥∏Ï†ÅÏúºÎ°ú 10MB Îã®ÏúÑÎ°ú ÌååÏùºÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ LLMÏùò Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ÏãúÍ∞ÑÏùÑ Îã®Ï∂ïÏãúÌÇµÎãàÎã§. \n\nyekÏùÄ RepomixÏôÄ ÎπÑÍµêÌï¥ ÏµúÎåÄ 230Î∞∞ Îπ†Î•∏ ÏÑ±Îä•ÏùÑ ÏûêÎûëÌïòÎ©∞, RustÏùò Î≥ëÎ†¨ Ï≤òÎ¶¨ÏôÄ Ìö®Ïú®Ï†ÅÏù∏ I/O ÏûëÏóÖÏùÑ ÌÜµÌï¥ ÎÜíÏùÄ ÏÜçÎèÑÎ•º Ï†úÍ≥µÌï©ÎãàÎã§. Ï£ºÏöî Í∏∞Îä•ÏúºÎ°úÎäî ÌååÏùº Ïö∞ÏÑ†ÏàúÏúÑ ÏßÄÏ†ï, Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞ Ï°∞Ï†à, Ïä§Ìä∏Î¶¨Î∞ç Ï∂úÎ†• ÏßÄÏõê, Îã§Ï§ë ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨, ÏÇ¨Ïö©Ïûê Ï†ïÏùò ÏÑ§Ï†ï Îì±Ïù¥ ÏûàÏäµÎãàÎã§. Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî MIT ÎùºÏù¥ÏÑ†Ïä§Î°ú Í≥µÍ∞úÎêú Ïò§ÌîàÏÜåÏä§ÏûÖÎãàÎã§. GitHub Ï†ÄÏû•ÏÜå ÎßÅÌÅ¨Îäî [Ïó¨Í∏∞](https://github.com/bodo-run/yek)ÏûÖÎãàÎã§.",
      "classification": "### yek: Rust Í∏∞Î∞ò LLM Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÎèÑÍµ¨\n\n**Category:** Tool",
      "keyword": "### yek: Rust Í∏∞Î∞ò LLM Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÎèÑÍµ¨\n\n**Keywords:** Rust, LLM, Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨, CLI ÎèÑÍµ¨, .gitignore"
    },
    {
      "No.": 29,
      "end_point": "https://discuss.pytorch.kr/tag/tool-for-llm",
      "post_date": null,
      "link": "https://discuss.pytorch.kr/t/toolgen/5327",
      "title": "ToolGen: ÎèÑÍµ¨ Í≤ÄÏÉâ Î∞è Ìò∏Ï∂úÏùÑ ÏúÑÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú",
      "content": "ToolGen ÏÜåÍ∞ú\nToolGenÏùÄ ÎèÑÍµ¨ ÏßÄÏãùÏùÑ ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïóê ÏßÅÏ†ë ÌÜµÌï©ÌïòÎèÑÎ°ù ÏÑ§Í≥ÑÎêú ÏãúÏä§ÌÖúÏûÖÎãàÎã§. Ïù¥ ÏãúÏä§ÌÖúÏùÄ ÎèÑÍµ¨Î•º Í≥†Ïú†Ìïú ÌÜ†ÌÅ∞ÏúºÎ°ú ÌëúÌòÑÌï®ÏúºÎ°úÏç®, Ïñ∏Ïñ¥ ÏÉùÏÑ± ÏûëÏóÖ Ï§ëÏóê ÎèÑÍµ¨ Ìò∏Ï∂úÏùÑ ÏõêÌôúÌïòÍ≤å ÏàòÌñâÌï† Ïàò ÏûàÍ≤å Ìï©ÎãàÎã§. Ïù¥Îü¨Ìïú ÌÜµÌï© Ï†ëÍ∑º Î∞©ÏãùÏùÄ API Ìò∏Ï∂úÏù¥ÎÇò Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÉâÍ≥º Í∞ôÏùÄ Ïô∏Î∂Ä ÎèÑÍµ¨ Ï†ëÍ∑ºÏù¥ ÌïÑÏöîÌïú Î≥µÏû°Ìïú ÏûëÏóÖÏóêÏÑú Ïñ∏Ïñ¥ Î™®Îç∏Ïù¥ ÎèôÏ†ÅÏúºÎ°ú ÎèÑÍµ¨Î•º Í≤ÄÏÉâÌïòÍ≥† Ìò∏Ï∂úÌï† Ïàò ÏûàÎèÑÎ°ù ÏßÄÏõêÌï©ÎãàÎã§.\n\nToolGenÏùò Ï£ºÏöî Î™©ÌëúÎäî LLMÏù¥ ÎèÑÍµ¨Î•º ÏâΩÍ≤å ÌÜµÌï©ÌïòÍ≥† ÌïÑÏöîÌï† ÎïåÎßàÎã§ ÌäπÏ†ï ÎèÑÍµ¨Î•º Ìò∏Ï∂úÌï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Ïù¥ Í∏∞Îä•ÏùÄ Îã§ÏñëÌïú API Ìò∏Ï∂úÏù¥ÎÇò Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÉâÏù¥ ÏöîÍµ¨ÎêòÎäî Î≥µÏû°Ìïú ÏûëÏóÖÏóêÏÑú ÌäπÌûà Ïú†Ïö©Ìï©ÎãàÎã§.\n\nÏ£ºÏöî Í∏∞Îä•\nToolGen: ÎèÑÍµ¨ Í≤ÄÏÉâ Î∞è Ìò∏Ï∂úÏùÑ ÏúÑÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú\n\nÌÜµÌï©Îêú ÎèÑÍµ¨ ÌÜ†ÌÅ∞Ìôî: ÎèÑÍµ¨Îì§ÏùÑ Í≥†Ïú†Ìïú ÌÜ†ÌÅ∞ÏúºÎ°ú ÌÜµÌï©ÌïòÏó¨ ÏûêÏó∞Ïñ¥ ÏÉùÏÑ±Í≥º Ìà¥ Ìò∏Ï∂úÏùÑ Ìï®Íªò Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.\nAPI ÏûêÎèôÌôî: ÎèÑÍµ¨ ÏÇ¨Ïö©ÏùÑ ÏúÑÌïú API ÌÇ§Î•º Î∂àÎü¨ÏôÄ Ìà¥ Ìò∏Ï∂ú Í≥ºÏ†ïÏùÑ ÏûêÎèôÌôîÌï©ÎãàÎã§.\nÎã®ÏàúÌôîÎêú ÎèÑÍµ¨ ÏÇ¨Ïö©: ÎèÑÍµ¨ Ìò∏Ï∂úÏóê ÌïÑÏöîÌïú Î≥µÏû°Ìïú ÏÑ§Ï†ï ÏóÜÏù¥ Î∞îÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî Í∞ÑÌé∏Ìïú ÏãúÏä§ÌÖú.\n\n:scroll: ToolGen ÎÖºÎ¨∏\nhttps://arxiv.org/pdf/2410.03439\n\n:hugs: ToolGen Î™®Îç∏\nhuggingface.co\n\nToolGen - a reasonwang Collection\nWe‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.\n\n:hugs: ToolGen Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú\nhuggingface.co\n\nreasonwang/ToolGen-Datasets ¬∑ Datasets at Hugging Face\nWe‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.\n\n:github: ToolGen GitHub Ï†ÄÏû•ÏÜå\nhttps://github.com/Reason-Wang/ToolGen",
      "summary": "### ToolGen: ÎèÑÍµ¨ Í≤ÄÏÉâ Î∞è Ìò∏Ï∂úÏùÑ ÏúÑÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú\n\n**Summary:** ToolGenÏùÄ ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïóê ÎèÑÍµ¨ ÏßÄÏãùÏùÑ ÌÜµÌï©ÌïòÍ∏∞ ÏúÑÌï¥ ÏÑ§Í≥ÑÎêú ÏãúÏä§ÌÖúÏúºÎ°ú, ÎèÑÍµ¨Î•º Í≥†Ïú†Ìïú ÌÜ†ÌÅ∞ÏúºÎ°ú ÌëúÌòÑÌïòÏó¨ Ïñ∏Ïñ¥ ÏÉùÏÑ± ÏûëÏóÖ Ï§ë ÎèÑÍµ¨ Ìò∏Ï∂úÏùÑ ÏõêÌôúÌïòÍ≤å ÏàòÌñâÌï† Ïàò ÏûàÎèÑÎ°ù Ìï©ÎãàÎã§. Ïù¥ ÏãúÏä§ÌÖúÏùò Ï£ºÏöî Î™©ÌëúÎäî LLMÏù¥ ÎèÑÍµ¨Î•º ÏâΩÍ≤å ÌÜµÌï©ÌïòÍ≥† ÌïÑÏöîÌï† ÎïåÎßàÎã§ Ìò∏Ï∂úÌï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî Í≤ÉÏûÖÎãàÎã§. ToolGenÏùÄ API Ìò∏Ï∂úÏù¥ÎÇò Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÉâÏù¥ ÏöîÍµ¨ÎêòÎäî Î≥µÏû°Ìïú ÏûëÏóÖÏóêÏÑú Ïú†Ïö©ÌïòÎ©∞, Îã§ÏùåÍ≥º Í∞ôÏùÄ Ï£ºÏöî Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§: ÌÜµÌï©Îêú ÎèÑÍµ¨ ÌÜ†ÌÅ∞Ìôî, API ÏûêÎèôÌôî, Í∑∏Î¶¨Í≥† Îã®ÏàúÌôîÎêú ÎèÑÍµ¨ ÏÇ¨Ïö©. Ï∂îÍ∞Ä ÏûêÎ£åÎ°úÎäî ToolGen ÎÖºÎ¨∏, Î™®Îç∏, Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú ÎßÅÌÅ¨ Î∞è GitHub Ï†ÄÏû•ÏÜåÍ∞Ä Ìè¨Ìï®Îê©ÎãàÎã§.",
      "classification": "### ToolGen: ÎèÑÍµ¨ Í≤ÄÏÉâ Î∞è Ìò∏Ï∂úÏùÑ ÏúÑÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú\n\n**Category:** Tool",
      "keyword": "### ToolGen: ÎèÑÍµ¨ Í≤ÄÏÉâ Î∞è Ìò∏Ï∂úÏùÑ ÏúÑÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú\n\n**Keywords:** ÎèÑÍµ¨ ÌÜµÌï©, ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏, API ÏûêÎèôÌôî, Ìà¥ Ìò∏Ï∂ú, ÏûêÏó∞Ïñ¥ ÏÉùÏÑ±"
    },
    {
      "No.": 30,
      "end_point": "https://discuss.pytorch.kr/tag/tool-for-llm",
      "post_date": null,
      "link": "https://discuss.pytorch.kr/t/crawl4ai-llm-ai-crawler/5282",
      "title": "Crawl4AI: LLM Î∞è AI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ ÏúÑÌïú Ïò§ÌîàÏÜåÏä§ Ïõπ ÌÅ¨Î°§Îü¨(Crawler)",
      "content": "Crawl4AI ÏÜåÍ∞ú\nCrawl4AIÎäî ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(LLM)ÏùÑ ÏúÑÌïú Ïõπ ÌÅ¨Î°§ÎßÅÍ≥º Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂úÏùÑ ÎπÑÎèôÍ∏∞Ï†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌïòÎäî ÎùºÏù¥Î∏åÎü¨Î¶¨ÏûÖÎãàÎã§. ÎπÑÎèôÍ∏∞ Í∏∞Îä•ÏùÑ ÌÜµÌï¥ Ïó¨Îü¨ URLÏùÑ ÎèôÏãúÏóê Ï≤òÎ¶¨Ìï† Ïàò ÏûàÍ≥†, Ïõπ ÌéòÏù¥ÏßÄÏùò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞, ÎØ∏ÎîîÏñ¥ ÌÉúÍ∑∏, ÎßÅÌÅ¨ Îì±ÏùÑ ÏÜêÏâΩÍ≤å Ï∂îÏ∂úÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÎèÑÍµ¨Îäî PlaywrightÏôÄ Í∞ôÏùÄ ÌÅ¨Î°§Îü¨Î•º ÌôúÏö©ÌïòÏó¨ ÏÑ±Îä•ÏùÑ Í∑πÎåÄÌôîÌïòÍ≥†, ÌÅ¨Î°§ÎßÅ ÎåÄÏÉÅ ÌéòÏù¥ÏßÄÏóêÏÑú Javascript Ïã§Ìñâ Î∞è CSS ÏÖÄÎ†âÌÑ∞Î•º ÌÜµÌïú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂úÎèÑ ÏßÄÏõêÌï©ÎãàÎã§.\n\nÍ∏∞Ï°¥Ïùò ÎèôÍ∏∞ ÌÅ¨Î°§ÎßÅ Î∞©ÏãùÏùÄ Ïõπ ÌéòÏù¥ÏßÄ Î°úÎìú ÏãúÍ∞ÑÍ≥º Ïã§Ìñâ ÏãúÍ∞ÑÏóê ÌÅ∞ ÏòÅÌñ•ÏùÑ Î∞õÎäî Î∞òÎ©¥, Crawl4AIÎäî ÎπÑÎèôÍ∏∞ Íµ¨Ï°∞Î•º ÌÜµÌï¥ Îπ†Î•¥Í≥† ÎåÄÍ∑úÎ™®Ïùò Îç∞Ïù¥ÌÑ∞Î•º Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏàòÏßëÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ AI ÏùëÏö© ÌîÑÎ°úÍ∑∏Îû®Ïóê ÌïÑÏöîÌïú ÎåÄÎüâÏùò Îç∞Ïù¥ÌÑ∞Î•º Ïã†ÏÜçÌïòÍ≤å ÌôïÎ≥¥Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\nCrawl4AIÏùò Ï£ºÏöî Í∏∞Îä•\nÎ¨¥Î£å Î∞è Ïò§ÌîàÏÜåÏä§: ÎàÑÍµ¨ÎÇò ÏûêÏú†Î°≠Í≤å ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî Î¨¥Î£å Ïò§ÌîàÏÜåÏä§ ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§.\nÎπ†Î•∏ ÏÑ±Îä•: ÏÉÅÏö© ÏÑúÎπÑÏä§Î≥¥Îã§ Îπ†Î•∏ ÏÑ±Îä•ÏùÑ ÏûêÎûëÌï©ÎãàÎã§.\nÎã§ÏñëÌïú Ï∂úÎ†• ÌòïÏãù ÏßÄÏõê: JSON, HTML, Markdown Îì±Ïùò LLM ÏπúÌôîÏ†ÅÏù∏ ÌòïÏãùÏúºÎ°ú Îç∞Ïù¥ÌÑ∞Î•º Ï∂úÎ†•Ìï©ÎãàÎã§.\nÎã§Ï§ë URL ÌÅ¨Î°§ÎßÅ: Ïó¨Îü¨ URLÏùÑ ÎèôÏãúÏóê Ï≤òÎ¶¨ÌïòÏó¨ Ìö®Ïú®Ï†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\nÎØ∏ÎîîÏñ¥ ÌÉúÍ∑∏ Ï∂îÏ∂ú: Ïù¥ÎØ∏ÏßÄ, Ïò§ÎîîÏò§, ÎπÑÎîîÏò§ ÌÉúÍ∑∏Î•º Î™®Îëê Ï∂îÏ∂úÌï©ÎãàÎã§.\nÏÑ∏ÏÖò Í¥ÄÎ¶¨ Î∞è ÌîÑÎ°ùÏãú ÏßÄÏõê: Î≥µÏû°Ìïú Îã§Ï§ë ÌéòÏù¥ÏßÄ ÏãúÎÇòÎ¶¨Ïò§Ïóê Ïú†Ïö©ÌïòÎ©∞ ÌîÑÎ°ùÏãúÎ•º ÌÜµÌï¥ Í∞úÏù∏ Ï†ïÎ≥¥ Î≥¥Ìò∏ Î∞è Ï†ëÍ∑ºÏÑ±ÏùÑ Í∞ïÌôîÌï©ÎãàÎã§.\nÍ≥†Í∏â Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú: CSS ÏÖÄÎ†âÌÑ∞ Î∞è JavaScript Ïã§ÌñâÏùÑ ÌÜµÌï¥ Ï†ïÌôïÌïú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂úÏù¥ Í∞ÄÎä•ÌïòÎ©∞, ÏÑ∏Î∂Ä Ï∂îÏ∂ú Ï†ÑÎûµÎèÑ ÏßÄÏõêÌï©ÎãàÎã§.\nÏÑ§Ïπò Î∞©Î≤ï\nCrawl4AIÎäî Python Ìå®ÌÇ§ÏßÄÎ°ú ÏÑ§ÏπòÌïòÍ±∞ÎÇò DockerÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\npipÎ•º Ïù¥Ïö©Ìïú ÏÑ§Ïπò\npip install crawl4ai\nÍ∏∞Î≥∏Ï†ÅÏúºÎ°ú PlaywrightÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÎπÑÎèôÍ∏∞ ÌÅ¨Î°§ÎßÅÏù¥ Í∞ÄÎä•ÌïòÎ©∞, ÌïÑÏöîÌïú Í≤ΩÏö∞ PlaywrightÎ•º ÏàòÎèôÏúºÎ°ú ÏÑ§ÏπòÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\nplaywright install\nPlaywrightÏóê Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌïú Í≤ΩÏö∞, Îã§ÏùåÏùò Î™ÖÎ†πÏñ¥Î•º Ïã§ÌñâÌïòÎ©¥ Ìï¥Í≤∞Ïù¥ ÎêòÎäî Í≤ΩÏö∞ÎèÑ ÏûàÏäµÎãàÎã§:\n\npython -m playwright install chromium\nÎèôÍ∏∞ Î≤ÑÏ†Ñ ÏÑ§Ïπò\npip install crawl4ai Î™ÖÎ†πÏñ¥Îäî ÏûêÎèôÏúºÎ°ú ÎπÑÎèôÍ∏∞(Asynchronous)Î°ú ÎèôÏûëÌïòÎäî ÌÅ¨Î°§Îü¨Î•º ÏÑ§ÏπòÌï©ÎãàÎã§. ÎèôÍ∏∞Ïãù(Synchronous)ÏúºÎ°ú ÎèôÏûëÌïòÎäî ÌÅ¨Î°§ÎßÅÏùÑ ÏõêÌï† Í≤ΩÏö∞ SeleniumÏùÑ ÏÇ¨Ïö©Ìïú ÎèôÍ∏∞ Î≤ÑÏ†ÑÏùÑ ÏÑ§ÏπòÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\npip install crawl4ai[sync]\nÏÇ¨Ïö© Î∞©Î≤ï\nÍ∞ÑÎã®Ìïú ÏÇ¨Ïö© ÏòàÏãúÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:\n\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.nbcnews.com/business\")\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nÏù¥ ÏòàÏãúÎäî NBC Îâ¥Ïä§Ïùò ÎπÑÏ¶àÎãàÏä§ ÌéòÏù¥ÏßÄÎ•º ÌÅ¨Î°§ÎßÅÌïòÏó¨ ÎÇ¥Ïö©ÏùÑ Markdown ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•Ìï©ÎãàÎã§.\n\nCrawl4AI ÎùºÏù¥ÏÑ†Ïä§\nCrawl4AI ÌîÑÎ°úÏ†ùÌä∏Îäî Apache 2.0 LicenseÎ°ú Í≥µÍ∞ú Î∞è Î∞∞Ìè¨ÎêòÍ≥† ÏûàÏäµÎãàÎã§.\n\n:github: Crawl4AI GitHub Ï†ÄÏû•ÏÜå\nhttps://github.com/unclecode/crawl4ai?tab=readme-ov-file\n\n:google_cloud: Crawl4AIÎ•º Google ColabÏóêÏÑú Î∞îÎ°ú ÏÇ¨Ïö©Ìï¥Î≥¥Í∏∞\n\ncolab.research.google.com\n\nGoogle Colab\n:books: Crawl4AI Í≥µÏãù Î¨∏ÏÑú\n\ncrawl4ai.com\nHome - Crawl4AI Documentation (v0.4.3bx)\nüöÄü§ñ Crawl4AI, Open-source LLM-Friendly Web Crawler & Scraper",
      "summary": "### Crawl4AI: LLM Î∞è AI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ ÏúÑÌïú Ïò§ÌîàÏÜåÏä§ Ïõπ ÌÅ¨Î°§Îü¨(Crawler)\n\n**Summary:** \nCrawl4AIÎäî ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(LLM)ÏùÑ ÏúÑÌïú ÎπÑÎèôÍ∏∞ Ïõπ ÌÅ¨Î°§Îü¨Î°ú, Ïó¨Îü¨ URLÏùÑ ÎèôÏãúÏóê Ï≤òÎ¶¨ÌïòÏó¨ Ïõπ ÌéòÏù¥ÏßÄÏùò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞, ÎØ∏ÎîîÏñ¥ ÌÉúÍ∑∏, ÎßÅÌÅ¨ Îì±ÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú Ï∂îÏ∂úÌï† Ïàò ÏûàÎäî ÎùºÏù¥Î∏åÎü¨Î¶¨ÏûÖÎãàÎã§. PlaywrightÎ•º ÌôúÏö©ÌïòÏó¨ Javascript Ïã§Ìñâ Î∞è CSS ÏÖÄÎ†âÌÑ∞Î•º ÌÜµÌïú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂úÏùÑ ÏßÄÏõêÌïòÎ©∞, Ï†ÑÌÜµÏ†ÅÏù∏ ÎèôÍ∏∞ ÌÅ¨Î°§ÎßÅ Î∞©ÏãùÏóê ÎπÑÌï¥ Îπ†Î•¥Í≥† ÎåÄÎüâÏùò Îç∞Ïù¥ÌÑ∞ ÏàòÏßëÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\n\nÏ£ºÏöî Í∏∞Îä•ÏúºÎ°úÎäî Î¨¥Î£å Î∞è Ïò§ÌîàÏÜåÏä§ Ï†úÍ≥µ, Îπ†Î•∏ ÏÑ±Îä•, Îã§ÏñëÌïú Ï∂úÎ†• ÌòïÏãù(JSON, HTML, Markdown) ÏßÄÏõê, Îã§Ï§ë URL ÌÅ¨Î°§ÎßÅ, ÎØ∏ÎîîÏñ¥ ÌÉúÍ∑∏ Ï∂îÏ∂ú, ÏÑ∏ÏÖò Í¥ÄÎ¶¨ Î∞è ÌîÑÎ°ùÏãú ÏßÄÏõê, Í≥†Í∏â Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú Îì±Ïù¥ ÏûàÏäµÎãàÎã§. ÏÑ§ÏπòÎäî Python Ìå®ÌÇ§ÏßÄÎÇò DockerÎ•º ÌÜµÌï¥ Í∞ÄÎä•ÌïòÎ©∞, ÎπÑÎèôÍ∏∞ Î∞è ÎèôÍ∏∞ÏãùÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî ÏÑ§Ïπò ÏòµÏÖòÏù¥ Ï†úÍ≥µÎê©ÎãàÎã§. ÏÇ¨Ïö© ÏòàÏãúÎ°úÎäî NBC Îâ¥Ïä§ ÎπÑÏ¶àÎãàÏä§ ÌéòÏù¥ÏßÄÎ•º ÌÅ¨Î°§ÎßÅÌïòÏó¨ Markdown ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•ÌïòÎäî ÏΩîÎìúÍ∞Ä Ï†úÏãúÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Crawl4AIÎäî Apache 2.0 LicenseÎ°ú Í≥µÍ∞úÎêòÏñ¥ ÏûàÏúºÎ©∞, GitHubÏôÄ Google ColabÏóêÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî ÎßÅÌÅ¨Í∞Ä Ï†úÍ≥µÎê©ÎãàÎã§.",
      "classification": "### Crawl4AI: LLM Î∞è AI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ ÏúÑÌïú Ïò§ÌîàÏÜåÏä§ Ïõπ ÌÅ¨Î°§Îü¨(Crawler)\n\n**Category:** Tool",
      "keyword": "### Crawl4AI: LLM Î∞è AI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ ÏúÑÌïú Ïò§ÌîàÏÜåÏä§ Ïõπ ÌÅ¨Î°§Îü¨(Crawler)\n\n**Keywords:** ÎπÑÎèôÍ∏∞ ÌÅ¨Î°§ÎßÅ, Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú, LLM ÏπúÌôîÏ†Å ÌòïÏãù, ÎØ∏ÎîîÏñ¥ ÌÉúÍ∑∏, ÏÑ∏ÏÖò Í¥ÄÎ¶¨"
    },
    {
      "No.": 31,
      "end_point": "https://news.hada.io/",
      "post_date": null,
      "link": "https://news.hada.io/topic?id=19051",
      "title": "2025ÎÖÑÏùÑ ÏúÑÌïú ÎèÑÍµ¨Îì§ : Oils, Radicle, Simplex Chat",
      "content": "Ï†ÄÏûêÎäî Oils, Radicle, Simplex Chat 3Í∞ÄÏßÄÍ∞Ä ÎçîÏö± ÎÑêÎ¶¨ ÏÇ¨Ïö©ÎêòÎ©¥ Ï¢ãÍ≤†Îã§Í≥† ÏÉùÍ∞Å\nOils: Í∏∞Ï°¥ POSIX Ïâò(Bash Îì±)ÏùÑ ÎåÄÏ≤¥\nRadicle: Î∂ÑÏÇ∞Ìòï Git Ìò∏Ïä§ÌåÖ ÏÜîÎ£®ÏÖòÏúºÎ°ú Github/GitLabÏùÑ ÎåÄÏ≤¥ Í∞ÄÎä•\nSimplex Chat: Ïù¥Î©îÏùºÏùÑ ÎåÄÏ≤¥ÌïòÍ±∞ÎÇò ÌòÅÏã† Í∞ÄÎä•\nÎ≥¥ÌÜµ ÏÉàÎ°úÏö¥ ÎèÑÍµ¨Î•º Î∞úÍ≤¨ÌïòÎ©¥, Í∑∏ ÎèÑÍµ¨Ïùò Í∏∞Î≥∏Í∞úÎÖê Ïù¥Ìï¥ÌïòÎäîÎç∞ 10Î∂Ñ, ÏãúÏûëÌïòÎäîÎç∞ 5ÏùºÍπåÏßÄ Í±∏Î¶¨Í≤å Îê®\nÎåÄÎ∂ÄÎ∂ÑÏùÄ Í∏∞Î≥∏ Í∞úÎÖêÏùÑ Ïù¥Ìï¥ÌïòÍ≥† ÎÇòÎ©¥ Ìù•ÎØ∏Í∞Ä ÎÅùÎÇòÎ≤ÑÎ¶¨Í∏∞Ïóê, Ï†ïÎßê Í∑ºÎ≥∏Ï†ÅÏù∏ ÌòÅÏã†Í≥º ÏùºÍ¥ÄÎêú Ï≤†ÌïôÏù¥ ÏûàÏñ¥Ïïº Í≥ÑÏÜç Í¥ÄÏã¨ÏùÑ Í∞ÄÏßà Ïàò ÏûàÏùå\nÏòàÎ•º Îì§Ïñ¥ Pijul Í∞ôÏùÄ ÌîÑÎ°úÏ†ùÌä∏Í∞Ä Ìù•ÎØ∏Î°úÏõ†ÏßÄÎßå GitÏóê ÎπÑÍµêÌï¥ Ï†ÅÏö© Ïù¥Ï†êÏù¥ ÌÅ¨Í≤å Ï≤¥Í∞êÎêòÏßÄ ÏïäÏïÑ ÏùºÏÉÅÏ†ÅÏúºÎ°ú Î∞ÄÍ≥† ÎÇòÍ∞ÄÏßÄÎäî Î™ªÌñàÏùå\nÏïÑÎûò ÏÜåÍ∞úÌï† ÏÑ∏ Í∞ÄÏßÄ Ìà¥ÏùÄ ÌòÑÏû¨ ÎåÄÏïàÏúºÎ°úÏÑúÏùò Í∞ÄÏπòÏôÄ Ïã§Ï†úÎ°ú Ïì∞Ïùº ÎßåÌïú ÏôÑÏÑ±ÎèÑÎ•º Í∞ñÏ∑ÑÎã§Í≥† Î¥Ñ\nOils for unix\nBash ÏâòÏùÑ ÏÉàÎ°≠Í≤å Íµ¨ÌòÑÌïú ÌîÑÎ°úÏ†ùÌä∏Î°ú, POSIX Ìò∏ÌôòÏÑ±ÏùÑ Í∞ñÏ∂îÎ©¥ÏÑúÎèÑ Ïò§ÎûòÎêú Ïâò ÌôòÍ≤ΩÏùò Î¨∏Ï†úÏ†êÏùÑ Ìï¥Í≤∞Ìï¥ ÎÇòÍ∞ÄÎäî Ï§ë\nKornShell Îì± Í∏∞Ï°¥ ÎåÄÏïàÏù¥ ÏûàÏóàÏßÄÎßå, Ïú†ÏùòÎØ∏Ìïú Í∞úÏÑ† ÏóÜÏù¥ Ïò§Îûú Í∏∞Í∞Ñ Ï†ïÏ≤¥ ÏÉÅÌÉúÏòÄÏùå\nNushell Ï≤òÎüº ÎπÑÏä∑Ìïú Íµ¨Î¨∏ÏúºÎ°ú ÏÉàÎ°úÏö¥ Ïñ∏Ïñ¥Î•º ÎßåÎìúÎäî Í≤ÉÎèÑ ÏïÑÎãò\nOilsÎäî Bash Íµ¨ÌòÑÏùÑ Î∞îÌÉïÏúºÎ°ú, Ï†ïÎßêÎ°ú Íπ®ÏßÑ Î∂ÄÎ∂ÑÎßå Ï†êÏßÑÏ†ÅÏúºÎ°ú Í≥†ÏπòÍ≥†, Ïä§ÌÅ¨Î¶ΩÌä∏ÎèÑ Îã®Í≥ÑÏ†ÅÏúºÎ°ú ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Í∞ÄÎä•ÌïòÎèÑÎ°ù Ìï®\nÍ∏∞Ï°¥ POSIX ÏÖ∏Ïùò Î¨∏Ï†úÏ†ê(Ïòà: ÏïîÏãúÏ†Å Í∏ÄÎ°úÎπô, ÏûòÎ™ªÎêú ÏÇ∞Ïà† Ïó∞ÏÇ∞ Îì±)ÏùÑ Ìï¥Í≤∞\nÌòÑÏû¨ Bash Ìò∏Ìôò Î∂ÄÎ∂ÑÏùÄ ÏïàÏ†ïÎêú Î≤†ÌÉÄ Îã®Í≥ÑÏóê ÏûàÏúºÎ©∞, Ïò¨Ìï¥ 1.0 Î≤ÑÏ†Ñ Ï∂úÏãúÍ∞Ä Î™©Ìëú\nOils ÏãúÎèÑÌï¥Î≥¥Í∏∞ : Alpine Linux Edge Î≤ÑÏ†ÑÏóêÏÑú oils-for-unix Ìå®ÌÇ§ÏßÄÎ•º Ïù¥Ïö©Ìï¥ Í∞ÑÎã®Ìûà ÎèÑÏª§ ÌôòÍ≤ΩÏóêÏÑú ÌÖåÏä§Ìä∏ Í∞ÄÎä•\nRadicle\nGit Ï†ÄÏû•ÏÜåÎ•º ÌÉàÏ§ëÏïô Î∞©ÏãùÏúºÎ°ú Ìò∏Ïä§ÌåÖ/Í≥µÏú†ÌïòÎäî ÏÜîÎ£®ÏÖò\nGitlabÏù¥ÎÇò GithubÎ≥¥Îã§ Ïö∞ÏàòÌïòÎ©∞, ÏÜåÏä§ ÏΩîÎìúÏùò Í∞ÄÏö©ÏÑ±ÏùÑ ÎÜíÏù¥Í∏∞ ÏúÑÌï¥ ÌÜ†Î†åÌä∏ÏôÄ Ïú†ÏÇ¨Ìïú Î∞©ÏãùÏùÑ ÏÇ¨Ïö©\nÎ°úÏª¨ Ìò∏Ïä§ÌåÖÏûÑÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Í∏∞Ï°¥Ïùò ÏÑúÎ≤Ñ-ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉÅÌò∏ÏûëÏö©Í≥º Ïú†ÏÇ¨Ìïú ÏßÅÍ¥ÄÏ†ÅÏù∏ UXÎ•º Ï†úÍ≥µ\nÏÉàÎ°úÏö¥ Í∞úÎÖêÏùÑ ÏâΩÍ≤å Î∞∞Ïö∏ Ïàò ÏûàÎèÑÎ°ù Î¨∏ÏÑúÌôîÏóê ÎßéÏùÄ ÎÖ∏Î†•ÏùÑ Í∏∞Ïö∏Ïó¨ÏÑú, ÏôÑÏ†ÑÌûà ÏÉàÎ°úÏö¥ ÏõåÌÅ¨ÌîåÎ°úÎùºÎèÑ ÏâΩÍ≤å Ï†ÅÏùë Í∞ÄÎä•\nActivityPub Í∏∞Î∞ò ForgeFedÏôÄ Îã¨Î¶¨, RadicleÏùÄ Git Îç∞Ïù¥ÌÑ∞Ïóê ÏµúÏ†ÅÌôîÎêòÏñ¥ Îçî ÎÜíÏùÄ Í∞ÄÏö©ÏÑ±ÏùÑ Í∏∞ÎåÄÌï† Ïàò ÏûàÏùå\nRadicle ÏãúÎèÑÌï¥Î≥¥Í∏∞ : ÎÇ¥ ÏÑúÎ≤ÑÏóê ÎûòÎîîÌÅ¥ ÎÖ∏ÎìúÎ•º ÎßåÎì§Í≥† Ïù¥ Î∏îÎ°úÍ∑∏Ïóê ÏÜåÏä§Î•º Í≤åÏãúÌï¥Îë†. Í≥µÍ∞úÎêú ÏÇ¨Ïö©Ïûê Í∞ÄÏù¥ÎìúÎ•º Îî∞Î•¥Í±∞ÎÇò, Í∑∏ÎÉ• ÎÇ¥ Î∏îÎ°úÍ∑∏Î•º ÌîºÏñ¥ÎßÅÏúºÎ°ú ÌÅ¥Î°†ÌïòÎäî Í≤ÉÎèÑ Í∞ÄÎä•\nSimplex Chat\nÍ≤âÎ≥¥Í∏∞ÏóêÎäî Îòê ÌïòÎÇòÏùò Ï±ÑÌåÖ Ïï±Ïù¥ÏßÄÎßå, Ï£ºÏÜå(Address) Í∞úÎÖêÏù¥ Í∑ºÎ≥∏Ï†ÅÏúºÎ°ú Îã§Î¶Ñ\nÏÉùÏÑ±Ìïú ÌîÑÎ°úÌïÑÏóê Ïó¨Îü¨ Ï£ºÏÜåÎ•º Ï∂îÍ∞ÄÌï† Ïàò ÏûàÍ≥†, Ïù¥ Ï£ºÏÜåÎäî ÌäπÏ†ï ÎåÄÌôî Ïó∞Í≤∞ÏóêÎßå ÏÇ¨Ïö©ÎêòÍ±∞ÎÇò Ïä§Ìå∏ Î∞úÏÉù Ïãú Ï¶âÏãú ÌèêÍ∏∞ Í∞ÄÎä•Ìï®\n\"Ï£ºÏÜåÎäî 1ÌöåÏö©Ïù¥Í±∞ÎÇò, ÏÇ≠Ï†úÌïòÍ∏∞ Ï†ÑÍπåÏßÄ Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•\"\n\"2Í∞úÏùò ÌîÑÎ°úÌïÑ Í∞ÑÏùò Î™®Îì† Ïó∞Í≤∞ÏùÄ Í≥†Ïú†Ìïú 1ÎåÄ1 Ï£ºÏÜåÎ•º ÏÇ¨Ïö©\"\nÍ≤∞Í≥ºÏ†ÅÏúºÎ°ú ‚ÄúÌïòÎÇòÏùò ÌîÑÎ°úÌïÑÏóê Ïó¨Îü¨ Ï£ºÏÜåÎ•º Ïú†ÎèôÏ†ÅÏúºÎ°ú Ïó∞Í≤∞/Î∂ÑÎ¶¨‚ÄùÌï† Ïàò ÏûàÏñ¥, Ïä§Ìå∏Ïóê ÎåÄÏùëÌïòÍ±∞ÎÇò Î™©Ï†ÅÎ≥ÑÎ°ú Ï£ºÏÜåÎ•º ÏÇ¨Ïö©ÌïòÍ∏∞ Ï¢ãÏùå\nÏû¨ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ï£ºÏÜåÎ•º ÏÇ≠Ï†úÌïòÎ©¥ ÏÉàÎ°úÏö¥ Ïó∞Í≤∞Îßå Ï∞®Îã®ÎêòÎ©∞, Í∏∞Ï°¥ Ïó∞ÎùΩÏ≤òÎ•º ÏÇ≠Ï†úÌïòÎ©¥ Ìï¥Îãπ Ïó∞ÎùΩÏ≤òÏùò Ï†ëÍ∑ºÏù¥ Ï∞®Îã®Îê®\nÏ¶â \"Ï£ºÏÜåÎ•º Ï∂îÏÉÅÌôîÌïòÍ≥†, Ï£ºÏÜåÏôÄ ÌîÑÎ°úÌïÑÏùÑ ÎîîÏª§ÌîåÎßÅ ÌïòÎäî Í≤É. ÎßàÏπò ÏÇ¨ÏÑúÌï® Ï≤òÎüº\"\nÏù¥Î©îÏùº ÏãúÏä§ÌÖúÏù¥ Ïù¥Îü∞ Î∞©ÏãùÏùÑ Ï±ÑÌÉùÌïúÎã§Î©¥, Ï£ºÏÜåÎ•º ÏÉàÎ°ú Î∞úÍ∏âÌïòÍ≥† ÌïÑÏöî Ïãú Î≤ÑÎ¶¨Îäî Í≥ºÏ†ïÏùÑ Í∞ÑÌé∏ÌôîÌï† Ïàò ÏûàÏùå\nE2E ÏïîÌò∏Ìôî, ÏôÑÏ†ÑÌïú ÌîÑÎùºÏù¥Î≤ÑÏãúÎèÑ ÏßÄÏõêÌï¥, Î©îÏùº ÎåÄÏã† Ïù¥Îü∞ ÌîÑÎ°úÌÜ†ÏΩúÏù¥ ÌôïÏÇ∞ÎêòÍ∏∏ Î∞îÎùºÎäî ÎßàÏùåÏù¥ ÌÅº\nÏïÑÏßÅ Ïù∏ÏßÄÎèÑÍ∞Ä ÎÇÆÏßÄÎßå, ‚ÄúÎòê Îã§Î•∏ Î©îÏã†Ï†Ä‚ÄùÎùºÍ≥† ÏßÄÎÇòÏπòÍ∏∞Ïóî Í∑ºÎ≥∏Ï†Å ÏïÑÏù¥ÎîîÏñ¥Í∞Ä Ï∞∏Ïã†Ìï®\nSimplex ÏãúÎèÑÌï¥Î≥¥Í∏∞ : ÌîºÎìúÎ∞±Ïö© Í∑∏Î£πÏùÑ ÏÉùÏÑ±Ìï¥ Îë†. ÏùµÎ™Ö Î™®ÎìúÎ°ú Í∑∏Î£πÏóê Ï∞∏Ïó¨ Í∞ÄÎä•ÌïòÎ©∞, Í∞ÑÎã®Ìïú ÌîÑÎ°úÌïÑ ÏÉùÏÑ± ÌõÑ Î∞îÎ°ú ÎåÄÌôîÎ•º Ï≤¥ÌóòÌï† Ïàò ÏûàÏùå\nÎ≠ò Ìï¥ÏïºÌï†Íπå?\nÏö∞Î¶¨Í∞Ä ÏÇ¨Ïö©ÌïòÎäî ÎèÑÍµ¨Í∞Ä Í≥ß ÎØ∏ÎûòÏùò ÌëúÏ§ÄÏù¥ Îê®\nOils, Radicle, Simplex Chat Î™®Îëê Í∏∞Ï°¥Ïùò Í¥ÄÌñâÏ†Å ÎèÑÍµ¨Î•º ÎÑòÏñ¥ÏÑúÎäî Í∞ÄÎä•ÏÑ±ÏùÑ ÏßÄÎãå ÌîÑÎ°úÏ†ùÌä∏ÏûÑ\nÏù¥Îü∞ ÏÉàÎ°úÏö¥ ÌîÑÎ°úÏ†ùÌä∏Î•º ÏßÅÏ†ë ÏãúÎèÑÌï¥Î≥¥Í≥†, Îçî ÎÇòÏùÄ Ïã§Î¨¥ ÌëúÏ§ÄÏùÑ ÎßåÎì§Ïñ¥ÎÇòÍ∞ÄÍ∏∞Î•º Ï†úÏïàÌï®\n\n‚ñ≤\ndbs0829 5ÏùºÏ†Ñ  [-]\noilsÎäî ÎÑàÎ¨¥ ÏùºÎ∞òÏ†ÅÏù∏ Îã®Ïñ¥ÎùºÏÑú Í¥úÌûà Ï†ïÍ∞êÏù¥ ÏïàÍ∞ÄÎÑ§Ïöî. Ï†ÄÎßå Í∑∏Îü¥ÏßÄ Î™®Î•¥Í≤†ÎäîÎç∞, Îî± Í≤ÄÏÉâÌñàÏùÑ Îïå Ìï¥Îãπ ÏÑúÎπÑÏä§Í∞Ä ÏïàÎÇòÏò§Îäî Í≤ΩÏö∞ ÏùÄÍ∑º Í±∞Î∂ÄÍ∞êÏù¥ Îì§Ïñ¥Ïöî.\n\nÎãµÎ≥ÄÎã¨Í∏∞\n‚ñ≤\nxguru 5ÏùºÏ†Ñ  [-]\nOil - ÏÉàÎ°úÏö¥ Unix Ïâò\nÏ†úÍ∞Ä 2021ÎÖÑÏóê ÏÜåÍ∞úÌñàÏóàÍ≥†, ÏòàÏ†Ñ Ïù¥Î¶ÑÏù¥ Oil Ïù¥ÏóàÎäîÎç∞ 2025ÎÖÑÎ∂ÄÌÑ∞ Oils Î°ú Ïù¥Î¶ÑÏù¥ Î≥ÄÍ≤ΩÎêòÏóàÏäµÎãàÎã§.\nRadicle - P2P Î∞©ÏãùÏùò GitHub ÎåÄÏ≤¥Ï†ú\nSimpleX - ÏÇ¨Ïö©Ïûê IDÍ∞Ä ÏóÜÎäî ÏµúÏ¥àÏùò Î©îÏã†Ï†Ä",
      "summary": "### 2025ÎÖÑÏùÑ ÏúÑÌïú ÎèÑÍµ¨Îì§ : Oils, Radicle, Simplex Chat\n\n**Summary:** Ï†ÄÏûêÎäî Oils, Radicle, Simplex Chat ÏÑ∏ Í∞ÄÏßÄ ÎèÑÍµ¨Í∞Ä Îçî ÎÑêÎ¶¨ ÏÇ¨Ïö©ÎêòÍ∏∞Î•º Ìù¨ÎßùÌïòÎ©∞, Í∞Å ÎèÑÍµ¨Ïùò Í∏∞Îä•Í≥º Ïû•Ï†êÏùÑ ÏÑ§Î™ÖÌïúÎã§. \n\n- **Oils**: Í∏∞Ï°¥ POSIX ÏÖ∏(Bash Îì±)ÏùÑ ÎåÄÏ≤¥Ìï† Ïàò ÏûàÎäî ÌîÑÎ°úÏ†ùÌä∏Î°ú, Ìò∏ÌôòÏÑ±ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú Ïò§ÎûòÎêú ÏÖ∏ ÌôòÍ≤ΩÏùò Î¨∏Ï†úÏ†êÏùÑ Ìï¥Í≤∞ÌïòÍ≥† ÏûàÎã§. Bash Íµ¨ÌòÑÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌïòÏó¨ Ï†êÏßÑÏ†ÅÏù∏ Í∞úÏÑ†ÏùÑ ÌÜµÌï¥, ÏïîÏãúÏ†Å Í∏ÄÎ°úÎπô Î∞è ÏûòÎ™ªÎêú ÏÇ∞Ïà† Ïó∞ÏÇ∞Í≥º Í∞ôÏùÄ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ≥† ÏûàÏúºÎ©∞, ÌòÑÏû¨ ÏïàÏ†ïÎêú Î≤†ÌÉÄ Îã®Í≥ÑÏóê ÏûàÎã§. 1.0 Î≤ÑÏ†Ñ Ï∂úÏãúÎ•º Î™©ÌëúÎ°ú ÌïòÍ≥† ÏûàÎã§. \n\n- **Radicle**: Î∂ÑÏÇ∞Ìòï Git Ìò∏Ïä§ÌåÖ ÏÜîÎ£®ÏÖòÏúºÎ°ú, Github Î∞è GitLabÏùò ÎåÄÏïàÏúºÎ°úÏÑú ÏÜåÏä§ ÏΩîÎìúÏùò Í∞ÄÏö©ÏÑ±ÏùÑ ÎÜíÏù¥Í∏∞ ÏúÑÌï¥ ÌÜ†Î†åÌä∏ÏôÄ Ïú†ÏÇ¨Ìïú Î∞©ÏãùÏùÑ ÏÇ¨Ïö©ÌïúÎã§. ÏßÅÍ¥ÄÏ†ÅÏù∏ ÏÇ¨Ïö©Ïûê Í≤ΩÌóòÏùÑ Ï†úÍ≥µÌïòÎ©∞, ÏâΩÍ≤å ÏÉàÎ°úÏö¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞Ïóê Ï†ÅÏùëÌï† Ïàò ÏûàÎèÑÎ°ù Î¨∏ÏÑúÌôîÍ∞Ä Ïûò ÎêòÏñ¥ ÏûàÎã§. \n\n- **Simplex Chat**: ÏÉàÎ°úÏö¥ Í∞úÎÖêÏùò Ï±ÑÌåÖ Ïï±ÏúºÎ°ú, Ï£ºÏÜå Í∞úÎÖêÏù¥ Îß§Ïö∞ Îã§Î•¥Îã§. ÏÇ¨Ïö©ÏûêÎäî Ïó¨Îü¨ Ï£ºÏÜåÎ•º Ï∂îÍ∞ÄÌïòÏó¨ ÌäπÏ†ï ÎåÄÌôîÏóêÎßå ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏúºÎ©∞, ÌïÑÏöî Ïãú Ï¶âÏãú ÌèêÍ∏∞Ìï† Ïàò ÏûàÎã§. E2E ÏïîÌò∏ÌôîÏôÄ ÌîÑÎùºÏù¥Î≤ÑÏãúÎ•º ÏßÄÏõêÌïòÎ©∞, Í∏∞Ï°¥ Ïù¥Î©îÏùº ÏãúÏä§ÌÖúÏóê ÎπÑÌï¥ ÌòÅÏã†ÏÑ±ÏùÑ ÏßÄÎãàÍ≥† ÏûàÎã§. \n\nÏ†ÄÏûêÎäî Ïù¥Îü¨Ìïú ÎèÑÍµ¨Îì§Ïù¥ Í∏∞Ï°¥ Í¥ÄÌñâÏùÑ ÎÑòÏñ¥ÏÑúÎäî Í∞ÄÎä•ÏÑ±ÏùÑ Í∞ÄÏßÄÍ≥† ÏûàÏúºÎ©∞, ÏßÅÏ†ë ÏÇ¨Ïö©Ìï¥Î≥¥ÏïÑ Îçî ÎÇòÏùÄ Ïã§Î¨¥ ÌëúÏ§ÄÏùÑ ÎßåÎì§Ïñ¥ ÎÇòÍ∞ÄÍ∏∞Î•º Ï†úÏïàÌïúÎã§.",
      "classification": "### 2025ÎÖÑÏùÑ ÏúÑÌïú ÎèÑÍµ¨Îì§ : Oils, Radicle, Simplex Chat\n\n**Category:** Tool",
      "keyword": "### 2025ÎÖÑÏùÑ ÏúÑÌïú ÎèÑÍµ¨Îì§ : Oils, Radicle, Simplex Chat\n\n**Keywords:** Oils, Radicle, Simplex Chat, POSIX, Î∂ÑÏÇ∞Ìòï Git"
    },
    {
      "No.": 32,
      "end_point": "https://news.hada.io/",
      "post_date": null,
      "link": "https://news.hada.io/topic?id=19179",
      "title": "Show GN: Moonlight v1.0.0 - ÎÖºÎ¨∏ÏùΩÍ∏∞Î•º ÏúÑÌïú AI PDF Î¶¨Îçî",
      "content": "ÎÖºÎ¨∏ÏùΩÍ∏∞Î•º ÏúÑÌïú AI PDF Î¶¨Îçî\nÏó∞Íµ¨ÏûêÎùºÎ©¥ ÎàÑÍµ¨ÎÇò Í≤™Ïñ¥Î≥∏ Ïñ¥Î†§ÏõÄÏù¥ ÏûàÏäµÎãàÎã§.\n\nÏàòÏãùÏùò Îß•ÎùΩ Î∞è Î≥ÄÏàò ÌååÏïÖÏóê ÏãúÍ∞ÑÏù¥ Í±∏Î¶¨Í≥†,\nÏù¥ÎØ∏ÏßÄÎÇò ÌÖåÏù¥Î∏îÏùò ÏùòÎØ∏Î•º Îπ†Î•¥Í≤å ÌååÏïÖÌïòÍ∏∞ Ïñ¥Î†§Ïö∞Î©∞,\nÌïòÏù¥ÌçºÎßÅÌÅ¨Î•º ÌÅ¥Î¶≠ÌïòÍ≥† Îã§Ïãú ÎèåÏïÑÏò§Îäî Î≤àÍ±∞Î°úÏö¥ Í≥ºÏ†ïÏóê ÏßÄÏπòÍ≥†,\nÏù∏Ïö© ÎÖºÎ¨∏ÏùÑ ÌïòÎÇòÌïòÎÇò Ï∞æÏïÑÎ≥¥Îäî Í≥ºÏ†ïÏù¥ Î∂àÌé∏ÌïòÍ≥† ÏãúÍ∞ÑÏù¥ Í±∏Î¶ΩÎãàÎã§.\nÏûëÎÖÑÏóê ÏãúÏûëÌïú Moonlight AI PDF ÎÖºÎ¨∏ Î¶¨ÎçîÏùò v1.0.0ÏùÑ ÏÜåÍ∞úÌï©ÎãàÎã§.\nÏó∞Íµ¨ÏûêÎì§Ïù¥ ÎÖºÎ¨∏ÏùÑ Îçî ÏâΩÍ≤å ÌÉêÏÉâÌïòÍ≥† Ïù¥Ìï¥Ìï† Ïàò ÏûàÎèÑÎ°ù ÎèÑÏôÄÏ£ºÎäî AI PDF Î¶¨ÎçîÎ°ú, ÎÖºÎ¨∏ÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Îã§Î£∞ Ïàò ÏûàÎèÑÎ°ù ÏßÄÏõêÌï©ÎãàÎã§. GPTÏôÄ Í∞ôÏùÄ AI Í∏∞Ïà†ÏùÑ ÌïôÏà† ÎÖºÎ¨∏ÏùÑ ÏùΩÎäî ÌùêÎ¶ÑÏóê ÏßÅÏ†ë Ï†ÅÏö©ÌïòÏó¨ ÏóÖÎ¨¥Ïùò Ìö®Ïú®ÏùÑ ÎÜíÏù¥Í≥†Ïûê Ìï©ÎãàÎã§.\n\nÏ£ºÏöî Í∏∞Îä•\n\nÌÖçÏä§Ìä∏ Î∞è Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö - ÏÑ†ÌÉùÌïú ÌÖçÏä§Ìä∏ / Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ Í∞ÄÎèÖÏÑ± Ï¢ãÍ≤å Íµ¨Ï°∞ÌôîÎêú ÏÑ§Î™ÖÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\nÏàòÏãù ÏÑ§Î™Ö - ÎÖºÎ¨∏Ïóê ÏûàÎäî ÏàòÏãù ÏÑ§Î™Ö, Î≥ÄÏàò Ï†ïÏùò Î∞è Ï∂îÍ∞ÄÏ†ÅÏù∏ ÏßàÏùòÎ•º ÌÜµÌï¥ ÏàòÏãùÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú Ïù¥Ìï¥Ìï† Ïàò ÏûàÏäµÎãàÎã§.\nÏàòÏãù Î≥µÏÇ¨ - pdf Î°úÎ∂ÄÌÑ∞ LaTeX ÌòïÏãùÏúºÎ°ú Î∞îÎ°ú Î≥µÏÇ¨Ìï¥ÏÑú notion, overleaf, github, tex Îì±Ïùò Ï†ïÎ¶¨ Îß§Ï≤¥Î°ú ÏâΩÍ≤å ÏòÆÍ∏∏ Ïàò ÏûàÏäµÎãàÎã§.\nÏù∏Ïö© Ïπ¥Îìú - Ï∞∏Í≥† Î¨∏ÌóåÏùÑ ÎÖºÎ¨∏ ÏïàÏóêÏÑú ÌÅ¥Î¶≠ ÌïúÎ≤àÏúºÎ°ú ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\nÎ≤àÏó≠ - Î™®Î•¥Îäî Î∂ÄÎ∂Ñ, ÌòπÏùÄ Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄÎ•º Î≤àÏó≠Ìï¥ÏÑú Î≥º Ïàò ÏûàÏäµÎãàÎã§.\nÏûêÎèô ÌïòÏù¥ÎùºÏù¥Ìä∏ - ÎÖºÎ¨∏Ïùò Ï£ºÏöî ÎÇ¥Ïö©Îì§ÏùÑ ÏûêÎèôÏúºÎ°ú ÌïòÏù¥ÎùºÏù¥Ìä∏ Ìï¥Ï£ºÏñ¥ ÏùΩÏùÑ Îïå Í∞ÄÏù¥Îìú Ïó≠Ìï†ÏùÑ Ìï¥Ï§çÎãàÎã§.\nÏßÄÍ∏à Î∞îÎ°ú ÏÇ¨Ïö©Ìï¥Î≥¥ÏÑ∏Ïöî!\n\nÌÅ¨Î°¨ ÏùµÏä§ÌÖêÏÖò ÏÑ§ÏπòÌïòÍ∏∞\nÎç∞Î™® ÎπÑÎîîÏò§",
      "summary": "### Show GN: Moonlight v1.0.0 - ÎÖºÎ¨∏ÏùΩÍ∏∞Î•º ÏúÑÌïú AI PDF Î¶¨Îçî\n\n**Summary:** Moonlight v1.0.0 is an AI PDF reader designed to assist researchers in reading and understanding academic papers more efficiently. It addresses common challenges faced by researchers, such as deciphering mathematical contexts, quickly interpreting images and tables, navigating hyperlinks, and locating citations. The AI tool applies advanced technologies like GPT to enhance the reading workflow. Key features include structured descriptions for selected texts and images, explanations of mathematical formulas with variable definitions, the ability to copy formulas in LaTeX format, quick access to reference materials through citation cards, translation capabilities for difficult sections, and automatic highlighting of essential content. Users are encouraged to install the Chrome extension and try the demo video to experience its functionality.",
      "classification": "### Show GN: Moonlight v1.0.0 - ÎÖºÎ¨∏ÏùΩÍ∏∞Î•º ÏúÑÌïú AI PDF Î¶¨Îçî\n\n**Category:** Tool",
      "keyword": "### Show GN: Moonlight v1.0.0 - ÎÖºÎ¨∏ÏùΩÍ∏∞Î•º ÏúÑÌïú AI PDF Î¶¨Îçî\n\n**Keywords:** AI PDF Î¶¨Îçî, ÎÖºÎ¨∏ ÌÉêÏÉâ, ÏàòÏãù ÏÑ§Î™Ö, Ïù∏Ïö© Ïπ¥Îìú, ÏûêÎèô ÌïòÏù¥ÎùºÏù¥Ìä∏"
    },
    {
      "No.": 33,
      "end_point": "https://betaai.substack.com/",
      "post_date": null,
      "link": "https://betaai.substack.com/p/2-ai-0f1",
      "title": "Multimodal Self-Instruct: Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄ Ïù¥Ìï¥Î†• Í∞ïÌôî",
      "content": "Multimodal Self-Instruct: Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄ Ïù¥Ìï¥Î†• Í∞ïÌôî\n\nLMMÏùò ÌïúÍ≥Ñ: Í∏∞Ï°¥ Î©ÄÌã∞Î™®Îã¨ Î™®Îç∏ÏùÄ ÏßÄÎèÑ, Ï∞®Ìä∏, Îã§Ïù¥Ïñ¥Í∑∏Îû® Îì±Ïùò Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄ Ìï¥ÏÑùÍ≥º ÏãúÍ∞ÅÏ†Å Ï∂îÎ°†Ïóê Ï∑®ÏïΩ.\n\nÌï©ÏÑ± Îç∞Ïù¥ÌÑ∞ ÌôúÏö©: LLMÍ≥º ÏΩîÎìú Í∏∞Îä•ÏùÑ Ïù¥Ïö©Ìï¥ 8Í∞ú ÏãúÍ∞ÅÏ†Å ÏãúÎÇòÎ¶¨Ïò§ÏóêÏÑú 11,193Í∞ú ÏßÄÏãúÎ¨∏ÏùÑ Ìè¨Ìï®Ìïú Î≤§ÏπòÎßàÌÅ¨ Íµ¨Ï∂ï.\n\nÏÑ±Îä• Í∞úÏÑ† ÏûÖÏ¶ù: 62,476Í∞ú Ìï©ÏÑ± Îç∞Ïù¥ÌÑ∞Î°ú LMMÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïú Í≤∞Í≥º, Ï∞®Ìä∏ Ìï¥ÏÑùÍ≥º ÏßÄÎèÑ ÎÇ¥ÎπÑÍ≤åÏù¥ÏÖò ÏÑ±Îä• Ìñ•ÏÉÅ ÌôïÏù∏.",
      "summary": "### Multimodal Self-Instruct: Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄ Ïù¥Ìï¥Î†• Í∞ïÌôî\n\n**Summary:** Î≥∏ Ïó∞Íµ¨Îäî Í∏∞Ï°¥Ïùò Î©ÄÌã∞Î™®Îã¨ Î™®Îç∏Ïù¥ ÏßÄÎèÑ, Ï∞®Ìä∏, Îã§Ïù¥Ïñ¥Í∑∏Îû®Í≥º Í∞ôÏùÄ Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄÎ•º Ìï¥ÏÑùÌïòÎäî Îç∞ ÌïúÍ≥ÑÍ∞Ä ÏûàÏùåÏùÑ ÏßÄÏ†ÅÌïúÎã§. Ïù¥Î•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ LLMÍ≥º ÏΩîÎìú Í∏∞Îä•ÏùÑ ÌôúÏö©ÌïòÏó¨ 8Í∞ú ÏãúÍ∞ÅÏ†Å ÏãúÎÇòÎ¶¨Ïò§ÏóêÏÑú 11,193Í∞úÏùò ÏßÄÏãúÎ¨∏ÏúºÎ°ú Íµ¨ÏÑ±Îêú Î≤§ÏπòÎßàÌÅ¨Î•º Íµ¨Ï∂ïÌïòÏòÄÎã§. ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú, 62,476Í∞úÏùò Ìï©ÏÑ± Îç∞Ïù¥ÌÑ∞Î•º ÌÜµÌï¥ LMMÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïú Í≤∞Í≥º Ï∞®Ìä∏ Ìï¥ÏÑùÍ≥º ÏßÄÎèÑ ÎÇ¥ÎπÑÍ≤åÏù¥ÏÖò ÏÑ±Îä•Ïù¥ Ìñ•ÏÉÅÎêòÏóàÏùåÏùÑ ÏûÖÏ¶ùÌïòÏòÄÎã§.",
      "classification": "### Multimodal Self-Instruct: Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄ Ïù¥Ìï¥Î†• Í∞ïÌôî\n\n**Category:** Research Paper",
      "keyword": "### Multimodal Self-Instruct: Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄ Ïù¥Ìï¥Î†• Í∞ïÌôî\n\n**Keywords:** Î©ÄÌã∞Î™®Îã¨ Î™®Îç∏, Ï∂îÏÉÅ Ïù¥ÎØ∏ÏßÄ, ÏãúÍ∞ÅÏ†Å Ï∂îÎ°†, Ìï©ÏÑ± Îç∞Ïù¥ÌÑ∞, ÏÑ±Îä• Í∞úÏÑ†"
    }
  ]
}