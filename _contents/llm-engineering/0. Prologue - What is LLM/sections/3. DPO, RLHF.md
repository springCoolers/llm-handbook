# 3. DPO, RLHF

## Summary
DPO(직접 선호도 최적화)와 RLHF(인간 피드백을 통한 강화학습)는 LLM(대형 언어 모델)의 미세 조정을 위한 두 가지 주요 방법입니다. DPO는 사용자 선호도를 직접 최적화하여 모델을 조정하는 간단하고 효율적인 방법입니다. 반면, RLHF는 보상 모델을 사용하여 모델을 조정하는 다단계 프로세스입니다. 두 방법은 각각의 장점과 단점을 가지고 있으며, 프로젝트의 특성에 따라 적절한 방법을 선택해야 합니다.

## Key Concepts
- **DPO(직접 선호도 최적화)** : 사용자 선호도를 직접 최적화하여 모델을 조정하는 간단하고 효율적인 방법으로, 보상 모델을 사용하지 않습니다.
- **RLHF(인간 피드백을 통한 강화학습)** : 보상 모델을 사용하여 모델을 조정하는 다단계 프로세스로, 인간 피드백을 통해 모델의 성능을 개선합니다.

## References
| URL 이름 | URL |
| --- | --- |
| Data Science Dojo | https://datasciencedojo.com/blog/rlhf-and-dpo-for-finetuning-llms/ |
| LinkedIn - Pankaj Bhatia | https://www.linkedin.com/pulse/revolutionizing-llm-training-dpo-vs-rlhf-unveiling-pankaj-bhatia |
| Reddit - Machine Learning | https://www.reddit.com/r/MachineLearning/comments/17974u1/d_can_direct_preference_optimization_dpo_be_used/ |
| MLTimes | https://mltimes.se/blog/from-rlhf-to-dpo/ |
| Dida | https://dida.do/blog/post-fine-tuning-llm-with-direct-preference-optimization |