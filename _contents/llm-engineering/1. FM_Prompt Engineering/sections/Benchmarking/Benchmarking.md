# Benchmarking
## Summary
Benchmarking in LLM은 대형 언어 모델의 성능을 평가하기 위한 표준화된 절차를 제공합니다. 이 절차는 다양한 태스크와 데이터셋을 포함하여 모델의 능력을 측정하고, 이를 통해 모델 간의 비교가 가능해집니다. Benchmarking은 LLM의 개발과 개선에 중요한 역할을 하며, 사용자와 개발자가 모델의 성능을 객관적으로 평가할 수 있도록 도와줍니다.

## Key Concepts
- **Benchmark Dataset** : 표준화된 테스트 세트의 모음으로, LLM의 특정 태스크 또는 시나리오에서의 성능을 평가하기 위해 사용됩니다.
- **LLM Evaluation Metrics** : 모델의 성능을 측정하는 지표로, 정확도, BLEU 점수, 퍼플렉시티 등이 포함됩니다.
- **Pre-production Evaluation** : 모델이 실제로 사용되기 전에 수행되는 평가로, 모델의 성능과 안정성을 확인합니다.
- **Post-production Evaluation** : 모델이 실제로 사용된 후 수행되는 평가로, 모델의 실제 성능과 사용자 피드백을 확인합니다.
- **Benchmark Leakage** : 모델이 벤치마크 데이터셋과 동일한 데이터로 학습되어, 실제 성능이 과대평가되는 현상입니다.

## References
| URL 이름 | URL |
| --- | --- |
| An Introduction to LLM Benchmarking - Confident AI | https://www.confident-ai.com/blog/the-current-state-of-benchmarking-llms |
| What are the most popular LLM benchmarks? - Symflower | https://symflower.com/en/company/blog/2024/llm-benchmarks/ |
| An In-depth Guide to Benchmarking LLMs | Symbl.ai | https://symbl.ai/developers/blog/an-in-depth-guide-to-benchmarking-llms/ |
| What Are LLM Benchmarks? - IBM | https://www.ibm.com/think/topics/llm-benchmarks |
| LLM Benchmarks: Understanding Language Model Performance | Humanloop | https://humanloop.com/blog/llm-benchmarks |