# Semantle 일본어 버전 개발 과정과 일본어 자연어처리에 대한 고민

> <日本語でセマントルをしたかっただけなのに : 일본어로 시맨틀을 하고 싶었을 뿐이었는데>

[일본어 Semantle 바로가기](https://semantoru.com/)

## 목차

1. [배경](#배경)
2. [프로젝트 소개](#프로젝트-소개)
3. [주요 NLP Task](#semantle에-사용된-주요-nlp-task)
4. [작업물 소개](#작업물-소개)
5. [후기](#후기)
6. [향후 계획](#향후-계획)
7. [발표 후 피드백](#발표-후-피드백)
---

## 배경

”Semantle처럼 간단하고 재미있는 NLP 기반의 게임을 만들면 어떨까?”

“그나저나 Semantle 일본어 버전도 하고 싶은데 왜 아무도 안 만들지?”

→ Semantle 일본어 버전을 내가 만들자!

---

## 프로젝트 소개

### Semantle이란?

단어의 유사도를 기반으로 정답 단어를 맞히는 게임입니다. 임의의 단어를 입력하면 해당 단어가 정답 단어와 얼마나 유사한지를 유사도 점수로 나타냅니다. 이를 통해서 정답 단어를 추측해나갈 수 있습니다.

가장 먼저 [영어](https://semantle.com/) 버전이 출시되었고, 다양한 언어 버전이 있으며 [한국어](https://semantle-ko.newsjel.ly/) 버전도 있어요.


### Semantle의 기본 사양

- 정답으로 낼 만한 단어들을 선정합니다.
    - 해당 언어 화자들이 자주 사용하는 단어여야 합니다.
    - 기본형 단어만 사용합니다. (시제나 수량 등에 따른 활용형은 제외합니다.)
- 단어들의 유사도를 계산하는 데 사용할 Word2Vec이 필요합니다.

---

## Semantle에 사용된 주요 NLP task

### Word2Vec

단어 임베딩을 학습하는 데 사용하는 모델입니다. cbow 모델과 skip-gram 모델 두 가지가 사용됩니다. 

**Continuous bag-of-words**

주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측합니다. bag-of-words가 단어의 등장 순서를 상관하지 않는 것과 달리, cbow는 단어의 순서를 고려합니다.

**skip-gram**

중심 단어로부터 주변 단어를 예측합니다. window 수만큼 중심 단어 앞뒤의 단어를 예측합니다.

### FastText

학습 방법은 Word2Vec과 유사하지만, 단어들을 subword 단위로 분절하여 벡터로 표현합니다. 그리고 단어의 벡터는 서브워드 벡터의 총합으로 표현됩니다.

- “국립국어원”
- → “<국립국어원>” : 시작과 끝에 `<>`를 붙여 단어의 시작과 끝을 표현합니다.
- → “<국”, “국립”, “립국”, “국어”, “어원”, “원>”

[fasttext](https://fasttext.cc/docs/en/options.html) 라이브러리를 활용하면 n-gram의 크기를 최소~최대 범위로 지정할 수 있습니다. 

- n의 범위를 3 ≤ n ≤ 5로 한 경우,, “국립국어원”
- → “<국립”, “국립국”, “립국어”, “국어원”, “어원>”, “<국립국”, “국립국어”, “립국어원”, “국어원>”, “<국립국어”, “국립국어원”, “립국어원>”

서브워드 벡터의 합으로 단어의 벡터가 결정된다니 아리송하지만 

사전에는 일정 빈도 이상 등장한 서브워드만 사전에 포함하므로 **그 자체로 어떤 의미를 가질 수 있는 서브워드들의 의미를 두루 고려한다**고 이해했습니다.

(위의 예시에서는 비교적 출현 빈도가 높을 '국립', '국어', '원'이 단어 벡터로 표현되고, '국립'+'국어'+'원'으로 '국립국어원'의 단어 벡터가 결정되겠지요.)

### FastText의 장점

subword 단위로 벡터화한다는 점에서 가지는 장점입니다. 

- OOV에 대응하기 쉽습니다.
- 빈도 수가 적은 단어에 대해서도 유의미한 임베딩 벡터를 얻을 수 있습니다.
- 오타의 경우에도 올바른 단어와 비슷한 임베딩 벡터를 얻을 수 있습니다.

### 한국어와 일본어 단어 임베딩

**한국어**

한국어에서 FastText의 장점을 살리기 위해서는 텍스트를 자모단위로 풀어서 학습하는 것이 좋다고 합니다.

한국어는 한 글자가 초성+중성+(종성)으로 구성되어 있기 때문이지요.

- 사이시옷 : 횟수(회 + 수), 숫자(수 + 자), 북엇국(북어 + 국), …
- 준말 : 언젠가는 / 언젠간, 뵈어요 / 봬요, 근데 / 그런데
- 용언 불규칙 활용 : 굽다/ 구워 / 구운 / 굽지 / … , 빠르다 / 빨라 / 빨리 / … , 파랗다 / 파란 / 파랗게 / 파래 / …
- 오타 : 알겟습니다, 괜챃ㄴ을까요, 팽귄…

꼬맨틀에서 사용한 사전학습된 FastText에는 [ICU tokenizer](https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-nori-tokenizer.html)가 사용되었습니다.

**일본어**

일본어는 한국어와는 차이가 있지만 여러 가지 어려운 부분들이 있습니다.

1.

하나의 단어가 여러 가지 표기로 쓰일 수 있습니다.

- 物ーものーモノ, 鮭ーしゃけ・さけーシャケ・サケ, …

MeCab 등 일본어 형태소 분석기에서는 한자나 일본어 표기로 바꾸어주는 기능을 제공하는 경우도 있습니다.

하지만 히라가나나 가타카나 표기는 동음이의어의 경우도 많고, 같은 한자라도 읽는 방법이 여러가지여서 처리가 까다롭습니다.

- もの（モノ）ー物/者, さけ（サケ）ー酒/鮭, …
- いるー居る、要る、入る、…

문장에 등장하는 표기 그대로 학습을 하게 되면 맥락에 의해 자연히 단어 벡터가 비슷하게 형성되길 기도하는 수밖에 없는데요, 

어려운 텍스트일수록 한자어 사용이 많고 쉬운 텍스트일수록 히라가나 사용이 많으므로 단어 간 유사도가 다르게 형성될 수 있을 것으로 예상됩니다. 

(실제로 플레이 시 표기에 따라 유사도 점수와 순위가 대체로 상이했습니다.)

2.

한자어 + する형 동사, 한자어 + だ형 형용사의 경우, 어미를 분절하여 한자어에 해당하는 부분만 명사로 처리됩니다. 

전체 단어 사전에서 동사의 비중이 줄어들게 되는 요인이 되었습니다.

3. 

한자는 부수의 조합으로 구성되어 있습니다. 부수를 다 분절했다면 더 풍부한 임베딩을 얻을 수도 있었겠지요?

- 魚の名前 : 鯖、鯵、鰹、鯉、鯛、…
- 樹木の名前 : 杉、桜、松、檜、…

일본 연구자분들이 한자를 분해하여 단어 임베딩을 만드는 방식을 연구한 바 있습니다. 그런데 실제로 많이 사용되고 있는지는 잘 모르겠어요.

- [Sub-character Neural Language Modeling in Japanese](https://aclanthology.org/W17-4122/)
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ac9c4a6-3b81-4d21-83b8-41e381ba2a2d/Untitled.png)
    
사전훈련된 FastText는 [MeCab](http://taku910.github.io/mecab/)을 사용한 걸로 명시되어 있으나, 구체적인 분절 방법은 명시되어 있지 않았습니다.

---

## 작업물 소개

<img width="928" alt="image" src="https://user-images.githubusercontent.com/96299403/230340707-a4d56673-92f8-4785-8f35-7abeb5e3c0eb.png">

(* 페이지 URL은 추후 업데이트 할 예정입니다.)

다른 언어 버전과는 정답 단어 목록을 필터링하는 과정에 차이가 있습니다.

- 사용 빈도가 높은 단어는 다른 버전과 마찬가지로 [Wiktionary](https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Japanese2022_10000)에서 가져옵니다.
- 이 중 표준 표기로 된 단어와 품사 필터링에 [松下님 데이터베이스](http://www17408ui.sakura.ne.jp/tatsum/database.html)를 활용합니다.
    - 다른 모델이나 사전 데이터에 잘 없는 단어별 표준표기가 정리되어 있어요. (good)
- 일본어에 해당하는 유니코드로 된 단어만 필터링합니다.
    - 전각문자 외 반각문자나 기호 등도 사용되지 않도록 합니다.

---

## 과제

- 하나의 단어가 여러 표기로 사용될 수 있는데, 정답 단어의 표기는 어떤 걸로 정해야 할까?
    - 한자를 기본으로 할 경우 과도하게 어려워질 수 있습니다. (いる는 익숙하지만 居る는 약간 당황스러워요.)
    - 히라가나를 기본으로 할 경우 유사도 계산 결과가 덜 유의미할 수 있고, 동음이의어가 많아 혼동이 예상됩니다. 
    - 고민 끝에 단어별로 하나씩 일반적으로 자주 쓰이는 표기만 채택하여 정답으로 사용하기로 했습니다.
    - 松下 교수님이 구축한 [데이터베이스](http://www17408ui.sakura.ne.jp/tatsum/database.html)의 “표준적 표기”를 활용할 계획이며, 라이센스에 대한 안내가 없어 별도 확인 중에 있습니다.
- 활용형 단어들을 추측할 수 있게 하는 것, 유사도 순위에 포함하는 것. 적절할까?
    - 영어와 한국어 버전을 플레이 하면서 활용형도 입력이 되어서 혼란스러웠던 적이 있습니다. ("어떤 단어까지 해볼 수 있는 걸까?")
    - 그러나 정답 단어는 명사/동사/부사/형용사의 기본형으로 제한되어 있고,
    - 전략적으로 활용형이 포함된 단어를 통해 힌트를 얻을 수 있다고 생각했습니다.
    - (결정적으로는) 다른 언어 버전도 이렇게 적용이 되어 있으므로 그대로 유지하기로 했습니다.
    - 단어의 활용형으로 추측을 할 수 없게 하려면 기본형으로만 word2vec을 학습하고 유사도를 계산할 필요가 있는데, 대규모 데이터로 사전학습한 word2vec보다 유의미하려면...

---

## 후기

단어 임베딩을 직접 구축하거나 여러 전처리 과정을 거치지는 않아 NLP 프로젝트로서는 조금 민망한 수준이 되었습니다.

일본어 표기법 때문에 고민이 많았지만 이 부분도 기존 데이터베이스를 활용하기로 하여 적당히 넘어갔네요.

개인적으로는 생전 처음하는 전반적인 앱 개발 프로세스 + 해외 박사님에게 메일 보내기 + 일본어로 설명 문구 쓰기가 더 어려웠습니다. :sweat_smile:

그렇지만 다음에 또 일본어 NLP를 할 일이 생긴다면 이번에 단어 임베딩이나 전처리에 대해 고민했던 일들이 참고가 될 것 같아요.

---

## 향후 계획

### Semantle-JP의 전격 출시

다음 내용들을 마무리하고 빠른 시일 내에 라이브하고 싶습니다. (어서 사람들과 같이 하고 싶은 마음:raised_hands:)

- 이름 확정 및 도메인 생성 (현재 후보 모집 단계)
    - セマントる(semantoru) : ググる(구글링하다), アピる(어필하다)와 같이 일본어 발음 ru로 끝나는 외래어 단어를 그 자체로 동사화 하는 신조어의 조어 방식이 있어서 빌려봤어요.
    - にぽマントル(nippomantle) : nippon + semantle의 합성어로, 'semantle'에 국가별 특징이나 개성이 담긴 말을 합성하는 전통에 따른 이름이예요.
- GA 연동
    - 주요 이벤트에 태그 삽입하여 통계를 얻어볼 생각입니다.
    - [참고] [ことのはたんご](https://plum-chloride.jp/kotonoha-tango/dev.html) wordle의 일본어 버전인데, 플레이 기록에 대한 통계를 제공하고 있어요.
    - 어떤 데이터를 수집할지 정하는 것이 중요하여 고민 중입니다.
- (플랜B) 필터링 데이터 변경
    - 정답 단어 필터링에 사용하려는 데이터베이스의 사용 허가를 받지 못하게 되면, [Unidic](https://clrd.ninjal.ac.jp/unidic/) 기준으로 변경할 예정입니다.
    - 단어 사용빈도에 따른 대표 표기는 여기에서 파악이 어려우므로, 단어의 기본형에서 등장 가능한 모든 표기가 정답 단어가 될 수 있습니다.

### Semantle 확장판

**챗봇 버전**

규칙은 기존 semantle과 같더라도 챗봇으로 만들면 신선할 것 같고, 일정 횟수 이상 시도한 경우 힌트를 제공하는 기능도 있으면 편리할 것 같았습니다.

- 어떤 품사의 단어야?
- 어떤 분야와 관련 있어?
- 어떤 텍스트에 자주 나와?
- …

그런데 제공할 힌트에 따라 DB와 룰베이스만으로 해결되기도 하겠지만 대단한 언어모델이 필요할 수도 있겠는데요, 구체적인 것은 아직 생각 안 해봤습니다. :grin:
- 참고 : [French Toast](http://www.topped-with-meat.com/connector/frenchtoast.html)

---

## 발표 후 피드백

발표 후 피드백 시간에 참석자분들로부터 semantle의 개선안에 대한 아이디어를 많이 주셨습니다. 모두 감사합니다!

### Word2vec으로 계산한 유사도가 사람들의 인식과 다른 경우가 많은데, 이 부분을 어떻게 개선할까?

1. 입력하는 단어를 로그로 수집 → 사람들이 연속해서 입력하는 단어는 상대적으로 유사하다고 가정 → 워드 임베딩에 반영
    - 초반에 마구잡이로 입력하는 단어들은 유사하지 않을텐데 반영해도 괜찮을까?
    - 초반에는 적게 반영되고 후반으로 갈수록(유사도가 높아졌을수록) 더 많이 반영되도록 학습률을 조정해보면 어떨까?
2. 문제에 대해 별점을 매겨서 임베딩에 반영
    - 결과 확인 창에서 별점을 입력할 수 있도록 하고, 별점을 취합해서 임베딩 벡터에 반영하자

### semantle을 플레이하는 유저들에 관한 추가 기능
1. 리더보드 기능
    - 추측 횟수가 적은 순으로 유저 순위를 보여주자
    - 계정은 관리가 어려우니 완료되면 올리고 싶은 닉네임을 입력해서 기록하도록 하면 어떨까? (오락실 게임처럼)
    - 계정이 없는 한 다른 기기나 브라우저에서는 새로운 게임을 할 수 있다보니 기록 만들기를 위한 치팅이 가능하다 (정답을 알아내고, 다른 기기로 한 번의 추측만 하면 되니까)
    - 방법이나 형식에 대해서 조금 더 고민해보기
2. 언어에 능한 사람들을 발견하기
    - GTM 태그로 로그를 수집해서 전반적으로 언어게임을 잘 하는 사람, 특정 언어에 대해 해박한 사람 등 유저 특징을 분류나 클러스터링으로 분석해보면 좋을 듯!

---

1주차 발표 내용은 여기까지입니다! 감사합니다. :smile:
